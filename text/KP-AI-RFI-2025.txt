March 14, 2025 
Office of Science and Technology Policy 
c/o Networking and Information Technology Research and  
Development (NITRD) National Coordination Office (NCO) 
Attn: Faisal D’Souza / AI Action Plan 
2415 Eisenhower Avenue 
Alexandria, VA 22314 
Submitted via email to: 
To Whom It May Concern: 
Kaiser Permanente appreciates the NITRD NCO’s consideration of our comments on the 
development of a new AI Action Plan, as directed by Executive Order 14179.1  
Kaiser Permanente is the largest private integrated health care delivery system in the United States, 
delivering health care to over 12.5 million members in eight states and the District of Columbia.2 
Our mission is to provide high-quality, affordable health care services and to improve the health 
of our members and the communities we serve.  We have a long history of developing and 
deploying innovative technologies and we prioritize safety, transparency, and improved health 
outcomes in the development, assessment, and deployment of any technology, including AI. 
The transformative potential for AI to improve nearly every aspect of health care, such as quality, 
patient experience, affordability and administrative efficiency, is truly profound. At the same time, 
it is critical that AI tools used in health care are safe, effective and trusted by both patients and 
health care providers. We support efforts by this administration to define priority policy actions 
that will promote innovation and safeguard against unnecessarily burdensome requirements.  
In addition to our more detailed comments below, we ask that the administration pursue the 
following high-priority policy actions: 
Federal government and private sector partnership to support AI innovation, research and
development . This should include clinical trials and establishment of a post-
implementation AI clinical tool surveillance program that health systems and providers can
adapt to serve their local patient populations and specific health care settings.
Development and implementation of a national AI regulatory framework that is flexible,
adaptable and evidence-based . This framework should provide an overarching risk-based1 Exec. Order No . 14179, Removing Barriers to American Leadershi p in Artificial Intelligence , 90 Fed. Reg. 8741 
(January 23 , 2025). 
2 Kaiser Permanente comprises Kaiser Foundation Health Plan, Inc. , one of the nation’s largest not-for-profit health 
plans, and its health plan subsidiaries outside California and Hawaii; the not-for-profit Kaiser Foundation Hospitals, 
which operates 40 hospitals and over 600 other clinical facilities; an d the Permanente Medical Groups , self-
governed physician group practices that exclusively contract with Kaiser Foundation Health Plan and its health plan 
subsidiaries to meet the health needs of Kaiser Permanente’s members. 
One Kaiser Plaza 


2 structure for industry-specific guidelines and standards that preempt state level legislation 
to avoid duplicative and inconsistent laws and requirements. 
Coordinated oversight of AI  across federal and state entities to avoid inefficient regulation
and enforcement.
AI in Health Care 
Health care increasingly utilizes AI tools and applications to improve patient outcomes, enhance 
operational and administrative efficiency, and advance medical research and education. Key 
categories of AI applications include assistive diagnosis of medical conditions for provider review, 
customized treatment recommendations, proactive case management, advanced analytics for 
population health management, improved patient engagement, identification of social health and 
economic barriers to healthy life, and a range of administrative activities. Specific examples3 of 
AI use cases in health care include: 
Applying clinical decision support tools across the spectrum of primary and specialty care,
such as computer vision programs to help clinicians identify subtle features that may be
associated with increased risk of diseases. For example, AI-enabled imaging of the retina
can be used to predict the risk of cardiovascular disease and stroke.
Using predictive analytics to better identify risks, quality gaps and outcomes across patient
populations.
Reducing provider burden and improving productivity by drafting patient communications,
such as discharge instructions, for a provider to review.
Improving efficiency in administrative tasks such as provider credentialing and member
verification.
Providing patient support via patient-facing chatbots and other self-directed tools.
Supporting medical education by enhancing training materials such as those related to
anatomy, pathology, surgical techniques, etc. For example, AI can be used to create
simulations of surgical procedures that allow trainees to practice skills and develop a better
understanding of complex procedures.
Predicting health care demand to optimize clinical workflows.
Many types of AI have been used extensively in health care for years, such as machine learning 
and pattern analysis to assist in interpreting imaging and algorithms to provide clinical decision 
support. However, the health care industry has just begun testing other types, such as generative 
AI applications.4 As this important work progresses, it is critical that the American health care 3 For a more detailed description of the use of AI tools by specialty, see American Medical Association and Manatt 
Health, Future of Health: The Emerging Landscape of Augmented Intelligence in Health Care, available at 
https://www.ama-assn.org/system/files/future-health-augmented-intelligence-health-care.pdf .  
4 For example, one generative AI tool currently being tested is aimed at reducing provider burden by transforming 
patient and provider conversations into a clinical note that can be reviewed by the provider before being added to 
electronic health records. See Balasubramanian, Is Ambient Dictation the Future of Clinical Documentation? (Dec. 
26, 2023), available at https://www.forbes.com/sites/saibala/2023/12/26/is-ambient-dictation-the-future-of-clinical-
documentation/?sh=7d4df3a88c08 .   


3 system leverage a regulatory framework that ensures tools used in health care are safe and effective 
without imposing unnecessarily burdensome requirements that slow adoption and discourage 
innovation. 
Investments in Innovation, Research and Development 
Investments in research and development are critical to promote innovation in AI related to health 
care to provide patients with access to life-changing advancements in care delivery that improve 
quality of care and clinical outcomes. The United States should continue to lead in AI model 
development in addition to other key areas, such as implementation science and clinical trials. 
To promote innovation and cutting-edge advancements, the health care industry would benefit 
from federal investments that support development and use of AI technology and infrastructure, 
standards development, workforce training, education and professional development programs. AI 
should augment human expertise, helping to alleviate administrative burden, improve clinical 
efficiency and enable health care professionals to focus on patient care. Investments should support 
training programs that prepare workers to collaborate effectively with AI, ensuring a seamless 
integration of AI-driven efficiencies across all levels of health care operations.   
Rigorous research studies (including traditional, large-scale clinical trials and other designs such 
as health system-embedded studies) are critical because they create a robust body of research and 
evidence to demonstrate how AI tools perform across patient populations. This allows a wider 
range of health care entities to evaluate safety and efficacy of AI tools and is more efficient than 
relying on trials done by individual health entities and systems. Further, establishment of a 
voluntary post-implementation surveillance program would allow health care entities to share data 
to inform best practices and highlight potential issues in a much more efficient and cost-effective 
manner. 
Federal Regulatory Framework 
We believe the adoption of a federal framework for governing AI, including standard definitions 
and a common risk framework, is important to both support innovation and speed to market for AI 
tools used in health care, and to avoid the alternative of a proliferation of inconsistent state-level 
requirements. States are at various stages of exploration and adoption of comprehensive AI 
regulatory frameworks in addition to issue- and sector-specific AI regulation. The development of 
a patchwork of inconsistent state AI requirements will create a highly complex regulatory and 
compliance environment that will hamper the adoption of AI tools and discourage innovation.  
We support the development of a national AI regulatory framework that is flexible, adaptable and 
evidence-based. It should provide a risk-based overarching structure consistent with nationally 
recognized AI risk management frameworks such as the National Institute of Standards and 
Technology (NIST) AI Risk Management Framework and the International Organization for 
Standardization/International Electrotechnical Commission (ISO/IEC) AI Guidance on Risk 
Management and allow for industry-specific guidelines that establish guardrails to ensure patient 
safety while being flexible and adaptable to keep pace with rapidly evolving technology. Where 
appropriate, the regulatory framework should adopt voluntary consensus standards to promote 
consistent development, use and oversight of AI while promoting innovation. Such a framework 


4 would ideally preempt conflicting state laws to ensure a consistent approach, while still permitting 
states to address region-specific issues or concerns. It should also serve as the basis for informing 
global AI regulatory frameworks and standards to ensure AI developers based in the US do not 
experience barriers to global operation. 
We also support the establishment and use of consistent definitions to describe AI and AI tools, 
developers, deployers and risk levels to ensure that efforts to regulate AI are appropriately tailored. 
These definitions should be informed by industry stakeholder feedback and focus regulatory 
efforts on new and emerging technologies, rather than tools that have been in place for years and 
been proven to be safe and effective. 
Trust in AI 
Trust in AI is essential for both clinicians and patients. The following policy actions related to 
transparency and accountability to improve trust in AI should be included in the AI federal 
regulatory framework to avoid inconsistent, duplicative, and overly burdensome state or industry 
specific transparency and accountability requirements for AI developers and deployers. 
Transparency 
Transparency is critical to ensure that AI used in health care is safe, effective and reliable. Health 
care providers may be hesitant to use “black box” AI tools that produce recommendations or 
other outputs that cannot be explained or are not fully understood. Additionally, transparency 
into how the tool was trained and/or developed can help providers identify and mitigate bias or 
other unintended results.  
For example, algorithms perform differently based on the data that were used to train them. An 
algorithm trained only using data from Medicare enrollees may not be applicable to pediatric 
populations. Providers need this information to evaluate whether the tool is appropriate for their 
practice and patient population. Additionally, AI tools and systems can change over time with 
exposure to different data sets and uses. A collaborative process should be established between 
AI tool developers and deployers to “recalibrate” the data and intended uses to ensure they are 
still performing as expected and have not deviated from the model. This process should also 
include a standardized reporting mechanism for unexpected AI behaviors that could impact 
patient safety. 
Another potential solution to ease transparency concerns is to require developers to provide a 
standard “nutrition label” that provides information regarding how the AI tool was created and 
what information was used to train or develop it. This would help deployers of AI tools better 
understand potential risks and issues inherent in the tool itself and whether it meets their intended 
uses. 
Accountability 
Both AI developers and health care entities that use AI tools must be accountable for issues that 
negatively impact the care delivery process. Accountability is critical to ensure patient trust in AI 
and confidence in its capabilities. However, it can be difficult to understand where accountability 
lies between developers of AI tools and individuals and organizations that deploy them. It is 


5 important to clarify these roles and responsibilities to ensure that deployers are responsible for 
their use of the AI tool and developers are accountable for data quality (including any third-party 
data sources used for development and validation), model performance and design flaws.  
Self-Developed Tools 
We recommend that the definition of “developer” used in the national regulatory framework 
described above exclude health care entities that create AI tools for their own internal use and do 
not make the tools available to others. This approach promotes access to innovative technologies 
and reduces costs and administrative burden. Health care entities carefully tailor AI tools to work 
within our clinical workflows based on our patient populations resulting in tools that are more 
efficient and clinically effective. It also reduces risks associated with AI tools developed by third 
parties, such as potential misuse or data inconsistencies or vulnerabilities. Health care entities that 
self-develop tools are ultimately responsible for how the tool performs and many of the 
requirements applicable to developers (e.g. transparency reporting requirements) would not make 
sense to apply to entities that both develop and deploy the tool internally.   
This approach is also consistent with the approach the Assistant Secretary for Technology Policy 
/ Office of the National Coordinator for Health IT (ASTP/ONC) has taken regarding the definition 
of Health IT developers. To ensure consistency, Congress should also authorize the federal Food 
and Drug Administration (FDA) to exclude AI tools customized or developed for use solely within 
a health care system from the definition of a medical device that is subject to approval. 
Coordinated Oversight 
It is critical that a comprehensive national AI regulatory framework coordinate across federal 
agencies and ensure consistency across states to avoid duplicative and inconsistent requirements. 
As noted above, proliferation of state-specific requirements may lead to inconsistent, duplicative 
and conflicting laws and regulations that create compliance risks for health care entities and AI 
tool developers. To reduce confusion and guard against differences in interpretation and 
enforcement, we recommend avoiding having multiple regulators oversee the same requirements. 
AI should be overseen by expert federal agencies well versed in emerging technology and 
associated risks and benefits. Before introducing new laws or regulations, policymakers should 
evaluate if existing rules adequately address AI use. 
For example, AI tools subject to review as medical devices by the FDA may be subject to 
regulation by more than one entity, which increases administrative burden and costs for the health 
care system without a corresponding benefit for patients. AI tools utilized by health systems would 
already be subject to requirements that ensure responsible AI development and deployment, and a 
coordinated oversight approach would ensure that these tools are not subject to regulation by 
multiple entities.  
Privacy and Security 
Patient health data is necessary to develop and train algorithms used in AI tools, as well as for 
the AI tool to perform its intended functions. Fortunately, health care information is already 
protected by the Health Insurance Portability and Accountability Act (HIPAA) privacy and 
security rules, which apply whenever a health care entity subject to HIPAA (a “covered entity”) 


6 develops, evaluates, or uses an AI tool or application to provide care. 
We recommend that the Department of Health and Human Services work with stakeholders to 
develop guidance that specifically describes existing privacy and security protections in place 
and how they apply to the use of AI data and tools in health care. This guidance should highlight 
any gaps or opportunities, such as consumer-driven tools and applications, so that any additional 
protections align with and build from the existing regulatory framework. 
Cybersecurity 
Health care entities are obligated to protect health information from unauthorized disclosures and 
take steps to mitigate risks of health care data breaches and cyberattacks. It is imperative that AI 
developers that create tools used in health care implement cybersecurity protections to safeguard 
sensitive health information held in AI models. These protections should be included in the 
development of the AI tool itself to encourage secure data sharing frameworks that enable AI 
driven research. AI developers should be accountable for data breaches or cyberattacks that occur 
due to issues with or inadequate cybersecurity protections in their models.  * *  * 
Kaiser Permanente appreciates the opportunity to offer our thoughts on this subject. If you have 
questions, please contact me at  or Megan Lane at 
Sincerely, 
Jamie Ferguson Stephen Parodi, MD 
Vice President, Health IT Strategy and Policy Executive Vice President 
Kaiser Foundation Health Plan, Inc  The Permanente Federation  


