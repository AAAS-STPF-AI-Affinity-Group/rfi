PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-a75w-vzod
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1472
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Will Peti llo 
General Comment
AI system s with hum an-com petitive intelligence pose profound risks to society, as acknowledged by num erous top AI researchers and
labs. Half of AI researchers believe that there is a 10% or greater chance that the invention of artificial superintelligence will m ean the end
of hum anity. Am ong AI safety scientists, this chance is estim ated to be an average of 30%. Notable exam ples of individuals sounding the
alarm  are Prof. Geoffrey Hinton and Prof. Yoshua Bengio, both Turing-award winners and pioneers of the deep learning m ethods that are
achieving the m ost success.
To m ake a long story short: m odern AI is not program m ed to follow a set of rules, but rather learns on its own to find patterns in data and
use those patterns to pursue goals. Unfortunately, aligning AI with the com plex goals and values of hum ans leads to system s pursuing goals
that are contrary to their creators' wishes. As system s becom e m ore com petent, there is an increasing danger that one of these instances
will pursue a m isaligned goal without us being able to stop it.
If AI is to benefit society, it m ust be developed with trem endous care. Safety engineering is a well-established concept. Before building a
bridge, civil engineers rigorously plan ahead—with safety as a top priority—to protect the public. In contrast, AI labs have em braced
startup culture's m antra "m ove fast and break things" to m axim ize profit, with Microsoft's Chief econom ist arguing that "we shouldn't
regulate AI until we see m eaningful harm ." Im agine if civil engineers acted the sam e way, refusing to design for structural integrity until a
few bridges collapsed!
Unfortunately, AI labs are locked in an out-of-control race to deploy ever m ore powerful digital m inds that no one—not even their
creators—can understand, predict, or control. A governm ent enforced pause on the developm ent of AI system s m ore powerful than
GPT-4 could end this suicide race, giving tim e for AI labs and independent experts to im plem ent a set of shared safety protocols for
advanced AI design that ensures these system s are safe beyond a reasonable doubt.
Pausing AI requires cooperation—between com panies as well as countries—to prevent the least safety-conscious actors from  seizing an
advantage. Just as DeepMind cannot slow down without risking falling behind OpenAI unless there is a governm ent enforced pause, the
US cannot slow down without risking falling behind China unless there is an international treaty, com plete with m onitoring and enforcem ent
m echanism s to give the agreem ent teeth.
Developing such a treaty will not be easy because the devil is in the details, which is why it is critical to get started now. Fortunately, there
are tem plates available, such as PauseAI’s proposal, CBPAI's Baruch Plan for AI, ControlAI’s "A Narrow Path", MIRI's work on
Technical Governance, and m any others, created by concerned citizens and independent experts. Coordination takes work, but the first
step is clear: pick up the phone and get started.
The real challenge with coordination is not with the technical details of enforcem ent, but with getting buy-in from  m ajor stakeholders. In
this, the US m ust lead. If the adm inistration recognizes the dangers of AI, states them  publicly, negotiates in good faith with other countries
to find a solution that benefits everyone, and com m its to following an agreem ent when (and only when!) such an agreem ent is reached,
then the hard part is over. What rem ains is a straightforward process of experts red-team ing ways to defect on the agreem ent, assum ing
the other side has thought of the sam e things, then com ing up with interventions that would prevent each m ode of defection, accepting the


m inor loss of sovereignty involved as a sm all price to pay for the ability to sim ilarly bind other nations. Such is the basis of the rule of law
and AI governance is no exception.
Luckily, there seem s to be broad support for slowing down AI developm ent. A recent poll indicates that 63% of Am ericans support
regulations to prevent AI com panies from  building superintelligent AI, but this has yet to translate into any legislation that would slow down
or prevent a superintelligent AI from  being created. The buy-in needed for international collaboration is already present; governm ents need
only listen to the will of the people.
Developm ent of advanced AI is not an arm s race, it is a suicide race. Whoever acts first will not secure a perm anent m ilitary advantage,
they will trigger the destruction of everything, including them selves. As with nuclear weapons, the only winning m ove is not to play.
Represent the people. Stop the race. Pause AI.


