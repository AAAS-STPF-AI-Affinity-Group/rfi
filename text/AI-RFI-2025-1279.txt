PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 13, 2025
Status: 
Tracking No. m 88-3jny-jqfb
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1279
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Matthew 
Milone 
General Comment
Hello. I'm  a software engineer and robotics teacher who's writing to advocate that the governm ent adopt a safety-first approach to AI
policy.
For decades, technologists have been concerned about the theoretical dangers of powerful AI. Within the last few m onths, their concerns
have been validated in experim ents by AI labs. For exam ple: in Decem ber, Anthropic and Redwood Research caught an AI strategically
copying itself and lying to hum ans to avoid being shut down. If the AI was sm art enough to outsm art hum ans, it would have been a
disaster. A sm arter system  could have hacked its way out of the lab or m anipulated a hum an into letting it out. For years, we've been
m aking AIs sm arter m uch faster than we've been m aking them  safer. If this trend continues, the results will be catastrophic.
Consequently, we need an international treaty that prohibits further developm ent of AI capabilities until the safety problem s are solved.
There is historical precedent for international prohibitions like this, including restrictions on nuclear technology and infectious disease
research.
I believe that problem s should be addressed as locally as possible--but in this case, a local, state, or even national scale wonâ€™t work.
There is no way for progress in AI safety to catch up unless progress in AI capabilities slows down; and if we slow down Am erica's
progress, then we have to slow down other countries' as well. Thanks for you tim e and consideration.
Sincerely,
Matthew V. Milone


