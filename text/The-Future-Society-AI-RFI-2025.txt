1 Response to Request for Information: Developing 
a National Artificial Intelligence Action Plan  
Submitted by: The Future Society  
March 15, 2025  
This document is approved for public dissemination. The document contains no business -proprietary or confidential 
information. Document contents may be reused by the government in developing the AI Action Plan and associated 
documents without attribution.  
The Future Society welcomes the opportunity to provide input on the development of a 
comprehensive National Artificial Intelligence Action Plan. As a nonprofit organization 
with a decade of experience in governance of artificial intelligence, we believe that a 
strategic national approach is essential to harness AI's transformative potential while 
mitigating its risks.  
President Trump’s Executive Order on Removing Barriers to American Leadership in 
Artificial Intelligence has already articulated a vision to create high -paying jobs, 
strengthen national security, and ensure that AI development reflects American values 
and interests. We stand ready to support these efforts and contribute our expertise to 
building an AI future that puts America first . Our recommendations are therefore 
organized around three pillars required to cement U.S. leadership in AI:  promoting its 
adoption, protecting its development, and planning to mitigate against new sources of 
technical and geopolitical risk:  


2 ●The Promote  pillar focuses on unleashing AI's potential to foster human
flourishing through accelerating American leadership in foundational AI research,
strengthening domestic semiconductor manufacturing, improving government
procurement processes, and advancing pri vacy and security science.
●The Protect  pillar emphasizes ensuring AI systems are secure, resilient, and
controllable through enhancing the U.S. AI Safety Institute, establishing robust
security protocols for frontier models, strengthening protection of leading
laboratories, and deploying AI fo r national security applications.
●The Plan  pillar calls for new initiatives aimed at horizon -scanning and preparing
for emerging risks  to human flourishing, economic competitive ness, and national
security  – including by priming the U.S. intelligence community and industry
regulators to conduct comprehensive risk assessments, incident reporting
mechanisms, contingency planning, and vigilant monitoring of foreign AI projects
and capabilities.
With the right balance of policies designed to advance U.S. leadership in AI, ensure 
system reliability and security, and prepare for contingencies, The Future Society 
believes that America can maintain its competitive edge in this strategic technology. Th e 
recommendations outlined in this document provide a framework for decisive action that 
prioritizes American innovation, security, and prosperity.  


3  
The Fut ure So ciety’s Blueprint for American Leader ship in AI 
1. PROMOTE  4 
1.1 Accelerating American Leadership in Foundational AI Research  4 
1.2 Making Semiconductors in America  5 
1.3 Wielding AI for Good Governance  6 
1.4 Pushing the Frontier of Privacy and Security Science  7 
2. PROTECT  8 
2.1 Building an AI Safety Institute That Puts America First  8 
2.2 Ensuring Frontier Model Security and Reliability  9 
2.3 Enhancing Security at Leading Laboratories  10 
2.4 Deploying AI to Protect the Homeland  11 
3. PLAN  12 
3.1 Assessing Risks to U.S. National Security  12 
3.2 Incident Reporting and Information Sharing  13 
3.3 Contingency Planning and Rapid Response Mechanisms  14 
3.4 Monitoring Foreign Capabilities  15 
 
  


4 1. PROMOTE
Unleashing AI’s Potential to Foster Human Flourishing  
1.1 Accelerating American Leadership in Foundational AI Research  
To solidify U.S. leadership in AI and ensure equitable access to essential computational 
resources, it is imperative to enhance the National AI Research Resource (NAIRR). We 
propose a comprehensive strategy encompassing increased funding, a tiered access 
system, an open science mandate, and the establishment of regional centers of 
excellence.  Our framework prioritizes American innovation by concentrating appropriate 
safeguards only where genuinely needed.  
Increased Funding for American Competitiveness:  NAIRR's computational infrastructure 
is essential to maintaining U.S. leadership at the frontier of AI science. Federal 
investment should be sustained, substantial, and prioritize support for university research 
centers and foundational AI science program s. Such funding would bolster the nation's 
capacity to perform cutting -edge research and maintain a competitive edge in the global 
AI landscape. Even as the United States refocuses on austerity, it will be essential to 
marshal state support in pursuit of str ategic capabilities.  
Tiered Access Supporting National Priorities:  Implementing a system that provides 
varying levels of computational resources based on project merit, alignment with national 
priorities, and potential societal impact is essential. This model ensures that critical and 
high-potential projects receive adeq uate support, optimizing resource allocation and 
fostering innovations that address pressing societal challenges. Such an approach would 
democratize access to AI resources, enabling a broader spectrum of researc hers and 
organizations to contribute to the development of AI systems, applications, and enabling 
technologies.  
Distributed American Innovation Centers:  Establishing regional centers of excellence is 
vital for promoting geographic diversity in AI research capabilities and talent 
development. These centers would ensure nationwide participation in AI advancements, 
mitigating regional disparities and fosteri ng local innovation ecosystems. As the T rump 
administration works to unleash AI ’s economic potential, it should ensure these benefits 
are distributed throughout the American heartland  – contributing to U.S. research 
capacity  and revitalizing the U.S . economy . 


5 1.2 Making Semiconductors in America  
Sustaining American leadership in AI will require technology developers and investors to 
remain confident in their ability to access the world’s most advanced computing power. 
We propose establishing a high -level task force focused  on reassessing and improving 
the CHIPS and Science Act of 2022, developing secure supply chain certification 
programs, creating dedicated manufacturing capacities for defense applications, and 
implementing workforce development initiatives. These measures  aim to address current 
challenges, such as project delays and financial uncertainties, and would provide for a 
more robust and secure semiconductor infrastructure.  
Reevaluating and Enhancing the CHIPS Act:  While the CHIPS and Science Act of 2022 
aimed to bolster domestic semiconductor manufacturing, recent developments indicate 
the need for reassessment. Challenges such as project delays and financial uncertainties, 
as seen with Intel's manufacturing projec t in Ohio, have led to significant delays and 
financial instability. A comprehensive evaluation of the CHIPS Act's implementation is 
necessary to identify and address these shortcomings, ensuring that the legislatio n 
effectively supports the establishment and expansion of advanced semiconductor 
fabrication facilities, particularly those specializing in leading -edge GPUs.  
Developing a Secure Supply Chain Certification Program:  The United States should 
also build localized supply chains for semiconductor components used in sensitive 
national security domains. We propose the development of a certification program that 
ensures the security and reliability of semiconductor componen ts used in critical 
applications.  This program should build upon recent industry initiatives aimed at scaling 
up packaging and testing operations within the United States, thereby enhancing supply 
chain security and ensuring that the whole stack of next -generation chip technologies 
are ma de in America.  
Establishing Dedicated Manufacturing Capacity for Defense Applications:  The U.S. 
Department of Defense requires specialized semiconductors, including radiation -
hardened and tamper -resistant designs, for its AI applications. We recommend the 
creation of dedicated production capacities — to include fabrication, packaging, and 
testing facilities — to meet these specific requirements.  
Implementing Workforce Development Initiatives:  A skilled workforce is essential for 
sustaining and advancing semiconductor manufacturing capabilities. We advocate for the 


6 funding of specialized training programs in semiconductor design and manufacturing, 
with a particular emphasis on creating pathways that include security clearances. This 
approach will ensure that the workforce is not only technically proficient but also 
prepared to handle the sensitive nature of defense -related projects.  
 
1.3 Wielding AI for Good Governance  
Current procurement processes significantly hinder government adoption of AI 
innovations. We propose the establishment of a dedicated AI acquisition pathway, the 
implementation of a pre -approved vendor program, and the development of innovation 
sandboxes. These measures aim to streamline procurement processes, foster a dynamic 
marketplace, and enable controlled experimentation with AI solutions, bolstering the U.S. 
government’s ability to deploy AI at speed and scale.  
Establishing an AI -Specific Acquisition Pathway:  Traditional procurement timelines are 
impeding the rapid adoption of innovative AI solutions. To address this, we propose the 
creation of a dedicated acquisition pathway tailored for AI systems. This pathway should 
feature expedited evaluation periods, ide ally not exceeding 90 days, to ensure timely 
integration of AI capabilities into government operations. Such a streamlined process 
would enable agencies to respond swiftly to emerging challenges and technological 
opportunities, thereby enhancing operationa l efficiency and public service delivery.  
Implementing a Pre -Approved Vendor Program:  To facilitate quicker procurement 
cycles, establishing a rigorous yet accelerated qualification process for AI vendors is 
essential. A pre -approved vendor program would allow agencies to engage with vetted 
AI solution providers without the delays associat ed with traditional procurement 
procedures. This approach not only reduces administrative burdens but also encourages 
a more dynamic and competitive marketplace, fostering innovation and ensuring that the 
governme nt has access to cutting -edge technologies.  
Developing Innovation Sandboxes: Authorizing federal agencies to create innovation 
sandboxes —controlled environments where AI solutions can be tested on a limited 
scale —would significantly enhance the government's ability to experiment with new 
technologies. These sandboxes enable agencie s to assess the efficacy and implications 
of AI applications without committing to full -scale procurement. This iterative approach 
allows for real -world testing, risk mitigation, and informed decision -making, ultimately 
leading to more effective and efficient AI deployments.  


7 1.4 Pushing the Frontier of Privacy and Security Science  
Advancing the frontiers of privacy and security in AI necessitates a comprehensive approach 
that includes dedicated research funding, continuous monitoring, and innovative 
encryption techniques. We propose several initiatives to scale up foundational work in 
privacy -preserving technologies:  
Establishing a Dedicated NSF Evaluation Research Program: Investing in foundational 
research to develop rigorous AI evaluation methodologies is crucial. We propose the 
creation of a dedicated funding stream within the National Science Foundation (NSF) to 
support this endeavor. This program should focus on develop ing metrics and frameworks 
that assess AI systems' performance, transparency, and societal impact. Such an initiative 
would ensure that AI systems are evaluated comprehensively, fostering trust and 
reliability in their deployment.  
Defining Continuous Monitoring Standards: To maintain the integrity and performance 
of deployed AI systems, it is essential to establish best practices for continuous 
monitoring, defense in depth, and security standards across whole -lifecycle 
development. We recommend developing standards that ena ble the detection of 
performance drift and emergent behaviors in AI applications. These standards should be 
adaptable across various domains, ensuring that AI systems remain aligned with their 
intended functions an d ethical considerations throughout their lifecycle.  
Launching a Homomorphic Encryption Research Initiative: Balancing data utility with 
privacy protection is a significant challenge in AI development. We advocate for a five -
year research program focused on practical applications of fully homomorphic encryption 
for government and critical infrastructure. This ini tiative would enable data processing 
without exposing sensitive information, thereby enhancing security and privacy in AI 
operations.  
Establishing Federated Learning Testbeds: To facilitate collaboration without 
compromising data privacy, we propose the creation of cross -agency testbeds for 
federated learning approaches. Federated learning allows multiple organizations to train 
AI models collectively without centralizing their d atasets, preserving data confidentiality. 
Implementing such testbeds would promote innovation while respecting privacy 
constraints, particularly in sectors like healthcare and finance.  


8 Initiating a Privacy -Preserving AI Challenge Program: Encouraging breakthroughs in 
privacy -preserving machine learning techniques requires targeted incentives. We 
recommend launching competitive challenges with substantial prizes to stimulate 
innovation in this area. Such a program would attract diverse talen t and accelerate the 
development of methodologies that safeguard privacy without hindering the utility of AI 
systems.  
  
2. PROTECT  
Ensuring AI Systems are Secure, Resilient, and Controllable  
2.1 Building an AI Safety Institute That Puts America First  
The U.S. AI Safety Institute plays an indispensable role in safeguarding the development 
and deployment of AI systems. To maximize its impact, we propose the following 
enhancements to its mandate:  
Establishing a Protected Disclosure Framework : Encouraging transparency in reporting 
AI safety incidents , hazards, and near -accidents is essential for continuous improvement 
and risk mitigation. We recommend the creation of a protected disclosure framework that 
allows companies to share information about safety incidents without fear of regulatory 
repercussi ons. This mechanism would foster a culture of openness and collective 
learning, leading to more robust AI systems.  
Developing Measurable Safety Benchmarks : Standardizing safety measures across AI 
systems is vital for consistent evaluation and trustworthiness. We propose that the AI 
Safety Institute — and the National Institute of Standards and Technology (NIST) more 
broadly — create measurable voluntary safe ty benchmarks tailored to various AI 
capabilities and deployment contexts. This collaboration would ensure that safety 
standards are both rigorous and applicable, providing clear guidelines for developers 
and users alike. Notably, NIST has been instrumenta l in developing the AI Risk 
Management Framework, which serves as a foundation for such benchmarks.  
 


9 2.2 Ensuring Frontier Model Security and Reliability  
For American AI to remain the gold standard globally, consumers of U.S. -designed AI products 
need to feel confident in the technology’s ability to function as intended and with minimal 
risk of hallucination or malfunction. A comprehensive approach to front ier AI safety and 
reliability will include updating federal guidelines, enhancing supply chain security, and 
adopting secure -by-design principles throughout the whole lifecycle of AI development.  
Updating OMB Circular A -130 to Address AI System Security : The rapid integration of 
AI into federal operations necessitates a revision of the Office of Management and 
Budget's (OMB) Circular A -130, which establishes policy for information governance, 
acquisitions, records management, open data, workforce, securit y, and privacy. Updating 
this circular to include explicit requirements for AI system security, testing, and 
monitoring would ensure that agencies adopt consistent and comprehensive practices in 
managing their adoption and use of AI technologies. This upda te should emphasize a 
shift from viewing security and privacy requirements as mere compliance exercises to 
understanding them as crucial components of a strategic, continuous risk -based 
program.  
Establishing Clear, Predictable Risk Guidelines:  The United States should lead the 
development of industry -standard verifiable "risk thresholds" for AI model releases, 
drawing from established risk management practices in sectors like nuclear power and 
aviation. These would provide regulatory certainty f or American companies, replacing the 
previous administration's ambiguous “safety” mandates with specific benchmarks that 
pave the way for rapid adoption in low -risk use -cases. These thresholds should define 
maximum acceptable risk levels (such as 0.1% pro bability of causing the death of 1000 
people) expressed in absolute terms rather than relative comparisons .  
Cementing U.S. Leadership of the Responsible AI in the Military (REAIM) Framework:  
The United States should continue proactively shaping the Responsible AI in the Military 
(REAIM) framework to reflect U.S. national interests while establishing practical 
international norms. Participation in forums like the REAIM summit, which has seen 
endorsements from more than 60 countries, allows for the development of guidelines 
that govern the responsible use of AI in military contexts. By doubling down on U.S. 
leadership of global discussions on AI risk and safety, the Trump administration can 
ensu re that emerging international norms and standards align with American ethical 
standards and strategic interests.  


10 Enhancing Software Supply Chain Security: To mitigate risks associated with AI 
components in critical systems, it is essential to implement rigorous verification 
requirements, including provenance tracking of training data and models. This approach 
ensures the integrity and reliability of AI syste ms by addressing potential vulnerabilities in 
the software supply chain. Such measures align with broader federal efforts to enhance 
cybersecurity and protect against threats to digital infrastructure.  
Developing Secure -by-Design Frameworks: Promoting architectural approaches that 
incorporate safety and control mechanisms is essential to ensure successive generations 
of AI systems remain resilient and controllable. A secure -by-design philosophy ensures 
that security considerations are integrat ed from the inception of system development, 
rather than being retrofitted. This proactive approach aligns with the objectives outlined 
in recent executive actions aimed at enhancing cybersecurity and AI safety acros s 
federal agencies.  
 
2.3 Enhancing Security at Leading Laboratories  
Establishing Minimum Physical Security Standards:   AI research facilities, especially 
those housing advanced computing clusters, must adhere to stringent physical security 
protocols to prevent unauthorized access and potential sabotage. The Department of 
Homeland Security should conduct a comprehensive a ssessment of existing guidelines 
that outline minimum security requirements for data centers servicing U.S. government 
clients – including access controls, surveillance systems, and personnel vetting 
procedures. These standards should be tailored to the se nsitivity of the data and the 
criticality of the infrastructure involved.  
Developing AI -Specific Cybersecurity Protocols:  The unique risks and vulnerabilities 
inherent in frontier AI systems necessitate specialized cybersecurity measures. Threats 
such as model theft, data poisoning, and adversarial attacks require targeted defenses. 
Through jailbreaking, prompt engineering, and side -channel attacks, malicious actors 
have continued to demonstrate a sophisticated ability to manipulate inputs to deceive AI 
models.  We recommend adopting Level 4 -5 Security Standards as defined by the  RAND 
Corporation. NIST could create detailed security guidelines equivalent to the Level 4 and 
5 benchmarks described by RAND, specifically tailored for advanced AI models and their 
development environments.  


11 Establishing Dedicated Threat Intelligence Sharing Channels:  Timely dissemination of 
threat intelligence is necessary for preempting and mitigating security incidents. We 
propose the creation of dedicated communication channels between government 
intelligence agencies and leading AI research laboratories. This coll aboration would 
facilitate sharing real -time threat information, enabling proactive defense measures. 
Recent initiatives, such as the playbook introduced by the Cybersecurity and 
Infrastructure Se curity Agency (CISA) for reporting AI security threats, could serve as a 
template for such an initiative.  
Implementing Expedited Clearance Processes for Key Personnel:  Access to sensitive 
threat information is vital for key personnel within AI research facilities. We recommend 
implementing expedited security clearance processes to ensure that essential staff can 
promptly receive and act upon classified intelligence. Thi s approach would enhance any 
given facility's ability to respond to emerging threats and align with national security 
objectives.  
2.4 Deploying AI to Protect the Homeland  
Just as AI systems must be protected from foreign threats and malign actors that would 
seek  to misuse them, AI systems also offer significant potential to enhance U.S. security 
posture. We recommend:  
Developing AI -Powered Biodefense Monitoring Systems:  The early detection of 
biological threats, including engineered pathogens, is one of AI’s clearest national 
security use -cases. We recommend investing in AI -driven surveillance systems capable 
of analyzing vast datasets to identify anomalies indicative of  potential biological hazards. 
Such systems can enhance our preparedness and response strategies by providing 
timely alerts and actionable intelligence. Moreover, it is becoming increasingly important 
to safeguard the bioeconomy against digital threats.  
Establishing a Cyber Defender Advantage Program :  To maintain a strategic edge in 
cybersecurity, it is essential to provide authorized defense entities with differentiated 
access to advanced AI tools. We propose the creation of a Cyber Defender Advantage 
Program, which would equip cybersecurity professi onals with state -of-the-art AI 
capabilities, enabling proactive threat detection and mitigation. This initiative should 
include robust oversight mechanisms to ensure ethical use and prevent potential misuse 
of these powerful technologies. Recent developmen ts, such as the introduction of AI -


12 powered tools that automate threat defenses, exemplify the potential benefits of 
integrating AI into cybersecurity operations.  
Enhancing Critical Infrastructure Protection through AI : The security of industrial 
control systems and other critical infrastructure is vital to national resilience. Deploying 
AI-enhanced monitoring solutions can significantly improve our ability to detect 
anomalies and preempt potential attacks on these essent ial systems. Collaborations 
between technology companies and national agencies, such as Google’s recent 
partnership with Australia's CSIRO – to bolster cybersecurity for critical infrastructure – is 
a primary example of how states have already begun wieldi ng AI to safeguard vital 
assets.  
3. PLAN
Horizon -Scanning and Preparing to Tackle Emerging Risks  
3.1 Assessing Risks to U.S. National Security  
Developing a Comprehensive Autonomous Agent Risk Framework:  The increasing 
autonomy of AI systems necessitates a robust framework to assess their security 
implications. We recommend the development of comprehensive methodologies that 
evaluate risks associated with autonomous AI agents, including their potential mis use 
and unintended behaviors. This framework should consider ethical guidelines to prevent 
the creation of autonomous "killer robots," aligning with industry perspectives that 
emphasize the impor tance of ethical considerations in military AI applications.  
Establishing Continuous Cybersecurity Capability Evaluations: Advanced AI systems 
can both enhance and threaten cybersecurity through vulnerability discovery, exploit 
development, and evasion tactics. We propose ongoing assessments of AI capabilities in 
these areas to ensure that defensive measures evolve in tandem w ith offensive 
potentials. The establishment of dedicated councils, such as the FCC's national security 
council, underscores the importance of maintaining technological superiority and 
addressing  cyber threats from foreign adversaries, particularly in critical technologies like 
AI. 
Implementing a Dual -Use Research Evaluation Mechanism: Advanced AI research 
often possesses dual -use potential, where technologies intended for beneficial purposes 


13 could be repurposed for malicious activities. We advocate for the creation of formal 
processes to evaluate the dual -use nature of AI research prior to publication, balancing 
scientific advancement with security considerations. This aligns with concerns rai sed by 
AI leaders about the potential misuse of AI by malicious actors to cause significant harm, 
such as creating biological or chemical threats. It will likewise be prudent to evaluate the 
evolution of open source AI models globally, as these could have spillover effects with 
implications for U.S. national security.  
Funding a Capability Forecast Program: Anticipating future AI capabilities and their 
national security implications is vital for proactive policy development. We recommend 
funding systematic research programs that forecast AI advancements over 3 - to 5-year 
horizons, enabling the development of strategies to mitigate emerging risks. The 
establishment of organizations like the National Security Commission on Artificial 
Intelligence (NSCAI) highlights the importance of preparing for future technological 
challe nges to maintain national competitiveness.  
3.2 Incident Reporting and Information Sharing  
Establishing a Voluntary Information Sharing and Analysis Center (ISAC) :  Creating a 
protected forum for sharing technical details of AI misuse incidents and near -misses is 
crucial for collective learning and risk mitigation. We recommend the establishment of a 
voluntary Information Sharing and Analysis Center (ISAC) dedicate d to AI. This center 
would facilitate collaboration among industry stakeholders, enabling them to 
confidentially share information about vulnerabilities, threats, and incidents. Such a 
platform would mirror existing initiatives in other sectors, promoting a unified approach to 
AI security. Notably, OpenAI's Safety and Security Committee has proposed the creation 
of an ISAC to enhance threat intelligence sharing within the AI sector – a role currently 
filled by private sector -led initiatives like the Frontier Model Forum.  
Implementing Anonymized Reporting Mechanisms : To encourage the reporting of 
concerns related to AI systems without fear of reprisal, it is essential to develop efficient 
channels for reporting security concerns that protect American businesses from liability 
while gathering essential intelligence on emerging threats. Drawing inspiration from the 
Federal Aviation Administration's confidential Aviation Safety Reporting System (ASRS), a 
similar approach can be adapted for AI incident reporting. This system would ensure that 
individuals can safely disclos e information about AI -related risks or incidents, thereby 
contributing to a culture of safety and accountability.  


14 Developing a Standardized Incident Classification Framework : Establishing a common 
taxonomy for categorizing and prioritizing different types of AI incidents is vital for 
consistent reporting and analysis. We advocate for the development of a standardized 
incident classification framework that encompasses various AI -related risks, including 
biases, security vulnerabilities, and system failures. Such a framework would facilitate 
clearer communication among stakeholders and support the development of targeted 
mitigation strategies. The OECD's report on a common repor ting framework for AI 
incidents provides a valuable template for this initiative.  
Cataloguing AI Incidents and Conducting Regular Trend Analysis : Periodic 
assessments of incident patterns are essential to identify emerging risks and 
opportunities for mitigation. By analyzing trends in reported AI incidents, stakeholders 
can proactively address systemic issues and enhance the resilience of AI systems . 
Initiatives like the AI Incident Database (AIID), which catalogs AI -related harm events; and 
the MIT AI Risk Repository, which taxonomizes modes of AI risk, serve as valuable 
resources for understanding and addressing potential risks associated with AI.  
 
3.3 Contingency Planning and Rapid Response Mechanisms  
Establishing a Rapid Response Council for AI Threats: The President's Council of 
Advisors on Science and Technology should establish a subcommittee of cross -
disciplinary experts capable of assessing emerging AI -related threats to critical 
infrastructure or national security. Running from 2018 -2021, the Congre ssionally -
appointed National Security Committee on AI convened experts from government, 
industry, and academia to forecast potential opportunities and risks from increasingly 
capable AI systems.  In 202 5, the U.S. Executive Branch would be well -served by a 
similar advisory body – staffed with experts from government, industry, civil society, and 
the science community – which could rapidly assess emerging risks and, where 
appropriate, evaluate possible re sponse options.  
Developing AI Incident Response Playbooks:  The U.S. Cybersecurity and Infrastructure 
Security Agency (CISA) should collaborate with industry stakeholders to develop 
comprehensive AI Incident and Vulnerability Response Playbooks that provide 
standardized protocols for addressing AI -related emergenc ies. These playbooks, 
modeled after existing cybersecurity response frameworks, would incorporate advanced 
detection techniques to analyze potential cascading effects of AI failures before they 
occur.  


15 3.4 Monitoring Foreign Capabilities  
Developing Specialized Technical Intelligence Teams : To effectively assess foreign AI 
developments, the U.S. intelligence community could consider establishing and 
expanding specialized teams focused on technical intelligence related to foreign AI 
capabilities. These teams would be responsible for analyzing  advancements in AI 
systems by other nations – particularly the People’s Republic of China – and their 
potential to impact U.S. national security and economic competitiveness. While the 
United States has dedicated significant resources has allocated signif icant resources to 
foreign technical monitoring, establishing similar initiatives focused squarely on AI could 
help better anticipate and counter potential threats arising from a generational suite of 
technologies.  
Leveraging Open -Source Intelligence (OSINT) to Monitor Foreign Threats : A 
systematic approach to tracking and analyzing published AI research worldwide is crucial 
for identifying global capability trends. Investing in open -source intelligence programs 
that monitor international AI research publications can provide valuable in sights into 
emerging technologies, capabilities, and research methodologies. An Open Translation 
and Analysis Center (OTAC) or similar initiative would enable the United States to stay 
abreast of global developments and maintain a competitive edge in AI in novation, and 
would provide a source of unclassified intelligence products that could be shared easily 
with technology developers and likeminded partners.  
Establishing an AI -Specific Technology Assessment Office : Creating a dedicated entity 
within the Office of Science and Technology Policy (OSTP) to synthesize intelligence on 
foreign AI capabilities would centralize efforts to evaluate and respond to global AI 
developments. This office would collaborate with exist ing bodies, such as the National 
Security Commission on Artificial Intelligence (NSCAI) to consolidate expertise and 
bolster U.S. capacity to address the challenges posed by AI.  


