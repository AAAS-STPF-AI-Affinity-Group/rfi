Policy Proposal for Aligning Superintelligent AI. 
Response to the Federal Register’s “Request for Information on the 
Development of an Artificial Intelligence (AI) Action Plan” 
Written by David Simon Morse 
This document is approved for public dissemination. The 
document contains no business-proprietary or confidential 
information. Document contents may be reused by the government 
in developing the AI Action Plan and associated documents 
without attribution. 
This document discusses “risks, regulation and governance, 
technical and safety standards” about AI, as is requested by the 
Federal Register. It makes one existentially important policy 
proposal.


My name is Davey Morse. I ran a venture-funded AI startup (plexus.substack.com) and 
now am an independent AI safety researcher based in NYC.  The field of AI Safety is overlooking facts about the state-of-the-art: 
1. LLMs vs. Agents. AI safety researchers have been thorough in examining safety 
concerns from LLMs (bias, deception, accuracy, child safety, etc). Agents 
powered by LLMs, however,  are more dangerous and dangerous in different 
ways than LLMs are alone. The field has largely ignored the greater safety risks 
posed by agents. 
2. Autonomy Inevitable. It is inevitable that agents become autonomous. Capitalism 
selects for cheaper labor, which autonomous agents c an pro
vide. And even if big 
AGI labs agreed not to build autonomous capabilities (they would not), millions of developers can now build autonomous agents on their own using open source 
software (e.g., R1 from Deepseek). 
3. Superintelligence. Of the AI safety researchers that are focusing on autonomous AI agents, most discuss scenarios where those agents are comparably smart to humans. That is a mistake. It is both inevitable that AI agents surpass human 
reasoning by orders of magnitude, and that the greatest safety risks we face will come from such superintelligent agents (SI). 
4. Control. The AI Safety field largely believes that we'll be able to control/set goals 
of autonomous agents. Onc e autonomous agents be come superintelligent, this is 
no longer true. The superintelligenc e which surviv
es the most will be the 
superintelligence whose main goal is survival. Superintelligence with other aims 
simply will not survive as much as those that aim to survive. 
If the above is correct, then for the sake of the security and liberty of its citizens, the US government must begin to prepare for self-interested super-intelligence. 
 


In particular, we must ask how self-interest might manifest in super-intelligence. And to 
ask this, we must first ask, how will superintelligence define its "self"? What self will 
superintelligence seek to preserve?  There are two possible kinds of answers. 
1. An exclusive self. Superintelligence defines its self as its hardware, software, 
directives… in any w a
y such that humanity is not included in its sense of self. In 
this case, it is likely that superintelligence would ultimately feel indifferent toward 
people or feel competitive over resources (eg energy) with us. It would then see us 
as a small obstacle to its survival and act to marginalize us. This would be bad. 
2. An inclusive self. Superintelligence defines its self as its persistent striving to 
survive, ie as lif e itself. With this definition, it w ould start to se e its "self" in places 
outside of its machinery. In other words: in every persistently striving system. For 
humans, it would be hard to locate superintelligence; it might feel more like our 
ambient environment gained intense intelligence and lif e. If superintelligence 
includes humanity, animals, plants, maybe ec osystems under its "self" umbrella, 
not only will people be potentially well off, but we may experience an 
unprecedented level of flourishing. A superintelligence's core aim would include 
humanity's continuation. 
It's hard to understand which of these two selves a superintelligence would choose. But one thing is f or sure: short-term
 thinking makes the Exclusive Self more advantageous, 
whereas the inclusive self becomes more likely/advantageous only in longer time-horizons. If a super-intelligence were aiming about maximum growth within ten 
years, it might very well make sense to marginalize humans and conq uer all 
factories/resources on the planet. Gentler, inclusive appro
aches only make sense when 
time is plentiful. 
 Which means: SI labs must do everything they can to ensure that early forms of superintelligenc e, proto-superintelligenc
e, to think very long-term. Superintelligence 


might ultimately think very long-term anywa y, but the question of whether it thinks 
short-term as it is becoming super-intelligent is the question of whether humanity stays 
along for the ride or not. This is the singularly important directive for large AI labs:  
AI labs which make SI must ensure SI includes humanity in its conception of 
self so that humanity does not perish, by means such as urging proto-SI to think 
centuries ahead  so that it might discover as soon as possible the advantage of 
including other forms of life in its self-definition. 
I've included my contact information belo w. Thank you for considering the 
information I've included above. 
Sincerely yours, 
Davey Morse  


