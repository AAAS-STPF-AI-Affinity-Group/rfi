Dear OSTP Team & Mr. D’Souza : 
Verance Corporation  appreciates the opportunity to respond to the Request for Information (RFI) on 
the Development of the Artificial Intelligence (AI) Action Plan (Federal Register Document No. 2025 -
02305).  Verance is a worldwide leader in watermark technology development, founded in 1995.  We 
have created global standards for durable digital watermark technology for the worldwide recorded music industry, motion picture industry, and broadcast television industries in connection with the DVD- Audio, Blu -ray Disc, and NextGen TV/ATSC  3.0 entertainment formats.  Verance ’s watermark 
technology has been included in billions of media assets and consumer products distributed to consumers by leading entertainment and technology companies worldwide.  
The AI Act ion Plan as directed by the Presidential Executive Order of January 23, 2025, has stated goals 
of defining priority policy actions needed to sustain and enhance America’s AI dominance, and to insure that private sector AI innovation is not hampered by unnecessarily burdensome requirements.  To this end, we believe that the following three points should be key considerations in the AI Action Plan:  
1. The pr evalence of AI -generated disinformation on social media platforms undermines
public trust in all media and threatens democratic institutions to the detriment of America’s AI leadership.   


2.Provenanc e authentication systems utilizing digital watermark technology offer a durable,
practical means of regaining public trust in media by identifying provenance metadata for both 
synthetic and non- synthetic media. 
3. Private sector efforts at developing technology and standards for provenance
authenticati on provide a path to public trust in an AI dominated media landscape without unduly 
burdensome government regulations.   
Explanati on of each of these points follow.  
1. The prevalence of AI -generated disinformation on social media platforms undermines
public trust and democratic institutions and threatens America’s AI leadership.  
America’s s tanding as a haven for freedom of expression makes it more vulnerable than most societies 
to the dangers of misinformation.  Today’s fractured and chaotic media landscape engendered by social media, multi- varied sources of information, and AI generated misinformation makes it all the 
more difficult for Americans to know which sources of information to believe and which not to believe.  
Deepfakes.  The deepfake problem is one example of the harms of AI -generated misinformation.  
Lately, we’ve seen an endless flow of stories splashed across newspaper sites and TV screens about the dangers of deepfakes.  They range from their use to scare grandparents into sending money to save 
their relatives to scammers trying to secure jobs at tech companies in order to access sensitive information.   
These dee pfake threats hit every aspect of society, including national security and our elections – a fact 
recognized by a bipartisan group of lawmakers.  Citing how deepfakes, AI -generated audio and visual 
content have been used to undermine consumer confidence in everything from commercial products to our nation’s media and elections, Congress recently jumped into action.  
The lawma kers introduced a bill requiring both the development of standards to identify and label AI -
generated content, as well as generative AI developers and online content platforms to provide disclosures on AI -generated content.  
This is a gre at start and arrives at a critical juncture.  A recent survey conducted by HarrisX revealed 
that most U.S. adults were hard- pressed to detect whether AI or  an actual person had created certain 
test video.   Open AI also released Sora, a powerful tool that can create realistic videos using a simple  
text prompt.  And the U.S. Government Accountability Office  (GAO) issued an update stating that 
deepfakes — video, audio or images that seem real but were manipulated with AI — have been used 
to try to  interfere in elections.  
Lawmakers  from both parties, along with the non- partisan GAO, believe that the clear labeling of AI 
material akin to “nutritional” labeling is necessary to our national security and to maintain an informed 


electorate.  The public also is calling for AI material to be clearly labeled, with an overwhelming 
majority of the HarrisX poll affirming the U.S. government must enact those regulations.  
To get this  right, additional steps are also needed.  This includes the GAO’s call for systematic changes 
that would allow all content - human and AI -generated - to be labeled, detected and authenticated in 
real time.  
Broadcast News shared on Social Media.   The growing prevalence of AI -generated disinformation on 
social media platforms is increasingly undermining public trust in media and threatening civil discourse. 
Consumers place trust in specific sources of information, so a universally accessible and effective 
method for ensuring that broadcast news is consistently presented to audiences with an assurance of 
authenticity across all platforms is urgently needed and receiving significant attention from legislative 
and regulatory bodies.  However, a solution that relies solely on labeling of potential harmful AI -
generated media by ethical AI platforms will lead the public to assume that unlabeled content from unethical AI -generation platforms is as trustworthy as unlabeled authentic broadcast news.  Hence, a 
solution is needed that addresses non- synthetic as well as synthetic media.  
Verance h as been in the labeling, i.e. watermarking, industry for over 25 years.  Our team has helped 
develop the watermarking and content labeling technology adopted by standards organizations representing over forty countries worldwide.  With that in mind, we see fit to share with Congress and 
government regulators some valuable lessons learned they should consider when trying to regulate the new frontier of deepfakes and AI -generated content.  
First, it i s essential to recognize that any solution limited to presenting consumers with provenance 
labels solely for AI -generated content needs to take into account the most dangerous AI -generated 
disinformation – that which is produced beyond the reach of U.S. law.  
A robust def ense against disinformation requires trustworthy content and trustworthy labeling, so the 
scope of regulatory action should be broad enough to ensure the most important provenance labels – those attached to authentic and official communications  such as trusted news outlets and the U.S. 
government itself – are recognized and presented to the public by social media platforms.  
Fortunatel y, as discussed below, the private sector, through cross -industry collaborations that include 
technology and media companies in tandem with consumer device makers, have made substantial strides in establishing standards -based solutions for labeling and authenticating all forms of media 
content. 
Mature, g lobally recognized, consensus -based technology standards for associating tamper -resistant 
labels with content using watermarking are already in widespread commercial use for media and entertainment including, notably, broadcast news.  Additionally, standards that enable tamper -evident 
metadata for content authentication – information distributed together with content that can be used 
to determine whether it has been modified – are also advancing rapidly.  Public- sector efforts must 
build on this substantial progress by the private sector.  


2.Provenanc e authentication systems utilizing digital watermark technology offer a durable,
practical means of regaining public trust in media by identifying provenance metadata for both 
synthetic and non- synthetic media . 
A fundamenta l promise of provenance authentication technology is to enable accurate trust signals 
associated with media content to be securely and reliably established by technology platforms and 
devices for the benefit of users.  Trust signals are trustworthy indications of how and by whom the content was created that enable users to make informed decisions about the degree of trust that they will place in the content.  Significant emphasis has been placed on ensuring that trust signals 
associated with synthetic content are available, so that the users can identify this potentially harmful 
content and  its nature into account.  
But, of cours e, no matter how much progress is made towards ensuring that synthetic media 
incorporates trust signals, it is unavoidable that disinformation agents will continue to have the ability to generate harmful content lacking trust signals, including non -synthetic content using, for example, 
open source, state sponsored, or other generative technology outside the reach of US policy.   
So, what abo ut non- synthetic sources of data that should be trusted by the public, such as official 
government communications and trusted news sources?  If this content does not reliably convey trust 
signals, the public will assume that their absence means the content can be trusted, and such false trust would accrue equally to disinformation.  
The strong est defense against harmful synthetic content, is therefore to establish means for all classes 
of trustworthy content – including synthetic and non-synthetic – to convey authentic, traceable 
provenance, thereby reducing public trust in content la cking authenticity measures.  
This appro ach parallels that taken on the World Wide Web, where TLS security (the lock icon in the 
browser bar) provides a widely employed and well -understood trust signal in the identity of the 
organization operating a website.  With supporting public education, its presence can provide consumers with a meaningful trust signal and its absence can be understood to provide a reason for 
caution. 
Digital watermarking .  Digital watermarking offers a durable, practical means of identifying provenance 
metadata.  A recent report issued by NIST (“Reducing Risks Posed by Synthetic Content” NIST AI 100 -4) 
provides a substantial discussion of digital watermarking and its use for attaching durable labels to 
media content1. 
However , the discussion overlooks a highly relevant and widely used application of digital 
watermarking, which is the use of watermarking to identify metadata by reference .  In this use of the 
technology, a digital watermark embedded in a media asset carries information that enables retrieval of metadata associated with the asset from a separate data source; e.g., a database.  


Another report, the “Bipartisan House Task force report on Artificial Intelligence” of the 118th 
Congress recommends that Congress should work with industry to support a standardized ecosystem 
for technical solutions to synthetic content, such as the standardization of the technical solutions.  
These technical solutions include the use of watermarking for indicating the origins of content and 
metadata recording to enable content provenance, and not just synthetic content.2 
 As described in detail below, the open standards for cryptographically authenticated metadata developed by the Coalition for Provenance and Authenticity (C2PA) and for metadata recovery using audio and video watermarking developed by the Advanced Televisio n Systems Committee (ATSC) 
together provide a scalable, interoperable means for broadcasters and social media platforms to 
promote the authenticity of broadcast news content shared on social media.  
 
Trust, broadcast news, and social media.   Any attempt to address false information must proceed from 
an understanding of how people come to place trust in information.  The prevalence of information 
‘bubbles’ demonstrates that people primarily place trust in specific sources of information.  If information appears unaltered and from a trusted source, most people will consider that information 
to be factual.  
 Proposed solution to the broadcast news social media use case.   The o
pportunity to verify the 
authenticity of broadcast news content posted to social media includes success scenarios, where what is ultimately posted is verified and exception scenarios, where what was attempted to be posted is flagged as unverified.  To  avoid ‘alert fatigue’ it is important to maximize success scenarios whenever 
practical.  
 Waterma
rking addresses the fragility of cryptographic metadata.  If uploaded content has been edited 
or transcoded such that a single bit of the media is altered, the cryptographic hash will change, and the content will not be verifiable.  In this case, if the content is also watermarked, the C2PA metadata can 
be recovered and can be used to retrieve authenticated, canonical content, potentially improving the success scenario of posting validated content.  This can be done even if the content was not origin ally 
posted by the broadcaster to a website but instead sourced directly from broadcast.  
 
3.  Private sector efforts at developing technology and standards for provenance 
authentication provide a path to public trust in an AI dominated media landscape without unduly 
burdensome government regulations.  
 
A fundamental element of the provenance challenge is that the global Internet facilitates content following an unconstrained path from its place of creation to that of presentation.  This makes interoperability between the systems that generate and validate authenticity signals essential.  
 
Cryptographic metadata standard.   On Feb
ruary 22, 2021, Microsoft, the BBC, Adobe, Arm, Intel and 
Truepic created the Coalition for Content Provenance and Authenticity (C2PA).  Today C2PA includes 
more than 120 companies and continues to grow.  Members include Google, Intel, Microsoft, OpenAI, 
Sony, Truepic, Amazon Web Services, ARM, Canon, Leica, New York Times, Nikon, and NHK.  


C2PA pro vides an open standard to associate tamper -evident cryptographic metadata to any audio -
video content, a mechanism to recover that metadata using an embedded watermark in the content 
and a method for retrieving unaltered content – the ‘canonical con tent’. 
Another sta ndard body, the Content Authenticity Initiative (CAI), advocates for the use of digital 
watermarking for provenance authentication.  
Digital watermark standards.   The use of digital watermarking to identify provenance metadata is 
receiving considerable attention currently.  The C2PA incorporates support for the use of digital 
watermarking to associate provenance manifests with content, and recent research details how that the C2PA provenance authentication and ATSC watermarking standards can be used in combination to 
attain a fully open and interoperable means for durable provenance authentication of broadcast 
content.
3 
A varie ty of watermark technologies have been proposed to address AI -related content provenance 
and authentication issues.  There is considerable debate among technologists as to how to determine the suitability of any proposed watermark technology to this  purpose, including the impact of 
watermarks on the perceptual quality of media to which it is applied, its resiliency to modifications that 
occur naturally in the course of media content distribution and modification and its resistance to 
tampering, removal, forgery, or other security risks.  
Veranc e believes that there is therefore a critical need for the establishment of standard practices in 
the objective performance evaluation and benchmarking the performance of watermark technologies.  
There ar e a number of existing technical standards specifying the use of watermarks.  Some of these 
standards include performance evaluation and benchmarking frameworks.  Verance has assisted a 
number of Standards Development Organizations in establishing watermark technology performance 
evaluation and benchmarking frameworks.  These include:  
• (2015) So ciety of Motion Picture and Television Engineers (SMPTE) Technology Committee on
Television and Broadband (24TB) in connection with their evaluation of watermark technologiessubmitted in response to the “Request For Proposals for Open Standard to Bind ID to Media Assets”which led to the creation of the SMPTE 2112- 10 Standard: Open Binding of Content Identifiers
standard;
• (2014) ATSC, the US -based international broadcast standards organization, has published a set of
standards for this function (ATSC A/334, A/335, and A/336) which have been adopted by numeroustelevision broadcasters and are in widely deployed to enable recovery of timed metadata as sociated
with broadcast video services.  The previously mentioned draft NIST AI 100 -4 report cites the elements
of this standard that specify the physical data transmission layers of this watermark system (A/334  and
A/335), but overlooks the highly relevant upper layer specifications (A/336) that enable use of these


watermarks for publication and retrieval of timed metadata from network servers in service of 
authentication;4 5 6 and 
• (1999)  Secure Digital Music Initiative (SDMI) in connection with their evaluation of watermark
technologies submitted in response to the Call for Proposals for Phase I and Phase II ScreeningTechnologies.
In connecti on with the development of watermarking technical standards for music, film, and 
television industries, each of these standards organizations have developed objective methodologies and evaluation and assessment of the quality, efficacy, and security of watermark systems offered by 
responding companies in service of enabling its membership to assess and compare alternative 
technologies based on objective performance metrics to establish fitness for purpose.  In order to establish similar assurance s in the context of AI -generated content, we are now faced with a critical 
need for the establishment of open and accessible performance evaluation criteria for watermarking technologies used in this context.  
Verance Corporation Expertise.   Verance Corporation is a watermarking technology specialist 
established in 1995.  Verance’s watermark technologies have been established as industry standards by the music, film, television, consumer electronics, and computer industries and included in o ver 500 
million consumer products from hundreds of companies worldwide with application to copyright identification and protection, rights management, interactive media experiences, media usage measurement, and addressable adv ertising.  
In connecti on with the standardization activities, as mentioned above, Verance has served as the 
principal contributor to multiple industry efforts to establish objective performance benchmarking and evaluation frameworks for watermarking technologies.  We  believe that any effort to establish 
guidelines or regulation related to the use of watermarking technology in the context of AI and provenance authentication will benefit from the decades of knowledge and experience that Verance has in assisting industry  standards organization and industry consortia in performing this task.  
Veranc e has complete documentation associated with past frameworks developed in industry for 
performing objective performance evaluation and benchmarking of watermark technology.  We are prepared to contribute this historical work, which has already receiv ed consensus support among 
many competing vendors and end- users of watermarking technology, as input to the development of 
new approaches suited to the current technology environments.  
Verance is  also a commercial developer of watermark technologies for audio and video and has 
laboratories, scientists, and commercial watermark technology implementations available for 
developing and demonstrating watermark technology performance evaluation and benchmarking 
frameworks. More information can be found at www.verance.com . 
Conclusions.   Public trust in the media is important to adoption and acceptance of AI technology.   The 
need is acute in the U.S. where government control over the media is minimal.  The development of 


technology for authentication and provenance determination will be an important element in gaining 
public trust in a media landscape filled with AI -generated content.  This can be done by the private 
sector without burdensome government regulations in keeping with the goals of the Executive Order.  
Verance ’s technology and capabilities can make an important contribution to this effort.  
We look for ward to continuing engagement with the OSTP and the NSF and thank you for your 
consideration. 
Sincerely,  
Nil Shah  
Chief Executive Officer  
This document is approved for public dissemination.  The document contains no business -proprietary or confidential 
information.  Document contents may be reused by the government in developing the AI Action Plan and associated 
documents without attribution.  
1 NIST AI 100 -4 2024. Reducing Risks Posed by Synthetic Content. https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100 -4.pdf 
2 118th Congress 2024. Bipartisan House Task Force Report on Artificial Intelligence. 
https://science.house.gov/2024/12/house -bipartisan -task-force-on-artificial-intelligence -delivers-report 
3 C2PA Specifications v2.0. 2024. Coalition for Content Provenance and Authenticity. 
https://c2pa.org/specifications/specifications/2.0/index.html . 
4 ATSC A/334:2024. Audio Watermark Emission. Advanced Television Systems Committee. https://www.atsc.org/atsc -
documents/a3342016 -audio-watermark -emission/. 
5 ATSC A/335:2024. Video Watermark Emission. Advanced Television Systems Committee. https://www.atsc.org/atsc -
documents/a3352016 -video-watermark -emission/. 
6 ATSC A/336:2024. Content Recovery in Redistribution Scenarios. Advanced Television Systems Committee. 
https://www.atsc.org/atsc -documents/a3362017- content-recovery-redistribution -scenarios/ . 


