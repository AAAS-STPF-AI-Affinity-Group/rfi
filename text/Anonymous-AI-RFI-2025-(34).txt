1 
I am writing in response to the NSF's request for information and OpenAI's submission 
on the same. I am explicitly speaking for myself and not representing my company, 
which is not OpenAI. I have spent the past several years working at one of the 
companies  at the forefront of AI, and am knowledgeable  about the technology and its 
effects. With apologies, I am choosing to remain anonymous because the last individual 
to publicly disagree with OpenAI and point out the conflict of AI training with the 
Copyright Act, former OpenAI employee Suchir Balaji, wa s found shot to death in his 
apartment in November 2024, with no signs of suicidal intent. I believe it is possible that 
his death resulted from his disagreement with OpenAI on fair use, and would like to note 
the potential danger for ordinary Americans in  disagreeing with powerful executives at 
the helm of billion -dollar companies. If a name is required for this comment to be 
considered complete, please use "John Doe."  
PREEMPTION  
President Trump's Executive Order prompting the current request for information calls 
for American AI that is "free from ideological bias or engineered social agendas," which I 
am glad to support. However, OpenAI's call for preemption of state laws goes in  a 
different direction from the goals of that Executive Order. The state laws that OpenAI is 
calling to overturn include California's prohibitions on deepfakes, 98% of which are 
pornographic and target women and girls; Tennessee's industry -defending regula tions 
against commercial exploitation of singers' and actors voices; and multiple states' laws 
protecting vulnerable Americans from the use of AI in scams and fraud.  
It is precisely the willingness of responsible US AI companies to follow the law and 
reduce the threat of their models to Americans that make US AI products more desirable 
to lawful customers both at home and abroad. For example, compliance with the EU's A I 
Act and GDPR is necessary for market competitiveness in Europe. OpenAI's desire for 
preemption translates directly to legalizing attacks on ordinary Americans, especially 
girls, politicians, public figures, creators, and innovative small business owners.  Many of 
these attackers would be foreigners and foreign states using American AI against 
Americans.  
When OpenAI writes about China that "Their models also more willingly generate how -
toâ€™s for illicit and harmful activities such as identity fraud and intellectual property theft, a 
reflection of how the CCP views violations of American IP rights as a featu re, not a 
flaw," this is also an accurate description of how OpenAI wishes to operate in the US. 
That is, OpenAI is seeking the President's blessing to become more like the CCP, so 
that OpenAI's models can also legally generate how -tos for illicit and harm ful activities 
and violate American AP rights as a feature, not a flaw. The present ethical difference 
between US AI models and those of less -regulated countries is precisely due to US 
regulation and US companies' development of AI in anticipation of the g uardrails that 


2 
OpenAI describes as "overly burdensome." This ethical advantage, which is also a 
commercial and competitive advantage globally, would disappear if the preemption 
requested by OpenAI were to be granted.  
Agreeing to OpenAI's demand for preemption would directly cause a large increase in 
AI-generated child sexual assault material (CSAM), suicides of deepfaked women and 
girls and defrauded Americans, and tremendous economic losses in the US --though 
OpenAI do es stand to profit. This can hardly be said "to promote human flourishing, 
economic competitiveness, and national security," which is the goal of the AI Action 
Plan. National laws should standardize the protections for Americans and American 
creative produ cts introduced at the state level, with the state laws as a floor. Preemption 
should not occur without those national measures for defending Americans firmly in 
place, and in effect.  
COPYRIGHT  
As the Executive Order states: "This order shall be implemented consistent with 
applicable law." The most pertinent federal law being violated by major AI companies 
today, with the possible exception of Adobe, is the Copyright Act. During its history, the 
US population has been disproportionately blessed with creative ability, resulting in the 
creation of businesses small and large, from the Walt Disney company to knitters who 
sell their designs online. All of these artists, creators, and business owners be nefited 
from US copyright protection and regulation. OpenAI's suggestions that the federal 
government give AI companies carte blanche to violate the copyright protections of 
creators is legally questionable and prioritizes the interests of one massive comp any 
against many smaller businesses and individuals. Doing as OpenAI requests would 
mean that any of the foreign piracy networks listed on the Notorious Markets for 
Counterfeiting and Piracy could call themselves AI companies and claim the US IP they 
are v iolating is training data for AI models, and thereby skirt the consequences of US 
laws.  
As the NSF may be aware, the US Copyright Office is due to publish the third part of its 
guidance on AI and Copyright, addressing the issue of fair use, in May or June 2025. 
Prior to proposing any policy changes, the NSF should incorporate that guidance, 
whatever it will be, into its understanding of how AI training interacts with the rights of 
creative and innovative Americans and the Copyright Act.  
CAUTIONS AND LIMITATIONS  
It is also notable that OpenAI has not mentioned anywhere in its comment submissions 
any of the limitations of AI that make its usefulness dependent on careful scrutiny, 
audits, and examination. These include the ineradicable tendency of AI models to 
hallu cinate, i.e. to make up nonexistent facts or citations; the inability of AI to filter for 


3 
irony, resulting in notable recommendations that people eat rocks or glue; the inability to 
handle completely new and unprecedented data that does not appear in the training set; 
the ability of generative models to regurgitate training data that can includ e private and 
personally identifying information; and so on. One study found that diagnostic AI models 
had a counterintuitive unhelpful effect on doctors: where AI models were confident in 
their diagnosis, doctors were also confident and so did not need th e AI model. Where AI 
models were uncertain, doctors were also uncertain, and relied more heavily on the AI 
model's prediction, even though that prediction was less likely to be correct. Particularly 
in sensitive or life -and-death contexts, such as healthca re, AI use should be careful, 
measured, and deployed only with human supervision. US citizens' sensitive and 
personal information, as well as sensitive US intelligence, should never be fed to 
publicly accessible models. All outputs should be carefully scru tinized, and failsafes for 
hallucinations, inaccuracies, and similar errors should be put in place.  
Thank you to the NSF and OSTP for their time and attention.  
Anonymous Employee of a US AI Company  
(John Doe)  
--- 
This document is approved for public dissemination. The document contains no 
business -proprietary or confidential information. Document contents may be reused by 
the government in developing the AI Action Plan and associated documents without 
attribution.  


