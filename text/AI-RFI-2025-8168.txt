PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8b-1yjy-zuy7
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-8168
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Kristen Barnett 
Porter  
General Comment
Tech com panies and the tech innovation ethos has a policy of destroying things first and asking safety questions once there's nothing left
but ashes. Art is created by hum ans. The point of copyrighting art is to protect the financial interests of the hum an creator. Machines are
sim ply not entitled to copyright protections because they have no personal financial interests to protect. Only the creators of that AI would
financially benefit. Why should any hum an or entity be able to profit off of artwork they didn't create when the creators of these AI
system s often can't even site 'where' the AI "learned" to "create" that 'art'? This is all feeling like a high end grift to repackage plagiarism  as
innovation. If the US doesn't firm ly, boldly, and com pletely protect hum ans, it's citizens, from  the m ost basic infringem ent of copyright we
probably will beat China is this "AI race" because we will no longer be Am erican. The burdens and cost of that victory will be paid for by
hum ans, People who require food, water, and shelter, while m achines rake in m ore stolen cash for Silicon Valley. Copyright protection is
for hum ans - People who need financial protection. The robots will be fine without it. And see attached files as well please
Attachments
Ai com m entary of theft
Free and open access in the age of generative AI




a newsletter by Molly White[citation needed]
ARTIFICIAL INTELLIGENCE
“Wait, not like that”: Free and open access in the age of
generative AI
The real threat isn’t AI using open knowledge — it’s AI companies killing the projects that
make knowledge free
Molly White
14 Mar 2025 —  9 min read
Listen to me read this post here (not an AI-generated voice!), subscribe to the feed in your podcast app, or download the recording for later.
The visions of the open access movement have inspired countless people to contribute their work
to the commons: a world where “every single human being can freely share in the sum of allknowledge” (Wikimedia), and where “education, culture, and science are equitably shared as ameans to benefit humanity” (Creative Commons
a).
But there are scenarios that can introduce doubt for those who contribute to free and openprojects like the Wikimedia projects, or who independently release their own works under freelicenses. I call these “wait, no, not like that” moments.
When a passionate Wikipedian discovers their carefully researched article has been packaged
into an e-book and sold on Amazon for someone elseʼs profit? Wait, no, not like that .
When a developer of an open source software project sees a multi-billion dollar tech companyrely on their work without contributing anything back? Wait, no, not like that.
When a nature photographer discovers their freely licensed wildlife photo was used in an NFTcollection minted on an environmentally destructive blockchain? Wait, no, not like that .


And perhaps most recently, when a person who publishes their work under a free license
discovers that work has been used by tech mega-giants to train extractive, exploitative largelanguage models? Wait, no, not like that .
These reactions are understandable. When we freely license our work, we do so in service ofthose goals: free and open access to knowledge and education. But when trillion dollarcompanies exploit that openness while giving nothing back, or when our work enables harmfulor exploitative uses, it can feel like we've been naïve. The natural response is to try to regaincontrol.
This is where many creators find themselves today, particularly in response to AI training. But
the solutions they're reaching for — more restrictive licenses, paywalls, or not publishing at all —risk destroying the very commons they originally set out to build.
Citation Needed  is an independent publication, entirely supported by readers like you.
Consider signing up  for a free or pay-what-you-want subscription — it really helps me to keep
doing this work.
The first impulse is often to try to tighten the licensing, maybe by switching away to somethinglike the 
Creative Commonsʼ non-commercial      (and thus, non-free) license. When NFTs enjoyed
a moment of popularity in the early 2020s, some artists looked to Creative Commons in hopesthat they might declare NFTs fundamentally incompatible with their free licenses (they didnʼt
1).
The same thing happened again with the explosion of generative AI companies training modelson CC-licensed works, and some were disappointed to see the group take the stance that, not onlydo CC licenses not prohibit AI training wholesale, AI training should be considered non-infringing by default from a copyright perspective.
2
But the trouble with trying to continually narrow the definitions of “free” is that it is impossibleto write a license that will perfectly prohibit each possibility that makes a person go “wait, no,not like that” while retaining the benefits of free and open access. If that is truly what a creatorwants, then they are likely better served by a traditional, all rights reserved model in which anyprospective reuser must individually negotiate terms with them; but this undermines thepurpose of free, and restricts permitted reuse only to those with the time, means, and bargaining


power to negotiate on a case by case basis.b
Particularly with AI, thereʼs also no indication that tightening the license even works . We already
know that major AI companies have been training their models on all rights reserved works in
their ongoing efforts to ingest as much data as possible. Such training may prove to have beenpermissible in US courts under fair use, and itʼs probably best that it does.
3456
Thereʼs also been an impulse by creators concerned about AI to dramatically limit how peoplecan access their work. Some artists have decided itʼs simply not worthwhile to maintain an onlinegallery of their work when that makes it easily accessible for AI training. Many haveimplemented restrictive content gates — paywalls, registration-walls, “are you a human”-walls,and similar — to try to fend off scrapers. This too closes off the commons, making it morechallenging or expensive for those “every single human beings” described in open accessmanifestos to access the material that was originally intended to be common goods.
Often by trying to wall off those considered to be bad actors, people wall off the very people they
intended to give access to. People who gate their work behind paywalls likely didnʼt set out tocreate works that only the wealthy could access. People who implement registration wallsprobably didnʼt intend for their work to only be available to those willing to put up with the riskof incessant email spam after they relinquish their personal information. People who try to staveoff bots with CAPTCHAs asking “are you a human?” probably didnʼt mean to limit their materialonly to abled people
7 who are willing to abide ever more protracted and irritating riddles.8 And
people using any of these strategies likely didnʼt want people to struggle to even find their workin the first place after the paywalls and regwalls and anti-bot mechanisms thwarted searchengine indexers or social media previews.
And frankly, if we want to create a world in which every single human being can freely share in
the sum of all knowledge, and where education, culture, and science are equitably shared as ameans to benefit humanity, we should stop attempting to erect these walls. If a kid learns thatcarbon dioxide traps heat in Earth's atmosphere or how to calculate compound interest thanks toan editorʼs work on a Wikipedia article, does it really matter if they learned it via ChatGPT or byasking Siri or from opening a browser and visiting Wikipedia.org?
Instead of worrying about “wait, not like that”, I think we need to reframe the conversation to
“wait, not only like that” or “wait, not in ways that threaten open access itself”.  The true threat
from AI models training on open access material is not that more people may access knowledge


thanks to new modalities. Itʼs that those models may stifle Wikipedia and other free knowledge
repositories, benefiting from the labor, money, and care that goes into supporting them whilealso bleeding them dry. Itʼs that trillion dollar companies become the sole arbiters of access toknowledge after subsuming the painstaking work of those who made knowledge free to all,killing those projects in the process.
Irresponsible AI companies are already imposing huge loads on Wikimedia infrastructure, which
is costly both from a pure bandwidth perspective, but also because it requires dedicatedengineers to maintain and improve systems to handle the massive automated traffic. AndAI companies that do not attribute their responses or otherwise provide any pointers back toWikipedia prevent users from knowing where that material came from, and do not encouragethose users to go visit Wikipedia, where they might then sign up as an editor, or donate afterseeing a request for support. (This is most AI companies, by the way. Many AI “visionaries” seemperfectly content to promise that artificial superintelligence is just around the corner, but claimthat attribution is somehow a permanently unsolvable problem.)
And while I rely on Wikipedia as an example here, the same goes for any website containing
freely licensed material, where scraping benefits AI companies at often extreme cost to thecontent hosts. This isn't just about strain on one individual project, it's about the systematicdismantling of the infrastructure that makes open knowledge possible.
Anyone at an AI company who stops to think for half a second should be able to recognize they
have a vampiric relationship with the commons. While they rely on these repositories for theirsustenance, their adversarial and disrespectful relationships with creators reduce the incentivesfor anyone to make their work publicly available going forward (freely licensed or otherwise).They drain resources from maintainers of those common repositories often without anycompensation. They reduce the visibility of the original sources, leaving people unaware thatthey can or should contribute towards maintaining such valuable projects. AI companies shouldwant a thriving open access ecosystem, ensuring that the models they trained on Wikipedia in2020 can be continually expanded and updated. Even if AI companies donʼt care about the benefitto the common good, it shouldnʼt be hard for them to understand that by bleeding these projectsdry, they are destroying their own food supply.
And yet many AI companies seem to give very little thought to this, seemingly looking only at the
months in front of them rather than operating on years-long timescales. (Though perhaps anyonewho has observed AI companiesʼ activities more generally will be unsurprised to see that they do
not act as though they believe their businesses will be sustainable on the order of years.)


not act as though they believe their businesses will be sustainable on the order of years.)
It would be very wise for these companies to immediately begin prioritizing the ongoing health
of the commons, so that they do not wind up strangling their golden goose. It would also be verywise for the rest of us to not rely on AI companies to suddenly, miraculously come to their sensesor develop a conscience en masse.
Instead, we must ensure that mechanisms are in place to force  AI companies to engage with these
repositories on their creators' terms.There are ways to do it: models like Wikimedia Enterprise, which welcomes AI companies to use
Wikimedia-hosted data, but requires them to do so using paid, high-volume pipes to ensure thatthey do not clog up the system for everyone else and to make them financially support the extraload theyʼre placing on the projectʼs infrastructure. Creative Commons is experimenting with theidea of “
preference signals ” — a non-copyright-based model by which to communicate to AI
companies and other entities the terms on which they may or may not reuse CC licensed work.c
Everyday people need to be given the tools — both legal and technical — to enforce their ownpreferences around how their works are used.
Some might argue that if AI companies are already ignoring copyright and training on all-rights-
reserved works, they'll simply ignore these mechanisms too. But there's a crucial difference:rather than relying on murky copyright claims or threatening to expand copyright in ways thatwould ultimately harm creators, we can establish clear legal frameworks around consent andcompensation that build on existing labor and contract law. Just as unions have successfullynegotiated terms of use, ethical engagement, and fair compensation in the past, collectivebargaining can help establish enforceable agreements between AI companies, those freelylicensing their works, and communities maintaining open knowledge repositories. Theseagreements would cover not just financial compensation for infrastructure costs, but alsorequirements around attribution, ethical use, and reinvestment in the commons.
The future of free and open access isn't about saying “wait, not like that” — itʼs about saying "yes,
like that, but under fair terms”. With fair compensation for infrastructure costs. With attributionand avenues by which new people can discover and give back to the underlying commons. Withdeep respect for the communities that make the commons — and the tools that build off them— possible. Only then can we truly build that world where every single human being can freely
share in the sum of all knowledge.


As I was writing this piece, I discovered that a SXSW panel featuring delegates from the Wikimedia Foundation and Creative
Commons, titled “ Openness Under Pressure: Navigating the Future of Open Access ”, discussed some of the same topics. (I was,
sadly, scheduled to speak at the same time and so was unable to attend in person). The audio recording is available online, and Iwould highly recommend giving it a listen if this is a topic that interests you!
Footnotes
a.Creative Commons      is a non-profit that releases the Creative Commons licenses     : easily reusable licenses that broadly
release some rights so that anyone can share and/or build upon the works under specified terms.
b.However, these restrictive licenses cut both ways. The more restrictive the license on your work, the more incentive forpowerful entities to bargain your own
 rights away from you. For example: when I agree to restrictive licensing terms in
freelance writing contracts, I am often prohibited from republishing my own writing later on (e.g. in an anthology of my work)
or sharing it with others (such as with my readers who have not purchased access to a paywalled publication) .
c.This is somewhat similar to my approach with Web3 is Going Great, which I published under a CC BY 3.0  license while also
separately stating that I do not wish for the content to be reused in NFT or other crypto projects. The question here will comedown to enforceability: frankly, I do not think this is a problem we can solve by simply asking AI companies nicely, and hopingthey are generous enough to comply with our requests. Many of these companies have shown nothing but contempt tothose who have created the works they have used without consent to train their models, and I don’t see how that willsuddenly change. If CC can establish a way to communicate these preferences and for creators to subsequently enforce
them, I will be very interested.
References
1.“FAQ: CC and NFTs ”, Creative Commons.
2.“Should CC-Licensed Content be Used to Train AI? It Depends. ” Creative Commons.
3.“Comment from Creative Commons”, published by the US Copyright Office.
4.“AI And Copyright: Expanding Copyright Hurts Everyone—Here’s What to Do Instead ”, EFF.
5.“Neither the devil you know nor the devil you don’t”, Cory Doctorow.
e.“If Creators Suing AI Companies Over Copyright Win, It Will Further Entrench Big Tech ”, TechDirt .
7.“Inaccessibility of CAPTCHA ”, W3C.
h.“You’re not imagining it, Captchas are getting harder ”, The Times .
Social share image is derived from “ VampireE3 ” (Carniphage, CC BY 2.0 ) and “ Schenker VIA14 Laptop asv2021-01” (A.Savin, Free Art
License ).


READ MORELicense
Loved this post? Consider signing up for a pay-what-you-want
subscription  or leaving a tip  to support Molly White's work, which is
entirely funded by readers like you. You can also check out the store!


Crypto reserves: no public good, no principles
The formerly anti-establishment bitcoin movement abandons its principles in favor of
number-go-up, applauds federal plan to stockpile seized crypto with no clear benefit to …
national interest11 Mar 2025
Issue 78 – President on brink of bailout for bitcoin
Trump tries to breathe life back into the crypto markets’ “Trump pump” while federal
regulatory agencies wash their hands of any crypto industry oversight
02 Mar 2025


Issue 77 – Whenever presidents get involved, if they become angry, you don't want to be there
A major crypto scandal tarnishes the reputation of Solana bigwigs, crypto influencers, and
Argentine President Javier Milei.
18 Feb 2025
Transcript of leaked call with Meteora’s Ben Chow


Citation Needed features critical coverage of the
cryptocurrency industry and of issues in the broadertechnology world.
It is independently published by Molly White, and entirely
supported by readers like you.
Subscribe
Archive
Recap issues
Podcast feed
Store
About
RSS
Tip jar
Privacy policyTwitter
Mastodon
Bluesky
YouTube
TikTok
Etc.
© 2025 Molly White.A transcript of a leaked call between DefiTuna’s Moty Povolotski and Meteora’s Ben Chow
18 Feb 2025


