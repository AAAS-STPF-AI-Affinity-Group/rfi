1 March 1 5, 2025  
Faisal D’Souza  
NCO/NITRD  
National Science Foundation 
2415 Eisenhower Avenue  
Alexandria, V A 22314  
Re: Request for Information (RFI) on the Development of an Artificial Intelligence (AI) Action  
Plan (“Plan”)  
Submitted by email to 
Dear Mr. D’Souza : 
We are  pleased to provide these comments regarding the Trump -Vance administration’s AI 
Action Plan.1  
Introduction  
Part I of t his document address es the ne ed to ensure that  advanced AI systems are designed and 
governed in service of American values.  That specifically includes  removing barriers to the 
development and widespread adoption of Western open -source models, as well as prioritizing 
accuracy , free speech , transparency , and explainability in model development , training , and 
outputs . Part II outlines ways that a strategic use of AI can supplement and restore reproductive 
health for the sake of human flourishing , including improving fertility outcomes, protecting the 
centrality of the doctor/patient relationship, data accountability, and promoting human -centered 
AI diagnostic tools.  
Part I: Prioritize American  values in AI design and g overnance  
EO 14179 calls for sustaining and enhancing “America’s global AI dominance in order to 
promote human flourishing , economic competitiveness, and national security. ”2 As Vice 
President Vance recently articulated in Paris, achieving that vision will require U.S. AI 
companies to maintain their technological edge while ensuring that the social diffusion of AI lifts 
all boats —in service of fundamental freedoms, American workers, and families.3 On the one 
hand, America must enact policies that help our leading edge AI firms exponentially enhance the 
*This document is approved for public dissemination. The document contains no business -proprietary or
confidential information. Document contents may be reused by the government in developing the AI Action Plan
and associated documents without attribution.
1 Federal Register , V ol. 90, No. 24 (Thursday, February 6, 2025), pp. 9088 -9089. 2025.
2 Federal Register , V ol. 90, No. 20 (Friday, January 31, 2025), 8741 -8742.
3 Wesley Hodges, “Vance Outlines an Open, Forward -Looking AI Agenda ,” Daily Signal , February 12, 2025,
https://www.dailysignal.com/2025/02/12/vance -outlines -open -forward -looking -ai-agenda/  (accessed March 15 ,
2025).


 
 
 
2 
 performance  (e.g., ‘intelligence’)  of their models .4 We must promote widespread adoption and 
integration of Western AI across a  vast range of applications  and use -cases.  But we must also 
work to ensure that the AI of the future reflect s American values  like openness, competition, 
transparency, and free speech.  
 
In pursuit of that shared vision , I offer the following recommendations for the administration’s 
consideration in developing the ir Plan.  
 
1. Bolster the development and proliferation of U.S. open-source AI 
 
Anti-competitive practices associated with Big Tech funded c losed -source foundation models  
threaten to undermine openness, decentralized control , and permissionless innovation .  
 
AI foundation models power a growing number of applications . However,  closed -source models 
are highly opaque —preventing dependent users ’ from identify ing the ideological biases and 
value -sets that underly model reasoning and outputs .5 This d earth of transparency and 
explainability could enable OpenAI or Google , for example,  to “shape a company’s practices, 
values, and public image in ways that are virtually impossible to detect, much less resist.”6  
 
Closed -source model opacity presents similar concerns for free speech. AI agents, search 
engines, or other applications built on top of th ose models reflect their biases —in terms of both 
the content they prioritize and exclude.7 Google’s experimental “AI mode ,” for example, directly 
answers user prompts in the overview box instead of directing users to third -party websites.8 AI 
search, chatbots, and agents have  the potential to enable greater productivity and free expression. 
But anchoring them to ideologically biased, closed -source AI models  is further entrench ing Big 
Tech ’s power to shape public discourse .9 Without transparen t and explainable open -source AI 
 
4 Performance benchmarks for LLMs —some of which are also classified as foundation models and frontier 
models —are key indicators of model “intelligence.” See LiveBench,  A Challenging, Contamination -Free LLM 
Benchmark , https://livebench.ai/#/details  (accessed March 15, 2025) ; Specific definitions and thresholds for 
classifying “frontier” and “foundation” models vary. However, a 2024 survey of AI model use for “intelligent 
transportation systems” helpfully defines and distinguishes the two. “Foundation models… are large, gener al-
purpose AI models that provide a base for a wide range of applications. They are characterized by their versatility 
and scalability.” In broad terms, “[f]rontier AI refers to the forefront of AI technology, encompassing the latest 
advancements, innovati ons, and experimental techniques in the field, especially AI foundation models and [large 
language models] LLMs.” See Mohamed R. Shoaib, Heba M. Emara, and Jun Zhao , “A Survey on the Applications 
of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems ,” January 12, 
2024, https://arxiv.org/html/2401.06831v1  (accessed March 15, 2025) .  
5 Jake Denton, “Big Tech’s AI Power Grab ,” March 11, 2024, https://www.heritage.org/big -tech/commentary/big -
techs -ai-power -grab (accessed March 15, 2025).  
6 Ibid. 
7 Daniel Cochrane, and Autumn Dorsey, “‘Green Computing ’ and Woke AI Is a Gift to China ,” March 7, 2025,  
https://www.heritage.org/china/commentary/green -computing -and-woke -ai-gift-china  (accessed March 15, 2025).  
8 Robby Stein, “Expanding AI Overviews and introducing AI Mode,” March 5, 2025, https://blog.google/ 
products/search/ai -mode -search/ (accessed March 15, 20 25).  
9 Danie l Cochrane, and Hans V on Spakovsky, “Don’t Let Big Tech Influence the Elections Yet Again This Year ,” 
May 3, 2024,  https://www.heritage.org/election -integrity/commentary/dont -let-big-tech-influence -the-elections -yet-
again -year (accessed March 15, 2025) ; Brian Chau, “ChatGPT’s score system shows political bias is no accident ,” 
UnHerd , December 20, 2022,  https://unherd.com/newsroom/chatgpts -score -system -shows -political -bias-is-no-
accident/  (accessed March 15, 2025) .  


3 models to power these and other applications , Big Tech censorship  could become even more 
prevalent and less detectable.  
In addition, a predominantly closed -source AI ecosystem undermines American dynamism by 
creating built -in advantage s for Big Tech incumbents over “little tech” disruptors .10 Martin 
Casado and Katherine Boyle of Andreessen Horowitz  point to how open -source technologies like 
Linux powered the internet revolution of the 1990s and 2000s.11 But they warn that Silicon 
Valley giants like Microsoft and Alphabet are attempting to “suppress open -source innovation 
and deter competitive startups ” under the guise of “safety” and “national security.”12  
Open -source models democratize access to AI capabilities, foster innovation across sectors, and 
enhance America's technological resilience against foreign competitors.  To unlock AI’s full 
potential and  ensure diffuse control of the technology , the administration should  implement the 
following recommendations:  
•Counter threats to  open -source  and “little tech :” The administration should specifically
address the most direct threats to open -source AI from domestic policies and foreign
governments. On the domestic front, proposed laws such as California ’s SB 1047 —vetoed in
2024 by Governor Newsom —would  have impose d liability on AI model developers for
“modifications [or misu se] of their AI models ” by third parties.13 Colorado enacted a similar
law last year.14 That law—and proposed  legislation  in other states —risk subjecting model
developers to ruinous liability if third parties modify or misuse their models for illicit
purpose s. Such broad , unpredic table liability discourages companies from developing
powerful AI models for public  use. But it particularly disincentivizes open -source AI . The
liability associated with  releasing open -source foundation model s under those conditions is
so great  that few i f any companies could afford t he risk . The administration should foreclose
that eventuality by calling on Congress to pass federal legislation preempt ing state regulation
of large, general purpose foundation models.15
10 Martin Casado , and Katherine Boyle , “AI Talks Leave ‘Little Tech’ Out ,” Wall Street Jo urnal , May 15, 2024 , 
https://archive.is/Jb5OE  (accessed March 15, 2025).  
11 Ibid. 
12 Ibid. 
13 See Alliance for the Future, “Newscard - SB 1047 Vetoed ,” September 29, 2024 , https://www.affuture.org/  
research/6 -sb1047newscard/  (acces sed March 15, 2025) ; Dean Ball, “AI and accountability: Policymakers risk 
halting AI innovation ,” Orange County Register , April 5, 2024 , https://archive.is/48PHj  (accessed March 15, 2025) ; 
Jake Den ton, “California’s Gift to Big Tech ,” Com pact Magazine , June 10, 2024, https://www.compactmag.com/  
article/californias -gift-to-big-tech/  (accessed March 15, 2025 ). 
14 Alex Siegal, and I van Garcia, “A Deep Dive into Colorado’s Artificial Intelligence Act ,” October 26, 2024,  
https://www.naag.org/attorney -general -journal/a -deep -dive-into-colorados -artificial -intelligence -act/ (accessed 
March 15, 2025).  
15 Federal legislation should preempt state laws that subject large  foundation model developers to liability if a third 
party use s or modifi es a general -purpose  foundation model for illicit activities . Policymakers could further tailor the 
measure to protect open -source AI models . For example, Congress could specifically preempt state l aws that creat e 
liability for developers of open -source foundation models based on potential use or modification of those model s by 
a third par ty. At the same time, federal preemption should  strike an appro priate balance —preserving states’ authority  
to pursue good -faith AI regulation in connection with specific contexts and use -cases.  


4 In addition, they should address similar threats from abroad  such as the European Union’s 
recently implemented AI Act.16 Like some of the state proposals, the AI Act requires 
“general purpose AI” providers to identify and mitigate “systemic risks .”17 This may force 
foundation model  developers  to implement Orwellian censorship measures (to address 
systemic risks) and keep  their model s closed -source .18 In response , the administration should 
seek full repeal of the AI Act or a t least, a broad exemption for U.S. open -source models.19 
The administration should also c onsider including language in U.S. negotiated trade and 
technology agreements to explicitly protect open -source AI firms from burdensome foreign 
regulations.  
•Create  a level playing field : Consider implementing  a gove rnment -wide framework  to
remove barriers to the purchase and use of open -source AI by federal entities , private
vendors, contractors , and government -supported research institutions. The framework could
achieve this, in part, through the following policies and directives:  First, require every federal
department, agency, and commission to  regularly identify and audit all policies or  regulations
that impact  government, vendor, and federal  grant recipient s’ use of AI. That includes IT
policies, procurement practices, and grant requirement s. Identify any that prohibit or
discourage the use of open -source AI  technologies .20 Second, for each barrier identified,
require a detailed cost -benefit analysis  justifying the restriction . Include potential cost
saving s, an assessment of technical benefits , and any cyber security -related implications of
switching to open -source for specific applications. Require additional expert validation of
potential cyber security or national security risks flagged .21
Third, for low-moderate risk applications (as defined by the framework), consider  requiring 
covered entities to identify and eliminate barriers preventing the use or integration of open -
source  AI—subject to reasonable security assurance standards . Fourth, for higher -
risk/sensitivity areas (e.g., defense or national security  applications ), create a streamlined 
process to propose and certify use-case-specific sandboxes authorizing use of or 
16 Jake Dent on, “The U.S. Shouldn’t Go the Way of Europe on AI ,” May 8, 2024,  https://www.heritage.org/big -
tech/commentary/the -us-shouldnt -go-the-way-europe -ai (March 15, 2025).  
17 “The AI Act defines systemic risk as ‘a risk that is specific to the high -impact capabilities of general -purpose AI 
models, having a significant impact on the Union market due to their reach, or due to actual or reasonably 
foreseeable negative effects on p ublic health, safety, public security, fundamental rights, or the society as a whole, 
that can be propagated at scale across the value chain.’”  See Jordi Calvet -Bademunt , and Joan Barata , “The Digital 
Services Act Meets the AI Act: Bridging Platform and AI Governance ,” May 29, 2024, https://www.techpolicy.press/  
the-digital -services -act-meets -the-ai-act-bridging -platform -and-ai-governance/  (accesse d March 15, 2025) .  
18 Ibid. 
19 An exemption for open -source would help ensure that “little tech” AI competitors are not penalized  by EU rules 
when  they create AI models that compete directly with Big Tech closed -source foundation models.  
20 The Federal Risk and Authorization Management Program (FedRAMP)  is a key example. See FedRAMP, 
Program Ba sics, https://www.fedramp.gov/program -basics/  (accessed March 15, 2025).  
21 Some of the purported risks attributed to op en-source AI may be overblown , and potential  benefits under -
explored . See for example  Rishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin  
Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, and Percy Liang , “Considerations 
for Governing Open Foundation Models ,” December 13, 2023,  https://hai.stanford.edu/policy/issue -brief -
considerations -governing -open -foundation -models  (accessed March 15, 2025) ; Jasmin Léveillé , “Embrace Open -
Source Military Research to Win the AI Competition ,” October 16, 2019, https://warontherocks.com/2019/10/  
embrace -open -source -military -research -to-win-the-ai-competition / (accessed March 15, 2025).  


5 experimentation with open -source AI —under specific conditions.  Similar  affordances could 
be extended to government vendors, contractors, and affiliated research institutions  subject to 
federal cyber secu rity standards and other rules.  Fifth, consider how and to what degree the 
U.S. Government  can leverage its purchasing power, grant -making, and other tools to invest 
in open -source, dual-use AI technologies with national security and commercial/civilian 
applications.  
Finally, the framework should require a broader cost-benefit analysis for rulemaking, 
regulatory guidance , and “soft” technical standards issued by government entities such as the 
Office of Management and Budget (OMB), the U.S. Office of Personnel Management 
(OPM), and the National Institute of Standards and Technology (NIST) —to ensure  a fair, 
open, and competitive playing field for open -source AI technologies.    
Address Big Tech’s anti -competitive  practices : Direct the F ederal Trade Commission (FTC)  
and the U.S. Department of Justice (DOJ)  to investigate whether Big Tech companies are 
engaged in anti -competitive practices to undermine competition from open -source 
competitors. For example, the FTC and DOJ should consider whether Big Tech’s privileged 
membership on government advisory bodies —or their coordination via industry associations 
and NGOs  to oppose open -source AI—constitute illegal collusion and/or anti -competitive 
practices.   
The DOJ and FTC  should also investigate whether Big Tech firms are engaged in unfair or 
deceptive trade practices by claiming that certain models (e.g., Meta’s Llama) are “open -
source” while continuing to maintain model  guardrails , using licenses to restrict use  or 
modification , and refusing to disclose training data.22 They should f urther  examine how AI 
companies  restrict users from modifying or utilizing partially open -source or closed -source 
AI tools for certain use-cases  through their acceptable use policies (AUPs) .23 Specifically, 
whether model developer AUPs constitute  illegal restrain ts on trade that undermine 
competition and hinder the use of AI to engage in expression protected by the First 
Amendment .24 
22 Ed Gent, “The tech industry can’t agree on what open -source AI means. That’s a problem ,” March 25, 2024, 
https://www.technologyreview.com/2024/03/25/1090111/tech -industry -open -source -ai-definition -problem/  
(accessed March 15, 2025) ; The Economist, “A battle is raging over the definition of open -source AI ,” Novem ber 6, 
2024, https://archive.is/YKNFC  (accessed March 15, 2025); Stefano Maffulli , “Meta’s LLaMa license is not Open 
Source ,” July 20, 2023, https://opensource.org/blog/metas -llama -2-license -is-not-open -source  (accessed M arch 15, 
2025).  
23 For example, the Open Source  Initiative, a key standard -setting body for open -source software, pointed out that 
Meta’s Llama AI model did not meet their standard for open -source, in part, because “it puts restrictions on 
commercial use for some users (paragraph 2) and also restricts the use of the model and software for certain 
purposes ” under its AUP.  See Stefano Maffulli , “Meta’s LLaMa license is not Open Source ,” July 20, 2023, 
https://opensource.org/blog/metas -llama -2-license -is-not-open -source  (accessed M arch 15, 2025).  
24 Kevin Klyman , “Acceptable Use Policies for Foundation Models ,” (date omitted  from publication ), 
https://crfm.stanford.edu/2024/04/08/aups.html  (accessed March 15, 2025) . 


 
 
 
6 
 2. Champion the freedom of expression and open-source AI in global standard -setting 
 
Prioritize the freedom of expression in the development of international AI standards backed by 
the U .S. Whereas the prior administration emphasized “AI safety,” reining -in “bias,” and 
ensuring “fairness,” the current administration should lead in the creation and dissemination of 
international standards and best practices that drive open, transparent, and explainable AI.25 That 
could include the following:  
 
• Establish criteria for classifying  AI models on a spectrum from “maximally”  open -source  to 
fully closed -source.26  
 
• Create a framework  to encourage and guide leading -edge Western AI companies to open -
source elements of their advanced foundation models.27 This could include recom mendations 
for the creat ion of  regulatory safe harbors and sandboxes for open -source AI technologies.  
 
• Develop  transparency framework s for model  reinforcement learning from human feedback 
(RHLF) , fine -tuning, and the datasets used in those processes  to identify and disclose 
ideological biases introduced into foundation models by developers.28  
 
• Explore cutting -edge technical methods for enhancing model explainability without 
sacrificing accuracy or performance.  
 
Create transparency and explainability benchmarks both for foundation models and more 
granular AI applications /use-cases.29  
 
Part II: Positive applications of AI in promoting fertility and reproductive health   
 
The administration’s prioritization of human flourishing as the vehicle through which global AI 
dominance may be achieved points to an important reality. There are two possible paths before 
the United States today: one path prioritizes AI innovation and domina nce in ways that 
 
25 Kara Frederick, and Jake Denton, “The U.S., Not China, Should Take the Lead on AI ,” October 11, 2024, 
https://www.heritage.org/big -tech/commentary/the -us-not-china -should -take-the-lead-ai (accessed March 15, 2025).  
26 David Gray Widder , Sarah West , and Meredith  Whittaker , “Open (For Business): Big Tech, Concentrated Power, 
and the Political Economy of Open AI ,” August 18, 2023 , 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807  (accessed March 15, 2025) ; 
https://opensource.org/osd ; https://arxiv.org/pdf/2302.04844 .  
27 An early example of this was  “[OpenAI ’s release ]” of “a smaller version of their large language model GPT -2 in 
2019. ”  See https://www.heritage.org/big -tech/commentary/the -us-not-china -should -take-the-lead-ai; The framework 
could specifically address national and cyber security concerns, as well as new prospects for economic  growth, 
technological innovation, and  competition  driven by an open -source AI ecosystem. It could also address  the role of 
open -source in fostering greater transparency and explainability in advanced AI systems.  
28 See for example  Angèle Christin, “Tuning Our Algorithmic Amplifiers: Encoding Societal Values into Social 
Media AIs,” October 20, 2023, https://hai.stanford.edu/news/tuning -our-algorithmic -amplifiers -encoding -societal -
values -social -media -ais (accessed March 15, 2025) ; https://arxiv.org/abs/2307.09288 ; https://www -
cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf .  
29 A primary goal of AI transparency and explainability must be to ensure users  have reasonable knowledge of 
whether the automated systems and the outputs produced by those systems align with their interests and values.  


 
 
 
7 
 circumvent or substitute the human person, and the other path channels AI innovation and 
dominance in ways that restore human health and supplement the human doctor/patient 
relationship by treating AI as a “decision support tool.”30 By placing human flourishing at the 
center of AI innovation and development, the administration signals that AI should be utilized as 
a tool in service of human flourishing, specifically as it relates to family formation and fertility 
outcomes in the Unit ed States. We couldn’t agree more.  
 
As many people struggle to find romantic partners or conceive children, advances in technology 
from AI and robotics to assisted reproductive technology promise men and women technological 
solutions to these innate desires. AI’s use in human biological repr oduction presents as many 
positive opportunities as it does possible abuses. Given this, it is important to consider ways that 
a strategic use of AI can supplement and restore human health for the sake of human flourishing.  
 
Promote AI diagnostic tools to identify, diagnose, and treat infertility  
 
Infertility is a heartbreaking condition affecting 16% of couples in the United States. Oftentimes, 
infertility doctors define infertility as a “disease” based on a couple’s inability to conceive 
children after 6 -12 months of barrier -free intercourse. Infe rtility, however, is not a disease in 
itself; it is a symptom  of underlying reproductive health conditions. For women, reproductive 
health conditions include endometriosis, adenomyosis, polycystic ovary syndrome (PCOS), 
blocked fallopian tubes, and hormona l imbalances. For men, they include low sperm count, low 
sperm motility, erectile dysfunction, and diet, lifestyle, and environmental factors. Men and 
women on the whole bear the burden of infertility equally, with one -third of cases due to women, 
one-third due to men, and the remaining one -third due to combined causes.31 Moreover, studies 
estimate that a couples’ diagnosis of infertility results from four or more reproductive health 
conditions, underscoring the need for comprehensive diagnostic approaches. There is no one -
size-fits-all solution to infertility.32  
 
 
30 Simon Coghlan, et al, “Ethics of artificial intelligence in prenatal and pediatric genomic medicine.”  Journal of 
community genetics  vol. 15, 1 (2024): 13 -24. doi:10.1007/s12687 -023-00678 -4 
31 “How Common Is Male Infertility, and What Are Its Causes? ” Eunice Kennedy Shriver National Institute of Child 
Health and Human Development, U.S. Department of Health and Human Services, 18 Nov. 2021. 
www.nichd.nih.gov/health/topics/menshealth/conditioninfo/infertility#:~:text=Overall%2C%20one%2Dthird%20of
%20infertility,combine%20with%20a%20woman’s%20egg .  
32 Stanford JB, Parnell T, Kantor K, Reeder MR, Najmabadi S, et al, International Natural Procreative Technology 
Evaluation and Surveillance of Treatment for Subfertility (iNEST): enrollment and methods. Hum Reprod Open. 
2022 Aug 9;2022(3):hoac033. doi: 10.1 093/hropen/hoac033. PMID: 35974874; PMCID: PMC9373967. 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9373967/ .  


8 As Dr. Casey Means notes, since the 1970’s, there has been a 1% increase  in miscarriage33 and 
erectile dysfunction34 and a 1% decrease  in sperm count35, testosterone36, and total fertility per 
year.37 Moreover, an individual’s reproductive health is strongly correlated with their overall 
metabolic and cardiovascular health. Prioritizing treatments for reproductive conditions ensures 
that persons are not only able to have the children they desire, but a lso live longer, healthier 
lives.38 
The complexity of these reproductive health conditions, and the lack of research and 
development devoted to them, means that 15% - 30% of cases are due to “unexplained 
infertility,” making this the leading diagnosis doctors give men and women. Given advanc ements 
in AI, this diagnosis should be unacceptable. The administration should prioritize uses of AI as a 
diagnostic tool that can analyze large data sets to identify, diagnose, and offer personalized 
treatment plans to heal reproductive health conditions and infertility, all while using AI as a 
“decision support tool” that promotes human -centered AI.  
•Improve fertility outcomes : In partnership with the Food and Drug Administration, the
National Science Foundation, and the National Institutes for Health, the administration
should fund AI research for improving fertility outcomes and early diagnostics, specifically
by a) analyzing l arge data sets that incorporate a person’s overall health and biomarkers to
assess the underlying reproductive health conditions resulting in a diagnosis of infertility.
This would include prioritizing AI diagnostics in fields s uch as endometriosis research where
it takes, on average 6 -11 years to receive a diagnosis and is a leading cause of infertility for
women. Moreover, the administration should direct these entities to b) develop accessible
tools that provide fertility insi ghts using AI -powered reproductive health tracking and early
fertility diagnostics, while ensuring strict privacy protections. Currently, analogous devices
include the Oura Ring, and health tracking devices.
•Establish an AI ethics board to evaluate applications combining AI and reproductive health :
In partnership with the Food and Drug Administration, the National Science Foundation, or
another relevant entity, the administration should establish an ethics board to review potential
risks and benefits to the use of AI in treating reproductive health, including concerns such as
patient privacy, informed consent of data, maintaining the centrality of doctors in the
diagnostic process, and ensuring that reproductive health prioritizes treatments that heal the
underlying root causes of infertility. Using a n ethical framework established by Dr. Joshua
Mitchell of Georgetown, AI innovations in line with human flourishing must prioritize
33 Lang, Kevin, and Ana Nuevo -Chiquero. “Trends in self -reported spontaneous abortions: 1970 -2000 .” 
Demography vol. 49,3 (2012): 
34 Ayta, I A et al. “ The likely worldwide increase in erectile dysfunction between 1995 and 2025 and some possible 
policy consequences .” BJU internationa l vol. 84,1 (1999): 50 -6.  
35 Levine, Hagai et al,. “ Temporal trends in sperm count: a systematic review and meta -regression analysis of 
samples collected globally in the 20th and 21st centuries .” Human Reproduction Update, V olume 29, Issue 2, 
March -April 2023, Pages 157 –176.  
36 Harding, Anne. “ Men’ s Testosterone Levels Declined in Last 20 Years | Reuters .” Reuters, 9 Aug. 2009, 
www.reuters.com/article/idUSKIM   
37 “Fertility Rate, Total (Births per Woman) .” World Bank Open Data, 
data.worldbank.org/indicator/SP.DYN.TFRT.IN. Accessed 27 Feb. 2025.  
38 Kurt T. Barnhart, Introduction: Fertility as a window to health , V ol. 110 Fertility and Sterility, p. 781 -782 (2018). 


 
 
 
9 
 developments that supplement human health and fertility, rather than uses of technology that 
substitute the human person altogether.  
 
• Prohibit the sale of reproductive data: Utilizing the Federal Trade Commission, the 
administration should ensure that AI -driven reproductive health applications, fertility 
tracking devices, genetic testing companies, and fertility clinics do not sell or misuse 
consumer data. Moreover, such entities should provide clear and understandable information 
for the purpose of informed consent on how a user’s/patient ’s data may be used prior to 
collecting or using such insights.  
 
• Promote human -centered AI diagnostic  tools: The administration should pursue 
developments in AI diagnostics and treatment plans for reproductive health conditions and 
infertility that reinforce, rather than replace, licensed medical professionals. In partnership 
with the Food and Drug Administration , applications for genomic AI research and device 
development should require the verifiable involvement of qualified medical professionals 
who can verify and review the conclusions of AI diagnostic tools. While h umans and AI may 
make mistakes, it is essential for human flourishing that humans remain at the center of AI 
diagnostic tools. AI diagnostics should promote patient -centered informed consent, resources 
to find the best doctor for their specific condition and  reinforce the importance of the 
patient/doctor relationship to maintain the human element of medicine. In this way, the use of 
AI as a diagnostic tool will aid doctors in their ability to identify and diagnose reproductive 
health conditions such that th ey have additional time to prioritize the patient relationship and 
overall health.  
 
By enacting these policies, we can harness AI’s potential to revolutionize restorative medicine 
that promote human flourishing, especially in the realm of family formation and fertility.  
 
Conclusion  
 
The Trump -Vance administration has the opportunity to  ensure that AI innovation remains firmly 
rooted in American values —openness, competition, free speech, and transparency —while also 
advancing AI applications that meaningfully enhance human flourishing. By prioritizing policies 
that foster the development of open -source AI, promote fair competition, and counteract 
ideological censorship, the administration can cement America’s leadership in AI innovation and 
governance.  
 
At the same time, AI’s transformative potential extends beyond governance and economic 
growth; it can also play a crucial role in restoring and supplementing human health, particularly 
in the realm of reproductive medicine. By strategically leveraging AI t o improve fertility 
outcomes, protect the doctor -patient relationship, ensure data accountability, and promote 
human -centered diagnostics, the administration can align AI policy with the fundamental goal of 
strengthening families and ensuring that technolo gy serves, rather than supplants, the human 
person.  
 
The administration’s commitment to AI as a tool for human flourishing signals a vision for a 
future in which innovation and ethics go hand in hand. By enacting policies that both defend 


 
 
 
10 
 American technological leadership and harness AI’s capabilities to support strong families and 
better health outcomes, we can build an AI ecosystem that upholds our deepest values while 
securing a more prosperous and humane future.  
 
Thank you for inviting and considering our comments.  
 
 
Sincerely,  
 
Daniel Cochrane * 
Senior Research Associate , Tech Policy Center  
The Heritage Foundation  
214 Massachusetts Avenue, NE  
Washington, DC 20002  
 
  
 
Emma Waters * 
Policy Analyst, Tech Policy Center  
The Heritage Foundation  
214 Massachusetts Avenue, NE  
Washington, DC 20002  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
* I file this comment in my individual capacity rather than as an employee of the Heritage Foundation; information 
regarding my institutional affiliation is provided for informational purposes only.  


