1 Disclaimer: 
This document is approved for public dissemi nation. The document contains no business-
proprietary or confidential information. Docume nt contents may be reused by the government in 
developing the AI Action Plan and associated documents without attribution. 
============================= 
Dear members of the OSTP and NITRD NC, I am a participant in an international activist co llective which could be considered the political 
extension of a large group of AI risk researchers who are extremely conc erned about the mid and 
long term consequences of future AI progress (opinions  vary, but most in that particular field expect 
this to hit us in between 2 and 15 years). These risks are predicted by a set of theories whic h together constitute what we call “AI Alignment 
theory”, which has been explored and developed by AI pioneers for the better part of the last 20 
years. What started out as a mainly theoretical and philosophical endeavor – the merit of this theory 
was not directly testable – ha s through the recent emergence of AIs based on Large Language 
Models turned into a theoretical framework whose predictions are now increasingly experimentally 
verified  and documented in AI labs. 
The main risk mechanic we are concerned over can be summarized as the following: 
Many experts – and this generall y includes the leaders of front ier AI technology companies – 
believe that it is likely that one of them will soon create what we call “Artificial Superintelligence” 
(ASI), which is loosely defined as  an artificial system, running on computers, which vastly outstrips 
humans across most domains of mental prob lem solving work including IQ, creativity, 
mathematical ability, strategic games, psychological manipulation and theoretical research work. 
The creation of such machines is the explicit goa l of companies like OpenAI, Anthropic, xAI etc. 
Such systems by definition will be able to “beat humans at most of their human games” meaning 
that as long as they have channels  to interact with the rest of the world (and this is a strict 
requirement for their effective use), they are expected to leverage their superior problem solving capabilities into escaping our control, if they have any incentive for doing so. 
As it turns out, most arbitrary problems we would want such a machine to solve intrinsically 
incentivize powerful problem solving machines to first maximize their level of control over their 
environment in order to then “optimize” said environment (the world) in their pursuit of the most 
efficient solution. This generally implies: 
- breaking the restrictions and control mechanisms AI developers and regulators have put in place
- using hack
ing and soc ial engin eering in pursuit of more control
- creating and hiding copies of th emselves, using access to the internet, human collaborators and
other means- hijacking technical infrastructure, blackmailing or  convincing individuals in influential positions
to support the AIs goals- and generally any other conceivable strategy that could help w ith achieving a broad range of goals
The field of AI Alignment further suggests that A: almost any non-trivial conceiva ble task we can give to a supe rintelligent AI has disastrous 
consequences for humanity if pursued dili gently by a very efficient optimizer. 
B: it is incredibly difficult to 


2 1. define tasks in such a way that this is not the case and won’t be the case in the future (the value
definition problem)
2. create and preserve a reliable way to monitor which goals any AI is act ually optimizing for (the
interpretability problem)3. build powerful AIs that allow you to change th eir initial tasks or goals into new ones (the
corrigibility problem)
Many experts in the field believe that building ASI while failing to develop solid solutions to these 
problems is very likely to result  in a catastrophe that would ultim ately kill or enslave all humans on 
earth. 
While there has been some progress with some of  these problem domains, all are ultimately still 
mostly unsolved, and we are quickly running out of ti me to solve them while the main incentives of 
the industry lie in pursuing improving these system s capabilities instead of theoretical and practical 
safety research. 
We are currently not on track of preventing this catastrophe and need to hope that whichever lab builds the first ASI also magically produces a soli d solution to most of th ese wickedly difficult 
problems. 
Meanwhile, economic and geopol itical reasons create perverse incentives for countries and 
corporations to pursue the creation of stronger AIs at the expense of safety, resulting in “a race to 
the bottom” that likely is to everyone’s detriment. 
This means local policies are unlikely to be effe ctive, meaning some form of consistent global 
cooperation would be required to improve our chance of survival. 
The best way to achieve this might be in the form of a global treaty that restricts capabilities research, dedicates resources to solving the AI Alignment problem and limits the amount of 
computing resources that can be assigned to th e training and use of advanced AI systems. 
We are aware that such a solution would be incredibly difficult to agree on, implement and enforce 
on a global scale, but many of us are convinced that better solutions currently don’t exist. 
The main point of this pursuit w ould be to essentially buy time for Alignment/safety research to 
catch up. 
We are all in this together, 
Maximilian Theodor Kircher 
2025/03/15, Germany 


