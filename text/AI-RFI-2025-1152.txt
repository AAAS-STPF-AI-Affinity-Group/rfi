PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 12, 2025
Status: 
Tracking No. m 85-gsll-1xia
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1152
Com m ent on FR Doc # 2025-02305
Submitter Information
Nam e: Hari Chidam baram
Em ail:  
General Comment
I think there needs to be consideration of what AGI (Artificial General Intelligence) really m eans and the danger of creating AIs sm arter
than us in every way. AGI could m ean that every hum an is out of a job (given that the AI can do anything that a hum an can do), and if it is
not aligned with hum an interests, could also result in com plete extinction. This is in addition to the concerns of power dynam ics. I suggest
global coordination to prevent race dynam ics, resulting in safer developm ent; banning open source AI to prevent dangerous actors from
jailbreaking the m odels and any actor from  im proving it (to potentially dangerous levels); and sim ple, clear, transparency requirem ents for
leading AI labs so we can see what is going on. Nationalization of AI m ight be needed at som e point.
Possible further resources:
Superintelligence FAQ--https://www.lesswrong.com /posts/LTtNXM9shNM9AC2m p/superintelligence-faq
AI Alignm ent video--https://youtu.be/EUjc1WuyPT8?si=Nl3sD6Ybz3KFwrdF
Zvi AI posts--https://www.lesswrong.com /posts/kqz4EH3bHdRJCKMGk/ai-106-not-so-fast


