Before the 
OFFICE OF SCIENCE AND TECHNOLOGY POLICY 
Washington, D.C. 20504 
In the MaƩer of Request for InformaƟon on the Development of an ArƟﬁcial 
Intelligence (AI) AcƟon Plan, Federal Register Number 2025-02305  
(Released March 15, 2025) 
Comments of the American Consumer InsƟtute 
The American Consumer InsƟtute Center for CiƟzen Research (ACI) is a nonproﬁt 501 
(c)(3) educaƟonal and research insƟtute with the mission to idenƟfy, analyze, and protect the 
interests of consumers in legislaƟve and rulemaking proceedings. ACI submits these comments 
in response to a request for informaƟon (RFI) on the development of an arƟﬁcial intelligence 
(AI) AcƟon Plan.  
Pro-Freedom AI Leadership 
The advent of new and emerging advanced computaƟon and arƟﬁcial intelligence 
technologies have the power to augment and improve healthcare, educaƟon, ﬁnance, 
transportaƟon, and more. But now, a budding bureaucracy is forming around AI technologies 
and their uses. With AI innovaƟon under threat of burdensome and redundant compliance 
costs, a strong acƟon plan is necessary to ensure American leadership. A strong acƟon plan 
must recognize where legislaƟon is necessary and where the government should let the market 
lead. Where states seek to regulate algorithmic bias, the Oﬃce of Science and Technology Policy 
should understand that miƟgaƟng algorithmic bias is already in the best interest of innovators, 
who have been working to address the problem for decades. ExisƟng and burgeoning risk 


2 management and regulatory frameworks are too vague and ought to be reformed if AI 
developers are to adhere to them as a plausible rebuƩal against noncompliance.  
Curbing needless regulaƟons should be a priority. The Trump administraƟon can signal 
to Congress through an execuƟve order to take a light-touch approach to AI regulaƟon. A strong 
acƟon plan should help innovators through digiƟzing government records and by creaƟng a 
federal sandbox or learning laboratory. It should also work with Congress on two separate 
federal frameworks on data privacy and arƟﬁcial intelligence. For its part, Congress should learn 
from mistakes in the states that suggest trying to do both at once can be messy and 
complicated. By implemenƟng these suggesƟons, the OSTP can build a robust AI AcƟon Plan to 
ensure the AI sector can ﬂourish in America. 
The American Consumer InsƟtute would like to thank the Oﬃce of Science and 
Technology Policy (OSTP) for seeking public comment on how it should conduct an AI AcƟon 
Plan. 
LegislaƟng and RegulaƟng for Algorithmic Bias 
Policymakers and regulators are right to consider the impact of arƟﬁcial intelligence on 
consumer safety and wellbeing. SƟll, they wrongly seek to solve a problem that the private 
sector is already addressing. Researchers and businesses have been working to correct biases 
for decades—and they conƟnue to do so.1 Large technology companies like IBM, Amazon, and 
Google have enƟre teams dedicated to AI fairness and bias reducƟon.2 Such a consorted eﬀort 
to address the issue is likely part of the reason there is no deﬁniƟve proof that AI is more biased 
1 Alexandra George, “ThwarƟng Bias in AI Systems, Carnegie Mellon University’s College of Engineering,” December 11, 2018, 
hƩps://engineering.cmu.edu/news-events/news/2018/12/11-daƩa-proxies.html ; Miana Massey, Maryland Researchers 
Working to Correct PotenƟal Bias in ArƟﬁcial Intelligence, CBS News, February 20, 2023, 
hƩps://www.cbsnews.com/balƟmore/news/maryland-researchers-working-to-correct-potenƟal-biases-in-arƟﬁcial-intelligence/ . 
2 R.K.E. Bellamy, et al, "AI Fairness 360: An Extensible Toolkit for DetecƟng and MiƟgaƟng Algorithmic Bias," IBM Journal of 
Development  63, no. 4/5 (July-September 2019): 4:1-4:15), hƩps://ieeexplore.ieee.org/document/8843908 ; Amazon News, 
"How Amazon Works to Make Machine Learning Tools Fairer for Everyone," About Amazon, Last visited March 15, 2025, 
hƩps://www.aboutamazon.com/news/devices/how-amazon-works-to-make-machine-learning-tools-fairer-for-everyone ; and 
Google Research, "Responsible AI," Google Research, Last visited March 15, 2025, hƩps://research.google/teams/responsible-
ai/. 


3 than human decision making. To the extent federal agencies, state legislatures, or industry seek 
to miƟgate bias, results should be benchmark against human error, instead of a utopian vision 
of a world with no error. But the best approach would be to let models improve organically 
through trial-and-error processes in the private sector like has occurred for decades.  
Many federal agencies also established their own internal protocols to limit AI bias as a 
part of the Biden administraƟon’s ExecuƟve Order 14110 on the Safe, Secure, and Trustworthy 
Development and Use of ArƟﬁcial Intelligence. The Trump administraƟon rightly repealed that 
order, which mainly eliminated the reporƟng requirements on fronƟer models, and asked 
departments and agencies to “revise or rescind all policies, direcƟves, regulaƟons, orders, and 
other acƟons taken under the Biden AI order that are inconsistent with enhancing America’s 
leadership in AI.”3 A Trump White ExecuƟve order in late January rightly directed OMB Director 
Russell Vought to revise Memoranda M-24-10 in accordance with “America’s global AI 
dominance in order to promote human ﬂourishing, economic compeƟƟveness, and naƟonal 
security.”4 OMB’s original mandates were overly broad and unintenƟonally vague. They 
conﬂicted with the Trump administraƟon’s stated agenda for permissive AI innovaƟon. And its 
rules would slow the pace of AI adopƟon at government agencies. Agencies should instead be 
directed to enforce already exisƟng laws as they apply to AI and to avoid broad regulatory 
decrees. 
Keep the AI Risk Management Framework Voluntary and Leave Room for 
Reform 
In many state AI fairness proposals, policymakers provide an aﬃrmaƟve defense in court 
if companies prove they acted in accordance with the AI Risk Management Framework (RMF)5 3 hƩps://www.whitehouse.gov/fact-sheets/2025/01/fact-sheet-president-donald-j-trump-takes-acƟon-to-enhance-americas-ai-
leadership/   
4 Oﬃce of Management and Budget, "Advancing Governance, InnovaƟon, and Risk Management for Agency Use of ArƟﬁcial 
Intelligence," M-24-10, White House, March 28, 2024, hƩps://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-
Advancing-Governance-InnovaƟon-and-Risk-Management-for-Agency-Use-of-ArƟﬁcial-Intelligence.pdf ; 
hƩps://www.whitehouse.gov/presidenƟal-acƟons/2025/01/removing-barriers-to-american-leadership-in-arƟﬁcial-intelligence/  
5 NaƟonal InsƟtute of Standards and Technology, "AI Risk Management Framework," NIST, Last visited March 15, 2025, 
hƩps://www.nist.gov/itl/ai-risk-management-framework . 


4 that was developed under the NaƟonal InsƟtute for Standards and Technology (NIST) during the 
Biden administraƟon. Importantly, complying with the RMF does not subsƟtute for compliance 
with the law itself. To receive the defense in court, industry must prove that they both cured 
violaƟons within the state law itself (thus complying with the provisions of the law) and that 
they complied with the NIST RMF. Both condiƟons must be saƟsﬁed. Think of the NIST AI RMF 
more like a ﬂoor that states then build atop. The RMF therefore does not subsƟtute for 
compliance, but it does provide a guiding north star as a stand-in for the best pracƟces 
businesses are encouraged to adopt in the AI ecosystem—making its development important as 
an iteraƟve guide for AI development. 
Unlike the widely respected, narrowly focused, and humble NIST privacy framework6—
which beneﬁted from decades of debate and research on normal and voluntary industry 
standards and best pracƟces for protecƟng privacy—the AI RMF is vague, open-ended, 
ambiguous, and complex. It does not have the beneﬁt of concrete direcƟves nor the beneﬁt of 
learning from decades of privacy norms.7 The AI RMF is much more broad and diﬃcult to 
comply with, which limits its eﬀecƟveness as a defense in court. AŌer all, compliance with 
vague and open-ended rules is by deﬁniƟon a maƩer of interpretaƟon and discreƟon. If 
businesses decide the uƟlity of compliance is minimal and that compliance with its vague 
suggesƟons is diﬃcult then they will likely just ignore it. 
The AI Risk Management Framework sƟll needs Ɵme to mature. It should be narrowly 
rewriƩen in a way that gives businesses guidance on how to miƟgate concrete, not hypotheƟcal, 
risks. And it must naturally develop over Ɵme as AI becomes further integrated into private 
business acƟviƟes as norms around using the technology naturally develop. Federal agencies 
should therefore make it clear that AI RMF has value because it is voluntary and iteraƟve. 
Agencies should also make it clear that states should not be using it as a means of creaƟng de-
facto law. None of this is to suggest that the AI RMF does not have value. It can be a helpful 6 Katharina Koerner, StandardizaƟon Landscape for Privacy: Part 1—the NIST Privacy Framework, InternaƟonal AssociaƟon of 
Privacy Professional, December 1, 2021, hƩps://iapp.org/news/a/standardizaƟon-landscape-for-privacy-part-1-the-nist-privacy-
framework/ . 
7 Dean W. Ball, "Here’s What I Think We Should Do about AI Policy," Hyperdimensional, November 14, 2024, 
hƩps://www.hyperdimensional.co/p/heres-what-i-think-we-should-do .  


5 guide to industry standards and best-pracƟces—but its value diminishes as it becomes 
compulsory. 
Make Federal Data More Accessible 
GeneraƟve AI runs algorithms to establish paƩerns based on copious amounts of 
collected data that it then turns into predicƟve uses.  BeƩer data access means more accurate 
predicƟons. Regulators, therefore, have a vested interest in streamlining rules and regulaƟons 
that slow the disseminaƟon of informaƟon that can be used for model training. But the federal 
government sits on vast troves of useful—but inaccessible or unusably formaƩed—data for 
model training. Federal agencies should start by making privacy protected, de-idenƟﬁed 
healthcare data available for AI training and then extend that into other domains like ﬁnancial 
and environmental data. Dandelion Health is already ﬁlling the market need for large, de-
idenƟﬁed healthcare data in the private sector.8 Agencies should complement those eﬀorts by 
making its privacy-protected data more available. Then the government should prioriƟze 
digiƟzing legacy records and unstructured data that can be used for processing so that the 
informaƟon can be made accessible to consumers, innovators, and government workers 
through arƟﬁcial intelligence applicaƟons. 
Data limitaƟons are parƟcularly burdensome to small startups and ﬂedgling AI ﬁrms with 
fewer resources to invest in data procurement. As the nonproﬁt Engine FoundaƟon notes, 
“government possesses troves of data useful for AI, especially for tailored, specialized models.”9  
Governments should make that data more readily available and more easily accessible. Even 
small data access improvements could lead to large impacts for startups. The AI startup 
Benchmark Labs, for example, relies on government data sources to provide farmers with real-
8 KaƟe Jennings, How This Startup Is Using 10 Million PaƟent Records to Reduce Bias in Healthcare AI, Forbes, December 21, 
2023, hƩps://www.forbes.com/sites/kaƟejennings/2023/12/21/how-this-startup-is-using-10-million-paƟent-records-to-reduce-
bias-in-healthcare-ai/?sh=70c073f9351d . 
9 Nathan Lindfors, “The Pro-Startup Policy Agenda to Power AI InnovaƟon,” Engine, March 4, 2025, 
hƩps://www.engine.is/news/category/the-pro-startup-policy-agenda-to-power-innovaƟon-and-win-the-ai-race . 


6 Ɵme data about micro-climate weather that aﬀects crop yields and agricultural output.10 Due to 
data handling and postprocessing constraints, the government data is only available with a lag.  
Benchmark Labs must instead turn to more expensive and complex server data.11 The federal 
government should take steps to improve documentaƟon and make quality, aﬀordable data 
more accessible. 
Collaborate with Congress on a Data Privacy Framework 
Data privacy laws reduce data access, which is criƟcal for model accuracy and bias 
miƟgaƟon. That makes model creaƟon and ﬁnetuning more expensive and complicated. 
Diﬀerent AI applicaƟons will require access to diﬀerent types of data. Fortunately, since data 
diﬀers by type and by rule, the United States is well posiƟoned to build out its already exisƟng 
sectoral approach to privacy, which has already contributed to the globally vibrant American 
technology sector by keeping the rules light-touch. Where diﬀerent rules can be tailored to ﬁt 
diﬀerent kinds of data without sƟﬂing innovaƟon, that should be done. SƟll, the United States 
needs a federal framework on privacy if only to pre-empt the growing patchwork of state data 
privacy laws that threatens to undermine AI adopƟon by startups and small AI businesses that 
rely on speciﬁc data types or that would have trouble complying with diﬀerent rules in many 
jurisdicƟons. State laws on the maƩer are oŌen binary, opaque, and could undermine an AI 
sector trying to ﬁll market needs and meet consumer demands.  
A federal standard on consumer data privacy could coordinate this mishmash of rules 
into one uniﬁed framework that does not complicate data access for industry on the AI 
fronƟer—but only if that framework is clear, concise, and narrowly tailored so that it is not 
stretched beyond its original intent. California provides a cauƟonary tale. It empowered a state 
agency to interpret and enforce its state consumer data privacy law, which it has used to apply 
expensive administraƟve burdens and privacy-invasive protocols to automated decision-making 10 Benchmark Labs, “AI-Ready Open Data Assets RFI, DOC-2024-0007,” 
hƩps://staƟc1.squarespace.com/staƟc/571681753c44d835a440c8b5/t/6695422d489f5e3a8bd641f7/1721057837134/Data+ava
ilability+RFI_+Benchmark+Labs.pdf   
11 Ibid. 


7 technologies.12 The Trump administraƟon should work with Congress to keep a naƟonal 
framework narrowly tailored so that it is not inadvertently applied to arƟﬁcial intelligence by 
future administraƟons. Provisions in the imperfect American Privacy Rights Act of 2024 that 
would have terminated proposed rulemakings by the FTC are a good ﬁrst step—but they should 
go further to limit agency overreach.13 Agencies should encourage Congress to take the lead on 
data privacy and help them explore more opƟons to limit the interpreƟve power of unelected 
agencies.  
Importantly, although data policy has impacts on arƟﬁcial intelligence, it should be 
developed independently of any naƟonal AI framework. States have already tried to govern data 
privacy and arƟﬁcial intelligence at once in their AI fairness proposals before discovering that 
these laws can be nightmarish to implement at the same Ɵme. Congress should not make the 
same mistake. 
An ExecuƟve Order and Working with Congress on a Light Touch AI Framework 
Whatever the harms of the data privacy patchwork, the arƟﬁcial intelligence patchwork 
will be worse. Some regulatory advocates suggest that AI has developed in the “Wild West”—or 
what they perceive as unregulated environments. State policymakers have interpreted a lack of 
Congressional acƟon on arƟﬁcial intelligence policy as a sign that Washington is poliƟcally 
gridlocked and unable to engage in arƟﬁcial intelligence regulaƟon.14 In 2025, states have 
already proposed 781 AI bills, which has already eclipsed the total number of AI bills in 2024.15 
That number will conƟnue to grow unless Congress asserts supremacy on certain AI issues with 
12 Logan Kolas, "OpposiƟon to Proposed Automated Decision-Making Technology RegulaƟons," Public Comment by the 
American Consumer InsƟtute, February 20, 2025, hƩps://www.theamericanconsumer.org/2025/02/public-comments-
opposiƟon-to-proposed-automated-decision-making-technology-regulaƟons/. 
13 United States Congress, "American Privacy Rights Act of 2024" discussion draŌ, 
hƩps://d1dth6e84htgma.cloudfront.net/American_Privacy_Rights_Act_of_2024_Discussion_DraŌ_0ec8168a66.pdf . 
14 Carl Franzen, "Trump Revoking Biden AI EO Will Make Industry More ChaoƟc, Experts Say," VentureBeat, February 11, 2025, 
hƩps://venturebeat.com/ai/trump-revoking-biden-ai-eo-will-make-industry-more-chaoƟc-experts-say/. 
15 "ArƟﬁcial Intelligence (AI) LegislaƟon Tracker," MulƟState, Last visited March 15, 2025, hƩps://www.mulƟstate.ai/arƟﬁcial-
intelligence-ai-legislaƟon . 


8 their own framework. Just like with privacy, a patchwork of AI rules threatens to limit capital 
investment, raise compliance costs, and decrease compeƟƟon by harming smaller businesses. 
Among this avalanche of state rules, an expansive AI fairness regime is sprawling across 
state capitols, which threatens to undermine the American leadership posiƟon in arƟﬁcial 
intelligence. With 24 AI fairness proposals in 16 states, Congress must take acƟon to miƟgate a 
NEPA-style compliance regime that drowns American innovators in paperwork and subjects 
them to a nightmarish compliance regime. To avoid the directly burdensome requirements, 
states should avoid passing AI fairness legislaƟon. If they persist, which is likely, the Trump 
administraƟon should issue an execuƟve order making it clear that AI development should be 
light-touch and permissive. It should also require regulatory rulemaking to undergo a cost-
beneﬁt analysis before implementaƟon.16 It should require federal agencies to compile and 
organize unstructured data. And it should call on relevant federal agencies to create an acƟon 
plan and menu of opƟons for federal pre-empƟon of state AI fairness laws.  
Congress will decide if and how to act on this framework, but the Trump administraƟon 
should be deliberate with its posiƟons, make its preferences clear, and it should provide a menu 
of opƟons for Congress to consider as a potenƟal framework for federal pre-empƟon as the 
increasingly unnavigable state AI web unfolds. Determining which rules—or lack thereof—
should be set by Congress and where states should be given creaƟve license will not be easy to 
determine, which is why the administraƟon should work with Congress to create a light-touch 
and permissive framework. 
Create a Sandbox for ArƟﬁcial Intelligence Innovators 
Federal agencies should examine innovaƟve and permissive frameworks in the states 
and adopt their best pracƟces at federal agencies when appropriate. Utah is leading by example 
in smart AI regulaƟon. Unlike other states pursuing heavy-handed administraƟve compliance 16 Neil Chilson and Josh Smith, "Comment on Request for InformaƟon on the Development of an ArƟﬁcial Intelligence AcƟon 
Plan," Abundance InsƟtute, March 15, 2025, hƩps://abundance.insƟtute/arƟcles/development-of-an-AI-acƟon-plan . 


9 regimes, Utah created a ﬂexible and innovaƟve approach to AI governance by establishing a 
sandbox for arƟﬁcial intelligence innovators where businesses can press the “pause” buƩon on 
outmoded AI regulaƟons that can be updated as impediments become clearer. Rather than 
have services outright blocked by regulatory ambiguity, sandbox parƟcipants can operate under 
the watchful eye of expert regulators, parƟcularly during their development and expansion 
phase. Unfortunately, the approach in Utah is limited if only because the vast majority of the 
U.S. regulatory burden is from federal rules and regulaƟons, not state barriers. As the sandbox is 
draŌed, federal regulators should search for and phase-out AI regulaƟons that cannot pass cost-
beneﬁt analysis tests. Then the federal government should establish a sandbox where AI 
innovators can apply for regulatory relief—or seek informaƟon about vague regulaƟons—
through regulatory miƟgaƟon agreements that promote innovaƟon. Importantly, parƟcipaƟng 
businesses should be asked to voluntarily report progress made toward their milestones. These 
reports will be handed up to the program administrators to learn how products or services can 
strategically and cost-eﬀecƟvely comply with exisƟng rules once their parƟcipaƟon in the 
sandbox expires. AŌer all, the purpose of the sandbox is to prepare the parƟcipant for business 
aŌer the sandbox—and this is an opportunity to inform Congress and federal agencies what is 
not working. As such, the overseeing agency should keep documented progress reports so that 
it can report to Congress and other federal agencies its recommendaƟons about rules, laws, and 
regulaƟons that it determines need reform or repeal. 
Conclusion 
In the face of a growing patchwork of state AI legislaƟon and the bureaucracy that 
surrounds it, a robust AI AcƟon Plan is essenƟal. OSTP should recognize that, in many cases, 
new legislaƟon is redundant, unnecessary, costly, and would likely inhibit innovaƟon. In these 
cases, such regulaƟons should not be pursued. Instead, an execuƟve order can signal to 
lawmakers that a light-touch approach should be the standard and that frameworks on which 
businesses depend for plausible rebuƩals should be reformed for clarity and speciﬁcity. 
Agencies should work with Congress on federal frameworks for AI and for data privacy—but 


10 those reform eﬀorts must be kept separate. Finally, OSTP should support the digiƟzaƟon of 
government records and the creaƟon of a federal learning laboratory to provide innovators with 
the resources and environment they need to create their best models and AI applicaƟons. This 
is the kind of acƟon plan America needs to stay ahead in the global AI race. 
Respecƞully submiƩed, 
Logan Kolas 
Director of Technology Policy  
The American Consumer InsƟtute Center for CiƟzen Research 
4350 N. Fairfax Drive, Suite 750 
Arlington, VA 22203 
www.TheAmericanConsumer.org  This document is approved for public disseminaƟon. The document contains no business-
proprietary or conﬁdenƟal informaƟon. Document contents may be reused by the government 
in developing the AI AcƟon Plan and associated documents without aƩribuƟon. 


