From: Seth Herd
To: ostp-ai-rfi
Subject: [External] AI Action Plan
Date: Saturday, March 15, 2025 11:42:16 PM
CAUTION: This email originated from outside your organization. Exercise caution when opening
attachments or clicking links, especially from unknown senders.
Gentlemen and ladies:
Your position in influencing US policy on AI development puts you on the stage of history.
We have an opportunity for unprecedented positive impacts if we act decisively and wisely,and large risks if we do not.
Executive SummaryI recommend forming an independent committee or structured process that can work toward a
better understanding of AI’s potential and risks in the near and medium term. I’ve beenstudying theories of AI progress professionally for two years, and I have a nearly idealacademic and research background. Yet I still don’t trust my own conclusions. No oneunderstands the situation at this point with any certainty, so we should work together tounderstand it better.
We are building technology with enormous potential
That could be copied or stolen by our rivals
Or misused with disastrous results.
Risks include very rapid job loss in the short term
And destabilizing the MAD security doctrine in the medium term
By rapidly developing new military technologies
China should be considered a potential collaborator as well as a rival
They must also pursue AI for its potential
But fear its destabilizing risks
They might collaborate to slow proliferation of dangerous AI technology
As Russia and the US slowed proliferation of nuclear weapons
We can’t stop improving AI and progressing toward AGI
But we must rapidly figure out how to steer this process
To capitalize on opportunities
And avoid potential world-changing catastrophes.
We should collaboratively and privately study AI potentials and risks
Even a cautious and sober assessment of the potentials of AI should make it a high priority to
better understand the opportunities and risks.


The progress of AI today may have striking similarities to the development of nuclear
weapons. Einstein’s 1939 letter to President Roosevelt regarding nuclear weapons triggeredthe creation of an Advisory Committee on Uranium. It was in retrospect considered too slowand cautious, but importantly it established the scientific legitimacy of nuclear weapons, andlaunched the Manhattan Project that put the US in the lead. That allowed us to slow theproliferation of that world-changing technology, reducing the odds of a catastrophic nuclearexchange and establishing the MAD doctrine that has kept Western civilization largely atpeace since then.
We should act quickly in case this situation is as urgent, as I and others believe.
AI development and increases in its capabilities is unfortunately much harder to understandand predict than whether uranium can undergo a runaway fission process usable as a powerfulbomb. We will need better collaborative structures, since experts currently remain indisagreement, and little progress is made in public or even the structured private discussions ofwhich I’m aware.
Regulatory policies on AI development are probably irrelevantThe only other obvious policy move is to somehow monitor progress within the organizations
who claim they’ll develop human-level AI within a few years.
Regulatory policy on AI development in the near term is liable to be largely symbolic. No
proposed regulation I’m aware of has any real effect other than to slow development throughbureaucracy. Government action on this unprecedented opportunity must go beyond playing tothe voting public, and take responsibility for steering the future.
The best policy in a more rational world would be to dramatically slow the development of AI
while we collectively work to understand its potential impacts. Failing that, broad laws againscreating autonomous systems might provide small reductions in risk. But I see no route toimplementing such broad and impactful policy, so understanding and steering AI progress, andperhaps putting it under government control if it is approaching really world-changingcapabilities, is likely the best we can do.
The remainder of this comment adds detail on this policy recommendation. It’s intended for a
broad audience.
Near-term impacts: Economic and Security Benefits and RisksConcerns about AI increasing discrimination or creating deep fakes and otherwise confusing
public debate are all but irrelevant. The potentials and risks even in the near term dramaticallyoutweigh those concerns.
The immediate potential upside of continued improvements in AI is large. Economic
productivity and national security could benefit immensely from even modest improvementsto current systems, and those are sure to happen. That potential will drive progress in the USand elsewhere, regardless of any regulatory action or policy.
The most pressing concern is potential rapid job displacement due to AI agents, which could
cause global economic disaster, and possibly within a few years. AI could swiftly automate


numerous professions simultaneously, leading to unprecedented unemployment rates and
economic instability. Unlike previous technological transitions, the speed and scale of
disruption from general-purpose AI that can learn to perform many jobs might leave no clearpath to recovery, creating long-term or permanent economic damage that can’t be reversedwithout unprecedented redistribution to the permanently jobless, on a global scale.
Once agents capable of replacing human jobs are developed, they will be used by necessity,
and any nation attempting to ban their use will rapidly fall behind. Understanding andcontrolling their rate of deployment, and having a plan to transition to a more productiveeconomy with many fewer jobs available, seems paramount.
AI companies currently have civilian-level security, and publish their research methods in
broad form, which allows foreign competitors to effectively copy their techniques (as withDeepSeek), or to outright steal the technology if a state-level cybersecurity operation becomesso inclined. Thus, the technology will proliferate unchecked unless we secure our AIdevelopment.
Of course the reality of these concerns should be investigated carefully before drastic action is
taken; but time may well be short.
Broad description of the current state of AI and potential rapid progressAdvanced AI surpassing human intelligence could rapidly change national security concerns.
And it is very difficult to predict when such intelligence might be achieved. Current systemsare very limited, but they are only on the edge of learning autonomously like humans do,applying their intelligence to improving their understanding of the world and how toaccomplish ones’ goals. Looking at current AI and concluding it will be a long time before it’shighly effective or dangerous might be like looking at a human ten-year-old and concludingthey won’t become competent or dangerous — just before they start to really reflect and directtheir own learning.
My own prediction, which is roughly as informed and expert as any (and in my biased opinion
among the very best informed) is that we will see AI capable of taking over many jobs in 2-3years, and that it will proceed fairly rapidly toward genuine superintelligence from there,taking mere years to surpass human thought in every useful area. But again, I don’t trust myown opinion, so we should carefully establish a process by which we can get betterpredictions- but we should do that with all haste.
Like nuclear fission, there is a potential runaway positive feedback cycle when AI becomes
able to improve its own intelligence, or accelerate further progress in AI. There is an openquestion is whether, how, and how soon AI advances could lead to an “intelligenceexplosion”, creating AI that autonomously builds increasingly powerful AI.
AI is commonly considered a tool, but its most rapid progress is in general intelligence that
can be applied to many problems. Anyone who hasn’t had deep conversations with currentsystems (Claude 3.7, ChatGPT 4.5, Gemini 2.0) needs to do so to grasp where we are with AI.It is worse than humans at some problems but better at others, and it can be applied to anyproblem that can be put into words. Some such problems are “how would you learn to do thisjob?” or “how could you do research to create new military technologies”). Current AI can’tdo those things, but it can clearly think about how to do them, and it is being rapidly improved


in a variety of ways.
Next generation AI: semi-autonomous agents powered by language models
The new generation of language models (OpenAI’s o1 and o3, DeepSeek and others) are
trained to “think to themselves” using long and complex “chains of thought.” They can solvedifficult problems better than the average specialized PhD on certain well-defined problems.And they keep performing better when they are allowed to “think” longer- extending intomillions of words of thought on a single problem.
We are on the edge of deploying these systems as part of agents with memories and goals that
will allow them to reason and to pursue the goals they were given. When they apply those longchains of thought to accomplishing the goals they were given, they may interpret those goalsdifferently than we intended them, and they may arrive at unexpected strategies foraccomplishing their given goals as they understand them.
This prospect is concerning, but the fact that these systems literally think in English is a huge
advantage in keeping them working to follow our instructions as intended. OpenAI hasrecently published an important paper calling on all AI developers to develop these systems inways that preserves those “faithful chains of thought” (I and others expect that theperformance advantages of allowing future systems to “think” entirely in their own inscrutablelanguage might well tempt us to give up the large safety advantage of being able to read oursystems’ “thinking”).
Medium-term risks: loss of control or catastrophic misuse of autonomous AIThere are enormous economic incentives to develop agents capable of doing valuable work,
and the current path is as described above: Large language models trained and designed tothink for themselves and pursue goals. These would not be merely technologies; such systemsare potentially around the corner, and they must be understood as entities.
At some point, progressing on autonomous AI becomes less like developing a technology, and
more akin to inviting an alien species to land on our planet. If they are continually improved(or improve themselves), they will be highly intelligent and autonomous, and capable ofreshaping human life dramatically.
Most people who seriously consider AI progress think there’s a real risk of a metaphorical
“Sorcerer’s Apprentice” scenario, in which we have learned enough to set entities in motion,but have not learned to control them. Unlike the cartoon, we cannot rely on a happy endingunless we are far more considered and cautious than we have been in developing previoustechnologies.
The hope that we can use AI as powerful tool that only does what we want could come true.
But given what we know now, it might be easily possible to turn systems like our current bestAI, general language models, into “tools that use themselves.” That is equivalent to an entity;and while current language models copy humans styles of thinking and reasoning, theiroperation is vastly different than the human brain, so they would be better thought of looselynot as a technology but as alien species’ or wildly neurodivergent but brilliant humans.
It should go without saying that an entity that learns on its own or improves its own software


could ultimately create technologies and strategies that would make humans obsolete.
Alignment of autonomous AI with human interests
Even if autonomous human-level-plus AI follows human commands, we should worry about it
in the hands of humans who would use it to accomplish their own selfish or foolish goals. Andwe aren’t even close to sure we can ensure it follows commands as we intend them, or hassuch good “artificial ethics” that it does only things that benefit humanity.
We as a research community have ideas about how to make autonomous AIs kind, helpful, or
at least obey orders- but we don’t have good or convincing plans, nor well-thought-out routesto get them. Expert opinions vary widely in how difficult the project of ensuring thatautonomous AI remains safe. Disturbingly, those who’ve considered the problem most deeplyseem to also estimate it as much harder, while those with expertise in AI but who have thoughtless about autonomous next-gen AI assume it will be relatively easy.
The uncertainty pointed to by these wildly varying estimates should be enough to make us
cautious when we approach AI capable of autonomously acting and learning (and we may berapidly approaching those capabilities). Stumbling into self-improving AI might work out wellfor humanity, but it might be like summoning a demon that interprets your deal differentlythan you’d hoped.
The intuitive response to these concerns is to say “maybe, but that’s sci-fi and it’s got to be far
off”. That could be right, but we should not gamble humanity’s future on intuitions that couldbe guided by wishful thinking (technically called motivated reasoning or confirmation bias, apowerful and ubiquitous human tendency that was one area of study in the latter half of myacademic career).
It’s easy to look at current AI and notice its weaknesses. One clever commentator likened that
to meeting a studious but disorganized 14 year old and concluding that humans couldn’taccomplish much of anything, ever.
The prospect of a semi-autonomous entity that learns on its own and follows instructions only
as it interprets them is terrifying if one can take it seriously. Most arguments that that can’thappen in the near future boil down mostly to wishful thinking and clever jokes. Seriousthinkers need to consider the worst as well as the best possibilities.
But we can’t just stop building AI. The world won’t wait.If and when autonomous, self-teaching AI is achieved, it’s imperative that it be in trustworthy
hands.
Conclusion and summaryFirst we tamed animals to help with our work, then made machines that could do more. Next
we made computers that could process information in very useful ways if we carefullyprogrammed them to do so. Now we are training artificial minds that can think for us. Soonthey’ll be able to think for themselves.
I have had the privilege of working full-time on these questions for around the last two years


(as a research fellow at the nonprofit Astera Institute; see my work and credentials at
sethaherd.com ). I humbly think my career as an academic working on cognitive psychology,
systems neuroscience, and their AI applications, combined with my personal interests in
ethical philosophy, clinical psychology, politics, and social dynamics is roughly as good asany other background for addressing these weighty matters. I have been thinking about themfor the last twenty years.
I still don’t know what US policy on AI development should be.We should work together to figure that out.Respectfully and with kind regards,Seth Herd, PhD
This document is approved for public dissemination. The document contains no business-
proprietary or confidential information. Document contents may be reused by the government indeveloping the AI Action Plan and associated documents without attribution.
All e-mails to and from this account are for NITRD official use only and subject to certain disclosure
requirements.If you have received this e-mail in error, we ask that you notify the sender and delete it immediately.


