March 14, 2025 
To: Faisal D’Souza, NCO    From: Blake Pierson 
Office of Science and Technology Policy            Fathom 
Executive Office of the President    447 Sutter St 
2415 Eisenhower Avenue           Suite 405 
Alexandria, VA 22314       San Francisco, CA, 94108 
Submitted by email to 
Re: Request for Information (RFI) on the Development of an Artificial Intelligence (AI) Action 
Plan (“Plan”)  
INTRODUCTION  
Fathom staunchly supports the work of the Office of Science and Technology Policy to define 
and prioritize policy actions required to strengthen America’s position in AI. President Trump 
and his administration have a unique opportunity to enable American AI leadership – the 
following recommendations will support that vision and help make it a reality.  
AI could empower humanity to discover new materials, develop life-saving medications, and 
harness novel energy sources— an industrial and scientific revolution. It could transform 
education, media, and communication—an information revolution. It could fundamentally 
reshape how we live, work, and interact with one another—a socioeconomic revolution. It could 
unlock achievements once thought impossible: deciphering ancient texts previously considered 
lost to time, advancing scientific and mathematical theory, and creating entirely new forms of 
artistic expression—a renaissance. This is the extraordinary potential, and perhaps one day the 
extraordinary legacy, of advanced AI.  We must face this transition with optimism, humility , flexibility, and proactivity. The choices we 
make now—about how we develop, deploy, and govern AI—will determine whether we harness 
its extraordinary potential or allow it to unfold haphazardly, or even not at all. To act prudently, 
we first need to know where we want to end up together as members of a free and varied 
society, as individuals, partners, parents, teachers, community members, business leaders, 
factory workers, artists, academics, church parishioners, and civil society advocates.  
Over the past year, Fathom has convened everyday Americans, national security experts, AI 
developers, and more. We conducted multiple polls, spent hundreds of hours conversing with 
stakeholders from across society, and hosted The Ashby Workshops, which brought together 
over 180 experts from business, government, academia, and civil society. These voices, so varied in their expertise and worldviews, all share a concern for the 
preservation and deepening of American values – that the United States, not China, must 
continue to lead in AI. 
1 


As one influential stakeholder put it, "we must all recommit to defending the principles that made 
us great,” and brought us to this moment of great possibility. AI systems rely on vast datasets 
made possible by the internet—itself built upon the ideals embedded in America's founding: the 
free exchange of ideas, speech, and information. The dynamic innovation ecosystem, 
substantial industrial infrastructure, and energy capacity required for AI development depend on 
robust protections for private enterprise, property rights, and free and fair competition. Likewise, 
vital safeguards for workers and consumers are critical to ensure that while business prospers, 
society remains confident that innovation will not trample our unalienable rights. By standing on 
these principles rather than abandoning them, and in some cases remembering them once 
again, we can navigate AI’s complexities with a shared sense of purpose and in a way that 
unites rather than divides us, turning technological change and the resulting disruption into a 
catalyst that benefits all Americans. Fathom’s mission is to find, build, and scale the solutions needed for our transition to a world 
with AI. Our research indicates that a successful transition is one in which our shared values 
persist amid the upheavals we are sure to encounter. Therefore, Fathom recommends 
President Trump’s AI Action Plan prioritize the protection and promotion of American values 
with, through, and despite AI.  
Fathom’s specific recommendations are structured around the following three insights: 
1.The U.S. must retain its global lead in AI to ensure the future of this technology
and the societies it will shape are molded by Western values. Leading means
outpacing strategic competitors in both technical innovation and the widespread, trusted
adoption of AI, ensuring that the future of this technology is shaped by our values, not
those of our adversaries.2.AI should augment human potential, not replace it, while improving daily life,
especially in critical areas like healthcare and public services. Realizing this vision
requires reimagining government and transforming science. It also requires us to commit
to building systems whose code we understand and control, ensuring our technologies
reflect our values.3.We need sensible rules of the road that foster innovation and security. Neither
government nor industry alone should dictate these rules; collaboration across
government and industry is vital to support a vibrant start-up ecosystem and maintain
our technological advantage. Realizing this vision requires governance innovation.
RECOMMENDATIONS  
Based on our insights, Fathom respectfully recommends President Trump’s AI Action Plan 
include the following priority actions: 
Invest in Short- and Long-Term AI Governance  
Like all general-purpose technologies, AI will present novel governance challenges. Sometimes, 
the optimal solution to these challenges will take the form of regulation. But often, they will 
require more imaginative thinking. To achieve effective AI governance, hasten the adoption of 
the technology across American society, and solidify our leadership position in the development 
2 


of this revolutionary technology, President Trump’s AI Action Plan should consider the following:  
1.Focus on Regulating Harmful Conduct: Successful regulation must focus on deterring
harmful conduct. Of course, existing law already codifies countless behaviors that
policymakers have desired to disincentivize through civil or criminal penalties –
everything from lying on accounting forms to treason. Thus, in many ways the challenge
lawmakers face is the application of existing law to the conduct of individuals and
businesses using and developing AI. In many cases, the solution will be fairly
straightforward; for example, fraud remains fraud regardless of whether it is performed
using pen and paper or an advanced superintelligent agent. However, enforcing laws
against people using advanced agents is likely to be a nontrivial task, requiring rapid
government adoption of AI.In other cases, AI may require correcting gaps in existing law, or even loosening law to
enable productive uses inadvertently made unlawful through the design of existing
regulatory structures. An example of the former is the knowing distribution of malicious
deepfakes, which is not necessarily a crime in every US state. An example of the latter
might be Food and Drug Administration clinical trials for new pharmaceuticals. The
clinical trial system was designed on the assumption that drugs would be made for large
populations of people with the same disease. But modern biology and AI are pushing
toward a future where therapeutics are increasingly tailored to the specific medical
needs of small groups of patients or even just a single patient. This could represent a
paradigm shift in how we have historically viewed ultra-rare diseases and their treatment,
and may require an entirely new form of drug approvals for treatments of this kind.Regulating conduct does not mean that developers of frontier AI models should be free
from regulations that apply to them specifically. Indeed, the development of AI is a novel
kind of conduct that may well require novel forms of regulatory intervention.
2.Build an AI Evaluations Ecosystem: Even in an AI regulatory paradigm that relies
primarily on existing law, businesses will face uncertainty about managing AI risks.
Already, businesses across the country are struggling to evaluate models for reliability
and for compliance with existing laws. AI policy thus often finds itself with a kind of
chicken-and-egg problem: businesses do not know how to comply, and policymakers do
not know how to describe what compliance looks like in a robust and measurable way.The solution to the chicken-and-egg problem faced by regulators and businesses is a
robust technical means of evaluating, auditing, and verifying the validity of models,
training data, and model outputs.
One important challenge with evaluations, however, is that they are easy to
manipulate—if a developer has access to the evaluations, it is easy for them to “game”
the benchmarks, either by training on the evaluation itself (having the model memorize
3 


the answers) or through more subtle means. This means that model evaluations often 
need to be kept private.  
However, while the model evaluations themselves are private, the benefits they provide 
are public goods. Therefore, we need evaluation bodies whose results are public but 
whose methods remain private. While there is a market demand for such bodies, it is not 
entirely obvious that market mechanisms alone will produce them. They may well do so 
for some industries and use cases, but under-provide them for others. Thus, President 
Trump’s AI Action Plan should leverage policy mechanisms to incentivize the creation of 
competitive markets for model evaluation providers – using the law to create the 
conditions for an AI evaluations ecosystem to flourish.  3.Foster Transparency: While major risks from AI may well present themselves in the near
future, for the time being they remain speculative. Therefore, the role of public policy at
this stage should be to foster greater insight into the nature of these risks and how
frontier AI firms plan to mitigate them, rather than forcing any specific development or
deployment practices on frontier AI companies.
The AI industry has already coalesced around a standard for such disclosures, all of
which describe each respective company’s approach to risk management for models
with capabilities that could present legitimate potential for catastrophic risk. None of
these documents address topics such as misinformation, bias, or other risks that could
negatively impact freedom of expression. Their focus instead is on the most severe risks,
which all companies developing AI acknowledge are genuine possibilities.Making these documents public allows researchers, policymakers, and any other
interested parties to understand how frontier AI companies are planning for major
increases in model capabilities. This knowledge is a valuable public good.
Currently, companies release these documents as part of voluntary commitments. Given
their utility, it would be prudent for a law to mandate that frontier AI companies continue
releasing them to the public.
4.Create a Unified National Legal Framework: America’s adoption of AI, and broader
economic competitiveness, is being threatened by a patchwork of proposed and enacted
state regulations. In the 2023-24 legislative session, states introduced more than 600
AI-related bills and passed more than 2001. Many of these bills are anodyne or deal with
aspects of law that most would agree are best left to state governments – for example,
creating civil or criminal penalties for the knowing distribution of malicious deepfakes2.
Some, however, have far-reaching implications.
2 Dean Ball, The Deepfake Challenge: Targeted AI Policy Solutions for States (Oct. 2024), available at 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5073572 (Accessed March 14, 2025) 1 Multistate.ai: Artificial Intelligence (AI) Legislation, March, 2025. 
4 


AI will benefit from a clear and unified nationwide legal standard. If models with the 
potential to cause physical harm or loss of property are developed, this legal standard 
could be expanded with provisions affecting the following policies: 
I. An acknowledgement that frontier AI systems have such significant capabilities
that there may be legally negligent means of developing and deploying such
systems if their use results in physical harm or loss of property;II. A means for developers to obtain safe harbor from the possibility of negligence
liability for major harms if they adhere to certain development and deployment
standards promulgated by either the federal government (e.g. the US AI Safety
Institute) or a multi-stakeholder public-private standards body.
5.Invest in Longer-Term Governance: Given the speed with which AI capabilities can
proliferate throughout the world it will likely be difficult or impossible for any government
to maintain durable control over the development and use of very capable neuralnetworks. However, just as the internet was fundamentally enabled by technical
protocols, AI adoption may also require new protocols. We recommend considering the
following suggested list of new protocols:
I. Facilitate validation of personhood and identity in digital environments;
II. Reliably identify AI agents as such, and connect them back to the users on
whose behalf they are acting; and
III. Ensure auditability of agent-agent interactions.
The private sector may create these protocols on their own, or solve these problems 
another way – but the area deserves further study in the AI Action Plan.  
Accelerate AI Adoption in Government  
AI presents governments with an immense opportunity to transform public service delivery. Far 
more efficient—and far better—government is within reach. To achieve this vision, we 
recommend that President Trump’s AI Action Plan include the following: 
1.Revise the Office of Management and Budget’s M-24-103 and M-24-184 memos on
agency use and procurement of AI to:
a.Narrow the definition of “safety-impacting AI” to focus exclusively on situations
where physical harm is a possibility;
b.Narrow the definition of “rights-impacting AI” to focus on Constitutionally
protected rights;
c.Eliminate requirements for agencies to consider the environmental impact of their
AI use or of their AI system provider’s AI training/use; and
4  Executive Office of the President, Office of Management and Budget, Memorandum-24-18 (Sept 2024), available at 
https://bidenwhitehouse.archives.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf (Accessed March 14, 
2025) 3 Executive Office of the President, Office of Management and Budget, Memorandum-24-10 (March 2024), available at 
https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Ag
ency-Use-of-Artificial-Intelligence.pdf, (Accessed March 14, 2025) 
5 


d.Create a pathway for generalist AI systems (language models and agentic
systems based upon language models) to be approved for a broad range of uses
at either an agency of government-wide level, as opposed to creating a separate
compliance process for each use case of such a system.2.Modernize agency data government and retention/access policies to ensure that these
policies do not present undue burdens to the adoption of AI;
3.Ensure sufficient construction of data centers meeting federal security requirements for
both sensitive and classified data is taking place;
4.Revise the role of the agency “Chief AI Officer,” mandated by President Biden’s
Executive Order on AI, to focus on driving adoption within agencies in addition to
mitigating risks from AI; and5.Eliminate the requirement in the 2023 Advancing American AI Act5 for agencies to
publish inventories of their AI use cases, or make clear that general descriptions of some
uses (for example, “using language models for back office process automation”) is
sufficient, as opposed to an itemized inventory of each AI use.
Building, Permitting, and Semiconductor Manufacturing  
AI is different from many other digital technologies in that it requires large-scale industrial 
infrastructure. Already, an industrial buildout of unprecedented speed and scale is taking 
place—by the end of the decade, it may be the largest such buildout in American history. In 
addition to data centers and the energy generation facilities needed to power them, AI will 
require cutting-edge semiconductor manufacturing capacity. Just like the data centers, the ideal 
outcome would be for many of those manufacturing facilities to be on American soil.  Accomplishing this will require large-scale reform to environmental regulation. America’s most 
successful environmental regulations rely on mandating objective standards for crucial things 
like air and water quality. These objective standards led to massive reductions in measurable 
pollution over the last several decades.  
Permitting reform is the most important step—in some ways the most meaningful AI-related 
policy decisions the federal government can make in the short-term6. But it is far from the only 
step. In particular, ensuring that America retains a fundamental advantage in semiconductor 
manufacturing is essential.  
In the longer term, it will also be crucial for American researchers to remain at the forefront of 
leading-edge semiconductor development. While it is true that TSMC has taken the lead in this 
regard, it remains the case that most of the fundamental advancements to semiconductor 
manufacturing were pioneered by Intel and other American firms. In recent years, it has become 
both more expensive and slower to move the frontier of semiconductors. New process nodes 
6 Thomas Hochman and Aidan Mackenzie, How the White House Can Reform NEPA (Feb. 2025) available at 
https://www.thefai.org/posts/how-the-white-house-can-reform-nepa (Accessed March 14, 2025); Thomas Hochman, A Permitting 
Reform Wishlist for Congress (Feb. 2025) available at https://www.greentape.pub/p/a-permitting-reform-wishlist-for (Accessed 
March 14, 2025) 5 Pub. L. No. 117-263 (May 2022), available at https://www.congress.gov/bill/117th-congress/house-bill/7776/text (Accessed March 
14, 2025) 
6 


(shrinking the size of transistors to allow for more computationally performant and efficient 
chips) are taking closer to 36 months than 24, and the cost of achieving a new node has been 
rising for the past decade. As transistors become smaller, we are pushing up against 
fundamental physics in novel ways. All of this suggests that radically new approaches to every 
step in the semiconductor manufacturing process will be needed.  
America is well-positioned to lead the way with these new approaches. Congress already 
allocated $11 billion for the National Semiconductor Technology Center, a public-private 
research consortium that would do exactly this. If it is properly structured and led to pursue truly 
radical new research directions, it can become a world-leading research body, eventually paving 
the way to relying primarily or exclusively on private, rather than taxpayer, funding7.  
Transforming Science 
AI stands to massively improve the research productivity of scientists8. Today, a scientist at a 
research university might have a small team of graduate students and other collaborators at 
their disposal. Together, they navigate the laborious process of keeping up with the latest 
research, ideating potential experiments, conducting experiments, processing and analyzing 
data, writing papers—and of course, applying for grants. AI tools of all kinds—from narrow 
models designed to automate tasks to advanced generalist agents that can serve as a kind of 
“automated researcher”—promise to fundamentally change the means of producing science 
itself.  Many of the fundamental AI tools already exist or are on the development roadmaps of leading 
AI companies. But to take full advantage of these opportunities, science will need new 
infrastructure that gives scientists the ability to perform orders of magnitude more experiments 
than were previously possible. Government can support this infrastructure by investing in the 
creation of automated, “self-driving” labs. Many domains of science are not designed for 
industrial-scale experimentation, and as a result a great deal of scientific equipment is primarily 
built for the artisanal nature of science today. By creating demand for new kinds of lab 
equipment, built for massively scalable experimentation, government can foster a new wave of 
scientific progress.  To do this, the Department of Energy, the National Institute of Standards and Technology, the 
National Science Foundation, and other key federal agencies should:  
1.Build cross-departmental science datasets and data infrastructure;
2.Partner with the private sector to develop science foundation models or, where sensitive
data is being employed, build such models within government; and
3.Create a competitive bidding process or “grand challenge” for private firms to offer
proposals on the creation of robotic labs (also referred to as “self-driving labs”).
8 Dean Ball, Accelerating Materials Science With AI And Robotics (Nov. 2024) available at 
https://fas.org/publication/accelerating-materials-science-with-ai-and-robotics/ (Accessed on March 14, 2025) 7 Arrian Ebrahimi and Jordan Schneider, How to Make the NSTC A Moonshot Success, (April 2024) available at 
https://ifp.org/how-to-make-the-nstc-a-moonshot-success/ (Accessed on March 14, 2025) 
7 


Ensuring AI Benefits Workers 
The impact of AI on the labor market is uncertain. While many economists believe it will 
primarily augment human workers, others are concerned that there could be near-term shocks 
in specific sectors. The President should direct the data collection agencies of the federal 
government – the Bureau of Labor Statistics, the Census Bureau, the Bureau of Economic 
Analysis, etc. – to begin collecting data relevant to understanding the role of AI in the economy. 
This could include measurements of human task content, work process reorganization, 
human-AI collaboration metrics, skill demand dynamics, augmentation versus substitution 
effects, and more, collected via employer surveys and other means.  Building Genuine AI Security 
AI security is necessary to enable widespread adoption of frontier technology, and another area 
where President Trump’s AI Action Plan can lead.  
If, for example, large-scale generalist AI systems develop capabilities with military applications, 
the Department of Defense (DoD) will want highly secure infrastructure on which to deploy 
these systems. No AI data center in America—and likely the world—has security characteristics 
sufficient to withstand a forceful cyberattack by a nation-state actor. But the DoD can lead the 
way by building a data center that meets the highest standards of cybersecurity, sufficient for the 
storage and analysis of Top Secret and Sensitive Compartmentalized Information. Such a 
facility would likely require both hardware and software innovations and is unlikely to be 
necessary for commercial use—making it an ideal target for government investment. However, 
some innovations made in the process could trickle down to the private sector, as has happened 
frequently in the history of American defense innovation.  Another field that could potentially benefit from defense innovation is AI alignment and control. 
While existing, private sector alignment methods may be sufficient for consumer and business 
applications, military uses of AI systems—where lives are on the line—may require more 
rigorous control and safety mechanisms. For example, autonomous systems with lethal 
capabilities may require mathematically verifiable guarantees9 that they will not engage in 
friendly fire. Again, guarantees of this kind may not be necessary for most non-military use 
cases, but just as with the data center innovations mentioned above, this research could 
eventually make its way into broader business applications (say, in medicine).  Finally, these recommendations would be incomplete without mentioning the criticality of export 
controls on advanced AI compute and semiconductor manufacturing equipment. Other plans 
submitted as part of this RFI are likely to go into great detail on this topic, but here we will 
mention just one area of potential reform: the Biden Diffusion Framework10.  
10 U.S. Department of Commerce, Bureau of Industry and Security, Framework for Artificial Intelligence Diffusion (Jan. 2025) 
Available at https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion 
(Accessed on March 14) 9 David Dalrymple et al., Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems, (May 2024) 
available at arXiv:2405.06624 (Accessed March 14, 2025) 
8 


The Diffusion Framework purports to regulate the distribution of AI hardware throughout the 
world. On top of this, however, it also regulates the export of closed-source model weights. It 
uses a flop-based threshold, which means that large pretrained models (like xAI’s Grok 3) are 
subject to far heavier regulations than lighter-weight but equally capable models (such as 
OpenAI’s o3-mini, particularly when configured in “high reasoning” mode). Moreover, these 
regulations are onerous, requiring data centers in other countries serving consumer chatbots to 
meet FedRamp High and, potentially, other security standards. The model weight component of 
the Diffusion Rule should be relaxed or eliminated. Relaxing country GPU caps on some Tier 2 
countries may also be a potent diplomatic tool.  
CONCLUSION 
The American people are ultimately positive, hopeful, and pragmatic. They want to believe in a 
hopeful future for themselves and their families. They want AI to transform society for the better, 
and they want common-sense decisions that keep them safe and improve their day-to-day lives 
along the way. This societal-driven vision is a future where human-controlled AI systems 
enhance rather than replace human intelligence, creating a world with greater freedom for 
individuals, expanded possibilities for humanity's collective advancement, and widespread 
societal benefits that reach all communities. By maintaining our commitment to democratic 
principles, free enterprise, and human dignity, we can ensure that AI serves as a force for 
liberation and flourishing rather than constraint and displacement. By approaching this challenge with wisdom, humility, openness, and a clear commitment to 
American values, we can navigate the AI revolution not as passive observers but as active 
architects of a more prosperous, just, and free society. The moment for reinvention is now. We 
must seize it together. 
### 
This document is approved for public dissemination. The document contains no 
business-proprietary or confidential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without attribution. 
9 


