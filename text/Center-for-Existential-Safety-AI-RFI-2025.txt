United States Of ﬁce of Science and Technology Policy 
(OSTP) - Development of an Arti ﬁcial Intelligence (AI) 
Action Plan Input 
Center for Existential Safe ty 
March 15, 2025 
1. Introduction
The Center for Existential Saf ety believes that the continued development of arti ﬁcial 
intelligence without extremely stringent safeguards will soon lead to humanity’s extinction 
or permanent disempowerment. 
We've know n sinc e at least 1863 with Samuel Butler's “Darwin Among the Machines" that 
machines might one day supplant humans. In 1951, amidst the digital computer revolution, 
Alan Turing cogently outlined arti ﬁcial intelligence's existential risks. And for the last 20+ 
years AI safety pioneers have been attempting to help our species come to terms with this 
risk.  
We are not alone in our concerns.  
The Nobel laurea te G eoffrey Hinton says, “I actually think the existential risk [AI causing 
human extinction] is more than 50%. ”  
Stuart Russell, the President of the International Association for Safe and Ethical Arti ﬁcial 
Intelligence, says, “The development of highly capable AI is likely to be the biggest event in 
human history. The world must act decisively to ensure it is not the last event in human 
history. ”  
Sam Altman, the CEO of OpenAI, said, “AI will probably most likely lead to the end of the 
world, but in the meantime, there’ll be great companies. ” 
To date, over 1, 000  risks and 3,500 reports of harms (or 14,000 reports of incidents and 
hazards) from AI have been cataloged. And capacity is growing at extraordinary rates by 
tech companies that prioritize pro ﬁts over safety. 
1 of 5 


In response, essentially no realistic plans have been developed, funded, and enacted on an 
international level to mitigate those risks. As a species, we spend far more on candy than 
we do on preventing our own extinction. 
We believe we have collectively failed to prepare for this crucial moment in humanity’s 
history. But with strong leadership from the United States, we can change that now. 
2. Recommendation
In the spirit of this open letter “calling on world leaders to show long-view leadership on 
existential threats” ,  we urge the United States to now lead us with a laser focus on 
ensuring humanity’s long-term existential safety.  
Our primary recommendation for the Trump Administration is to join the International AI 
Governance Alliance. If the United States joins and champions this alliance, we believe it 
will be leading humanity toward a much safer, brighter future for all. We believe this could 
be the biggest win-win in all of human history. The Trump Administration could champion 
this, with the President himself playing a deﬁning role.  
We consider this to be strongly in America’s national interests. Indeed, it is one of the only 
ways to ensure its national security. If nearly all people don’t agree to cooperate on AI, then 
everyone in the world is likely to die or be disempowered soon. This is because any one 
person anywhere might intentionally or unintentionally unleash a rogue superintelligence 
that quickly supplants humanity as the most powerful force on Earth. Or they may develop 
and deploy narrow AI systems to cause catastrophic damage or obtain unprecedented 
power, disrupting the geopolitical order.  3. Details
Full details are on the International AI Governance Alliance website. Below we include some 
key parts. 
The Need 
Many experts agree that humanity faces a plausible risk of extinction due to the 
rapidly advancing capabilities of arti ﬁcial intelligence. 
The explicit near-term goal of many Big Tech companies is to create godlike AI 
systems far smarter and more capable than humanity itself. 
2 of 5 


These companies acknowledge the extinction risks posed by their products, yet 
continue developing them without the consent of the eight billion people they are 
affecting. This is entirely unacceptable. 
For everyone's existential safety, all large-scale frontier development of AI systems 
must pause immediately. We must regroup as a species and collectively decide what 
we want to do about AI. 
The Response 
We need a supranational organization capable of effectively mitigating extinction 
risks from AI and fairly distributing its economic bene ﬁts to all. We can call this the 
International Arti ﬁcial Intelligence Governance Alliance (IAIGA). For simplicity, it is 
pronounced ‘eye-guh' . 
This organization should do three things: 
1. Develop the Global AI Safety Treaty that regulates research and development
of AI systems, ensuring they are provably safe and aligned to human values.
2. Manage the enforcement of this treaty.
3. Manage the fair distribution of the economic value that these AI systems
generate.
To be effective, this organization must operate at a speed and scope commensurate 
with the technology it aims to govern. It must be proactive, not reactive. 
It must also operate openly, in conjunction with all signatories and alongside 
extensive public and expert review. 
It cannot be “politics as usual" with endless talk and limited action. Humanity's very 
existence is at stake, which means we all must transcend “normal" politicking. 
The Commitment 
We propose a Collective Commitment on AI for humanity. We commit to: 
1. Adhere to internationally accepted safety standards for AI research and
development.
3 of 5 


2. Adhere to an internationally accepted distribution system for AI-derived
economic value.
Individuals and organizations can make this commitment now. After completion, 
nations can adopt the Global AI Safety Treaty, which would put this commitment 
into law. 
Committing now indicates your support for this safer, fairer future state of the 
world. Everyone can win, if we develop AI with extraordinary care. Otherwise, 
everyone will lose. 
How will you determine the AI safety, enforcement, and distribution policies and 
standards of the Global AI Safety Treaty? 
We believe it is too important for any one individual, company, organization or 
country to determine these policies and standards on their own, as they profoundly 
affect all life on Earth. 
Therefore, we will invite all signatories to participate in an open door meeting to 
collaboratively make these decisions. 
Prior to the meeting, we will run an open call for proposals from policymakers and 
the general public. Anyone can submit their ideas, even non-signatories. 
In the meeting signatories will be invited to debate all known approaches to AI 
governance. See some examples here. Proposals will be evaluated by signatories for 
their ability to adequately address humanity's near-term risk of extinction and the 
fair allocation of resources derived from AI. A prediction market for each proposal 
will also be created to capture collective insight into their viability. 
Given the extreme urgency for immediate action, the meeting will not conclude 
until agreement on a single proposal is reached. We will then share that with world 
leaders. 
We will reconvene at least annually to reaf ﬁrm or update our policies and standards. 
4. Endorsements
4 of 5 


In addition to our recommendation above, we also endorse the Machine Intelligence 
Research Institute’s Response to RFI on the Development of an Arti ﬁcial Intelligence (AI) 
Action Plan.  
— 
This document is approved for public dissemination. The document contains no 
business-proprietary or con ﬁdential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without attribution. 
5 of 5 


