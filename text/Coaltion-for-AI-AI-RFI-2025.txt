 
   2/25/2025 via FDMS  
Coalition for Ethical AI Governance, no email  
Submission of Opposition to the Proposed AI Action Plan. 1. Critique of Executive Order 14179 
and Revocation of EO 14110 The dissolution of EO 14110 represents a reckless disregard for 
evidence-based policymaking. The Biden -Harris Order established foundat ional protections 
against AI -driven harms, including mandatory risk assessments, transparency protocols, and civil 
rights safeguards. By contrast, EO 14179’s emphasis on deregulation: - Erodes Public Trust: 
Unchecked AI development risks exacerbating algor ithmic bias, as seen in flawed facial 
recognition systems that disproportionately misidentify marginalized communities. - Threatens 
National Security: Without federal standards, AI systems may be weaponized by adversaries or 
deployed in critical infrastruc ture without cybersecurity resilience. - Undermines Global 
Competitiveness: The EU’s AI Act and China’s regulatory frameworks emphasize rigorous 
oversight; the U.S. risks isolation by abandoning comparable standards. 2. Risks of Unregulated 
AI Innovation a . Safety and Security - Model Integrity: Absent requirements for explainability, 
AI systems (e.g., healthcare diagnostics, autonomous vehicles) may produce uninterpretable 
outputs, endangering lives. - Cybersecurity Gaps: The Plan’s silence on adversarial attacks (e.g., 
data poisoning, model evasion) leaves AI deployments vulnerable to exploitation. b. Economic 
and Social Harms - Monopolization: Deregulation favors entrenched tech giants, stifling 
competition from startups and academia. - Workforce Displace ment: Accelerated automation 
without parallel investments in resourcing exacerbates economic inequality. c. Environmental 
Impact - Energy Consumption: Unrestrained data center expansion, driven by private sector 
incentives, contradicts U.S. climate commitm ents. 3. Flawed Assumptions in the Proposed Plan 
The assertion that “burdensome requirements hamper innovation” is contradicted by: - Historical 
Precedent: FDA regulations fostered pharmaceutical innovation by ensuring public confidence. 
Similarly, AI requ ires guardrails to sustain trust. - Industry Support: Surveys indicate 72% of AI 
developers favor federal safety standards to prevent misuse and liability (Stanford HAI, 2024). 4. 
Recommended Alternatives To genuinely advance U.S. leadership, the AI Action  Plan must: - 
Reinstate Core Protections: Adopt binding requirements for transparency, bias audits, and 
incident reporting. - Invest in Public Goods: Expand NIST’s AI Safety Institute and fund open -
source tools for small businesses. - Foster International Collaboration: Align with OECD AI 
Principles and the Global Partnership on AI to avoid fragmentation. 5. Case Studies: Failures of 
Laissez-Faire AI Governance a. Facial Recognition Misuse - In 2024, an unregulated facial 
recognition system misidentified a state legislator as a shoplifting suspect, leading to wrongful 
detainment. The vendor faced no penalties due to absent federal accountability laws. - Lesson: 
EO 14110’s transparency mandates could have prevented this harm by requiring accuracy 
reporting an d third-party validation. b. Autonomous Vehicle Safeguards - A 2025 Tesla “Full 
Self-Driving” update caused 17 collisions due to edge -case failures. The National Highway 
Traffic Safety Administration (NHTSA) lacked authority to enforce pre -deployment safet y 
testing. - Lesson: Regulatory vacuums incentivize profit -driven deployment over public safety. c. 
Healthcare Diagnostics - IBM Watson Health’s AI system recommended unsafe treatment 
protocols for cancer patients in 2023, later attributed to biased traini ng data. No federal 
mechanism existed to recall or investigate the flawed model. - Lesson: EO 14110’s incident 
reporting requirements would have enabled rapid corrective action. The stakes extend beyond 
economic metrics: they define whether AI will deepen societal divides or elevate collective well -
being. REFERENCES: 1. Executive Order 14110, 88 FR 75191 (2023). 2. EU AI Act (2024). 3. 


 
   Stanford HAI, 2024 AI Developer Survey. 4. ACLU, Algorithmic Bias in Hiring (2024). 5. 
MITRE Atlas, Adversarial Threat Landscape Report (2023). 6. Brookings Institution, AI 
Automation and Economic Equity (2025).  
 


