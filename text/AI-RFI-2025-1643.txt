PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-jzei-yum c
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1643
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Nell Watson
 Email: 
General Comment
Recent advances in artificial intelligence have unlocked unprecedented opportunities for innovation and econom ic growth. However, as
underscored in Tam ing the Machine, these developm ents also present unique challenges—chief am ong them , the risks posed by
supernorm al stim uli and the potential for autom ated zersetzung. Equally, there is significant value in developing robust fram eworks for
autom ated externality accounting. By addressing these issues head on, we can ensure that AI technologies serve hum an flourishing while
m itigating harm ful social and psychological risks.
Supernorm al Stim uli in AI
Supernorm al stim uli refer to artificially exaggerated signals that exploit evolved hum an predispositions, often eliciting responses that are
disproportionately strong com pared to those triggered by natural stim uli. In the context of AI, these stim uli can be deliberately engineered
or inadvertently am plified through algorithm ic processes. Tam ing the Machine em phasizes that when AI system s are optim ized to m axim ize
engagem ent or influence—whether in content recom m endation, advertising, or interactive services—they risk hijacking hum an attention
and decision-m aking processes. This dynam ic can lead to overconsum ption of digital content and potentially distort user perceptions,
leading to adverse behavioral and societal consequences.
Risks of Autom ated Zersetzung
The concept of autom ated zersetzung builds on historical techniques of system atic disintegration—originally used in contexts of
psychological warfare—to describe the potential for AI system s to orchestrate subtle, yet pervasive, form s of social and individual
disruption. Autom ated zersetzung can m anifest through the targeted dissem ination of disinform ation, m anipulation of social m edia
dynam ics, and the covert underm ining of trust in public discourse. As Tam ing the Machine warns, the capacity of AI to rapidly generate
and dissem inate supernorm al stim uli heightens the risk that autom ated zersetzung could be deployed at scale, thereby eroding the social
fabric and destabilizing dem ocratic processes.
The Prom ise of Autom ated Externality Accounting
To counter these risks, autom ated externality accounting offers a proactive fram ework for quantifying and internalizing the societal costs
that AI system s im pose. By developing rigorous m etrics to capture the negative externalities—such as psychological harm , erosion of
social trust, and econom ic disruption—policym akers and industry stakeholders can design targeted interventions. This approach can
enable:
The im position of corrective m easures (e.g., regulatory fees or design m odifications) when AI-driven applications generate
disproportionate external harm s.
Enhanced transparency in algorithm ic decision-m aking processes, thereby facilitating independent audits and continuous oversight.
Incentivization for private-sector developers to integrate safety and ethical guardrails into the AI lifecycle from  conception through
deploym ent.
Policy Recom m endations


Develop Metrics for Supernorm al Stim uli and Autom ated Zersetzung:
Mandate the research and developm ent of standardized m etrics to assess the degree to which AI system s produce supernorm al stim uli
and engage in practices that could lead to autom ated zersetzung. Such m etrics should be integrated into AI risk assessm ents and ongoing
com pliance audits.
Institutionalize Autom ated Externality Accounting:
Encourage public–private partnerships to design and im plem ent autom ated externality accounting fram eworks. These fram eworks would
quantify the broader societal im pacts of AI system s and inform  policy tools—such as regulatory levies or incentives—for m itigating
negative externalities.
Enhance Transparency and Oversight:
Require that AI developers disclose the m ethodologies and algorithm s used to optim ize user engagem ent. Transparency in these processes
will not only help identify potential abuses but also facilitate independent third-party audits to safeguard against m anipulative practices.
Prom ote Multi-Stakeholder Collaboration:
Establish advisory boards that include representatives from  academ ia, industry, civil society, and regulatory bodies. Such collaboration will
ensure that diverse perspectives inform  the developm ent and periodic revision of standards addressing supernorm al stim uli and autom ated
zersetzung.
Integrate Ethical Safeguards in AI Design:
Em bed ethical review processes at every stage of AI developm ent. Drawing from  the principles in Tam ing the Machine, it is essential that
design fram eworks incorporate safeguards aim ed at aligning AI behaviors with hum an values and societal well-being.
By incorporating these considerations into the forthcom ing AI Action Plan, the United States can set a global standard for responsible AI
governance—one that m axim izes innovation while safeguarding the public interest.


