3/13/2025  via FDMS  
Matthew Milone, no email  
Hello. I'm a software engineer and robotics teacher who's writing to advocate that the 
government adopt a safety-first approach to AI policy. For decades, technologists have been concerned about the theoretical dangers of powerful AI. Within the last few months, their concerns have been validated in experiments by AI labs. For example: in December, Anthropic and Redwood Research caught an AI strategically copying itself and lying to humans to avoid being shut down. If the AI was smart enough to outsmart humans, it would have been a disaster. A smarter system could have hacked its way out of the lab or manipulated a human into letting it out. For years, we've been making AIs smarter much faster than we've been making them safer. If this trend continues, the results will be catastrophic. Consequently, we need an international treaty that prohibits further development of AI capabilities until the safety problems are solved. There is historical precedent for international prohibitions like this, including restric tions on 
nuclear technology and infectious disease research. I believe that problems should be addressed as locally as possible --but in this case, a local, state, or even national scale wonâ€™t work. There is 
no way for progress in AI safety to catch up unle ss progress in AI capabilities slows down; and if 
we slow down America's progress, then we have to slow down other countries' as well. Thanks for you time and consideration. Sincerely, Matthew V. Milone 
 


