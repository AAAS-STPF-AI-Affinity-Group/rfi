From:
To: ostp-ai-rfi
Subject: [External] AI Action Plan
Date: Monday, March 17, 2025 9:21:12 AM
CAUTION: This email originated from outside your organization. Exercise caution when opening
attachments or clicking links, especially from unknown senders.
Introduction
I'm a software engineer who has observed the transformative impacts of artificial
intelligence firsthand. Language models have become increasingly powerfulmultipliers of developer productivity, at first as learning tools since they're betterthan internet search engines for many topics, and increasingly as autonomous agentsthat write code directly. At the same time, however, language models have hobbledthe software industry as a whole by making recruitment harder. Employers used tobe able to find great engineers by posting jobs online, but now, public job postingsare reliably flooded with hundreds or thousands of AI-generated applicationsoptimized to the point of being undifferentiable. Employers are increasinglyreturning to older, less efficient ways to find candidates. Teams are becoming lessproductive as they start to feel the effects. As AI programs for writing jobapplications continue to proliferate and become easier for nontechnical job seekersto use, other industries will become less productive as well. It's too late to stop thistrend, but it can serve as an early warning for greater unintended harms that can stillbe prevented. Artificial intelligence is advancing quickly and its advances cannot beassumed to be beneficial to America.
Background
The AI Action Plan is a government initiative of extreme importance, even
compared to the other government initiatives making the news. Its highest prioritymust be mitigating the downside risks of artificial intelligence.
Analysis
Training transformers directly on text from the internet made them generally
knowledgeable. The latest frontier models are specifically trained to reason bywriting thoughts before they act, making them much better at self-correction, andSam Altman and Dario Amodei are promising much larger advances in a fewmonths. I suspect the next wave will be models specifically trained to read andwrite memories as they think, making them much better at learning from their ownexperience. It's hard to predict what happens then, but it's safe to say that


increasingly destructive actions will become increasingly accessible as AI
capabilities continue to proliferate. Much of society relies on certain types of crime
being inherently difficult to commit, an assumption which will break down whenanyone can command human-level autonomous agents to work tirelessly on anytask. Large organizations, especially those relying on legacy systems, may collapseas autonomous agents efficiently uncover their security vulnerabilities and poisontheir data. Automated identity theft could cause large banks to lose everyone'saccounts with no reliable way to correct the error. Designs provided by AI couldmake it easy to build counterfeit currency printers. An advanced cyberattacktargeting a vehicle manufacturer could cause millions of cars to crash at once.Software written by AI could allow turning a cheap quadcopter with a haphazardlyattached gun into a viable sniper drone. AI will even be capable of designing deadlyviruses. This is far from an exhaustive list. These crises will happen withoutwarning, deploying AI defensively won't prevent all of them, and manyorganizations won't upgrade their security soon enough regardless. Finally, at someunknown threshold, autonomous agents will surpass their creators' ability todevelop more powerful intelligence. Calling recursive self improvement a nationalsecurity risk is an understatement: it is America's final deadline.
Recommendations
The only way to prevent these disasters is to control the proliferation of frontier
artificial intelligence. Frontier models must not be open sourced, and even closedsource models must be limited, since they are all trivial to jailbreak and probablyalways will be. The US government must procure exclusive access to the mostpowerful models to ensure that its defense stays ahead of the strongest publiclyavailable offense. AI research labs must have state-level security. AI chips must betracked as WMD inputs. Since superhuman intelligence is inherently uncontrollableby humans even under ideal circumstances, a hard stopping point must be defined inadvance. Once all of this is in place, America can focus on ensuring that othernations, particularly China, follow its lead. Last week's Superintelligence Strategypaper by Dan Hendrycks, Eric Schmidt, and Alexandr Wang is a good starting pointfor considering the risks that America must be prepared for and the measures that itcan take: https://www.nationalsecurity.ai
Conclusion
Artificial intelligence is dual use and advancing rapidly. The dangers are
unprecedented. Without government policies to control proliferation, AI willbecome analogous to the weapons HG Wells hypothesized in The World Set Free:"It was a matter of common knowledge that a man could carry about in a handbag


an amount of latent energy sufficient to wreck half a city." If America doesn't
become the world leader in AI security, then America won't survive AI.
Austin HenrieThis document is approved for public dissemination. The document contains no
business-proprietary or confidential information. Document contents may be reusedby the government in developing the AI Action Plan and associated documentswithout attribution.
 
All e-mails to and from this account are for NITRD official use only and subject to certain disclosure
requirements.If you have received this e-mail in error, we ask that you notify the sender and delete it immediately.


