March 1 5, 2025  
To Whom It  May Concern: 
This docume nt responds to the Request for Information (RFI) issued by the NITRD NCO on 
behalf of the OSTP for input from all interested parties on the Development of an Artificial 
Intelligence (AI) Action Plan (“Plan”)  to identify the  highest priority policy actions that should 
be in the new AI Action Plan. This document is approved for public dissemination. The 
document contains no business -proprietary or confidential information. Document 
contents may be reused by the government in developing the AI Action Plan and associated documents without attribution.   
This  individ ual response1 addresses two of the actions on the list and adds a third that 
suggests the P lan include an approach to develop trusted AI data.   
1.Technical and Safety standards
Simply put, standards should not be seen as “an end unto themselves”.   The government is not exempt from the management mantra that what you measure is what you get.   Rather 
than simply calling for standards to be published, which will lead to a proliferation of standards, the plan should call for standards  that are adopted and that are evaluated to 
assess whether the standards are actually meeting the stated goals.  
A recent National  Institute of Standards and Technology (NIST) Visiting Committee on 
Advanced Technology report stated “the committee’s view is that we currently lack frameworks for measuring the aggregate impacts of standardization, along with the tools and metrics required for assessing the effect of policy interventions in this domain. ” 
2  The 
report noted that this is at least partly due to the “lack of any formal or shared method to 
measure investments in standardization or their impacts ”.  That Committee recommended 
that “NIST establish a project in collaboration with academia and the standards community 
to create a defined set of objectives, conceptual framework, taxonomy of metrics, and  
1 These views are personal and do not represent any institution with which I am or have been affiliated. The 
following list discloses those institutions to ensure full transparency. I served on the National AI Research 
Resources (NAIRR) Taskforce and curren tly serve on the Advisory Committee for the NAIRR Pilot.  I have 
recently completed a one year IPA at the National Institute for Standards and Technology.  I have been deeply 
involved with the National Science Foundations Technology, Innovation and Partner ship program’s 
workforce work.   I serve on the Advisory C ommittee  for the National Data Platform.  
2 Report to NIST Leadership for the Implementation of the U.S. Standards Strategy for Critical and Emerging 
Technology by the   Visiting Committee on Advanced Technology of the National Institute of Standards and 
Technology February 2024 https://www.nist.gov/director/vcat/reports   


common q ualitative factors for measuring both the value of investment in standards and 
their impact. ” (Recommendation 26)[1] . 
Recommend ation : The Plan could charge an agency with e stablishing  an AI standards  
evaluation framework that could be used to define and scope the definition of the intended 
outcomes associated with the development of AI standards. The Plan should charge the 
agency to work with the community to identify the components of the theory of change and associated measurement  so that  the framework could also be used to provide  in federal, 
state, and local governments with  early indications of standards  effectiveness for 
monitoring purposes.   An exemplar is provided below
3 
The Plan  could recommend that once a theory of change is established for particular AI 
standards – or, indeed, for an entire class of products or processes such as the entity 
resolution use case – the framework could be used as a basis for providing information  
about progress at each step in a theory of change. That information  could be used, in concert 
with stakeholders, to monitor progress, identify problems, and provide accountability and  
3 Source: Figure 3: NIST AI 100 -7, an initial public draft entitled “Towards an Approach for Evaluating the 
Impact of AI Standards” authored by Julia Lane.  The draft has been  approved for, but is waiting , publication.  


transpare ncy to the public.   The theory of change could be adjusted and modified as the 
impacts are evaluated as described below4. 
The Plan s hould intentionally advertise both the establishment of an evaluation 
methodology and the development of a clear analytic framework that can be used to report 
the results of AI systems in order to build  public trust.  
2.Education, workforce, innovation and competition
The Plan should leverage two existing efforts  to inform future investments in AI: the 
National AI Research Resources (NAIRR) pilot and the investments made by NSF’s 
Technology, Innovation, and Partnerships (TIP) Directorate. 
2a.  NAIRR exa mple : Much initial thinking was done by the NAIRR Task Force, which 
developed a detailed theory of change about the impacts of AI investment on these goals5.   
The NAIRR pilot has implemented many of the recommendations of the Taskforce, and has 
learned many lessons about implementation, measurement, and uptake6 
4 Source: Figure 4: NIST AI 100 -7, an initial public draft entitled “Towards an Approach for Evaluating the 
Impact of AI Standards” which is approved for, but waiting publications.  
5 https://www.ai.gov/wp- content/uploads/2023/01/NAIRR -TF-Final -Report -2023.pdf   
6 https://nairrpilot.org/  


Recommen dation s: The Plan should incorporate the Theory of Change in the NAIRR Task 
force report.   The Plan should reference and build on NAIRR pilot experience when making   
additional investments.   The Plan could consider further investments in the NAIRR which 
has already two years of experience and has excellent leadership.  
2b. NSF TI P example  The National Science Foundation’s TIP  Directorate has made great 
strides in understanding the impact of AI research investments in education, workforce, innovation and competition in funding  the “Industries of Ideas” project
7.  That has been 
made possible by about 15 years of investments in the STAR METRICS/UMETRICS 
program, initiated by the NSTC and OSTP (in cooperation with NITRD)8. 
The Indus tries of Ideas project represents  a fundamentally new measurement approach  to 
measuring impact that could inform the Plan. Because AI is a swiftly changing technology, the measures need to be timely.   Because the impacts are often very local, the measures 
need to be developed and validated by local stakeholders.   Because the impacts are on both 
jobs and earnings at the firm and individual level and,  the measures need to be developed 
using longitudinal linked employer -employee workforce data. Because the very use of the 
term “impact” requires a counterfactual, the measures need to be developed for both AI “touched” and AI “untouched” firms so the differences at the worker and firm level can be compared.    And finally, for the results to be useful and informative for decision making, the 
measurement framework needs to be understandable. The measurement framework in the “industry of ideas” meets all of these requirements, and has strong support with a number of state workforce development agencies, labor market information agencies,  economic development entities and training providers. The p roject  could be extended to  integrate 
skill demand data with dynamic information on real earnings and employment outcomes for the universe of workers in one state in a manner that is explicitly designed with input from a dozen engaged and committed other states to scale to their nee ds.  The approach 
would provide better timely, local, and actionable information to  all stakeholders .  The 
results would  align training, hiring, and workforce policies with the actual realities of the 
job market, transforming generic workforce development into targeted investments with measurable, evidence- based returns.  
7 https://iris.isr.umich.edu/tip   Lane, Julia, Jason Owen -Smith, and Bruce A. Weinberg. "How to track the 
economic impact of public investments in AI."  Nature   (2024): 302 -304.    
https://www.nature.com/articles/d41586 -024- 01721 -1  
 https://www.aei.org/research -products/report/the -industry -of-ideas -measuring -how -artificial -
intelligence- changes -labor -markets/   
8 Lane, Julia, and Stefano Bertuzzi. "The STAR METRICS project: current and future uses for S&E workforce 
data." Science of Science Measurement Workshop, held Washington DC. Vol. 12. 2010. 2. Valdez, B. and J. 
Lane, The science of science policy: a federal research roadmap.  Report to The Subcommittee on Social, 
Behavioral and Economics Sciences, Committee on Science, National Science and Technology Council. 
Washington, DC: Office of Science and Technology Policy, Executive Office of the President, 2008.   


 
 The core approach is grounded in the notion that measuring the  economy -wide impact of 
federal investments in AI R&D  requires identifying the people at the heart of these 
investments. It is people – not documents - who generate ideas, launch start- ups and 
influence the next generation of innovators through academic and professional networks. In emerging industries, where ideas matter a lot, people are the main value- creating unit — 
not machines or office floor space.   Tracking the impact of research investments requires 
capturing HR and finance information on all the people funded on research grants – students and doctoral or postdoctoral trainees as well as research staff and faculty collaborators.  Many of these individuals might never publish a paper, file a patent or 
become a PI themselves. But conducting research teaches them about cutting -edge 
algorithms and the application of these technologies.   The movement of these trainees and 
staff through to the wider economy, and the transmission of their ideas, is captured when they get jobs in the private sector. Their earnings and employment are recorded in state 
administrative data. This linkage — between academia and private- sector employment — 
is the new data layer that is being analyzed in a pilot  project in Ohio that is likely to be 
extended to other states. The initial results?  People employed: More than 470,000; 
Students and trainees employed: More than 290,000; Average number employed by a 
project lead: 22.   Additional work is being done on the impact on business survival and 
growth.  
 
Recommendations:  The Plan could recommend that science agencies build on and expand  the NSF/TIP investments in the  data infrastructure.  The Plan could recommend 
that the infrastructure be used by federal agencies to provide consistent and verifiable descriptive and causal estimates of the impact of  research investments  on the areas of 
interest.   
3. Producing Trusted AI Data  
Rigorous scientific discovery depends on high -quality data and reproducible empirical 
research, particularly in AI . The lack of high quality AI data is “ holding AI science back ”
9 - 
for example, the Protein Data Bank was critical to the discoveries that led to the 2024 
Nobel Prize in Chemistry.  Poor data and the resultant misinformation – particularly with 
the increased importance of data for artificial intelligence applications - c an lead to harmful 
outcomes for both science and the public.  
 
The problem to be addressed is how to signal what data can be trusted, and for what 
purpose. That requires information both on how data are produced and how data can be used. One approach that has been used in many other contexts is to develop community -
driven data quality standards.  There are at least three steps.   
 
 
 
9 https://www.technologyreview.com/2024/10/15/1105533/a -data -bottleneck -is-holding -ai-science -back -
says -new -nobel -winner/   


Step 1 is t o identify  a community of data producers and users  using a specific  dataset or set 
of datasets to inform their AI research  that .   
Currentl y, identifying a research  community by data use is challenging.  There ar e only 
relatively  ad-hoc methods to find what different datasets measure, what research has been 
done using those data by which researchers, with what code, and with what results .  The 
resulting lack of information leads to an underestimating of the value of investing in data 
which in turn leads both agencies and researchers to underinvest in creating, documenting, and disseminating datasets.  Since agencies cannot determine the return on investment in 
data, they cannot appropriately allocate resources.  And s ince researchers who create and 
improve datasets receive limited rewards for their subsequent and unknown reuse, their 
incentives to invest in and disseminate research data are dulled – many researchers who say they will share their data don’t
10.    
Recommen dation : Develop automated tools to identify communities of data users11 as t he 
first step to rewarding agency and researcher  contributions to data documentation and 
dissemination12. 
Step 2 is t o engag e that community to develop metadata standards that signal two types of 
data quality: both how the data is produced and for what ends it can be used.  
Data quality h as multiple dimensions, reflecting both its production , which can be 
documented by the community of data producers, and its use, which can be documented by the community of users  for different purposes .  For example, data series may be perfectly 
valuable for some uses, but their combination and comparisons will result in highly erroneous conclusions.  Metadata standards sh ould make fitness -to-purpose  clear to all 
potential users.  
Much wo rk has been done to develop metadata about how data are produced. NIH  and 
other agencies have invested heavily in developing measures of how data are produced and 
providing data access through federally supported data repositories (e.g.,  the Generalist 
Repository Ecosystem  Initiative13).  Tools like Googe Dataset Search have also been 
developed to find datasets14. 
10 Watson, Clare. "Many researchers say they’ll share data—but don’t."  Nature   (2022): 853.  
https://www.nature.com/articles/d41586 -022- 01692 -1  
11 Meng, Xiao -Li. "Data Democratization: An Ecosystemic Contemplation and Coordination."  Harvard Data 
Science Review  Special Issue 4 (2024).  
https://hdsr.mitpress.mit.edu/pub/o3qm160v/release/6?readingCollection=a6db7dff   
12 Lane, Julia, Alfred Spector, and Michael Stebbins. "An invisible hand for creating public value from 
data." Harvard Data Science Review  Special Issue 4 (2024).  
https://hdsr.mitpress.mit.edu/pub/6blzwgpg/release/4?readingCollection=a6db7dff   
13 Generalist Repository Ecosystem Initiative  
14 Sostek, Katrina, et al. "Discovering datasets on the web scale: Challenges and recommendations for Google 
Dataset Search."  Harvard Data Science Review  Special Issue 4 (2024).  
https://hdsr.mitpress.mit.edu/pub/psnc8zsr?readingCollection=a6db7dff   


Recommen dation : The Plan should recommend that agencies develop  metadata about 
data use and reward their respective communities for their contribution.  The Plan should 
support agencies in the development a community -driven  contextual m etadata f ramework 
that identifies  domain -specific metadata requirements  and contextual standards  about data 
use and quality  through community engagement. It should require the de�inition of  quality 
indicators by use relevant to speci�ic use cases , and that could be align ed with existing 
production standards, such as the FAIR principles15  and  the Federal  
Committee on Statistical Methodology  framework16.  It should  also standardize a dataset 
annotation process , whereby annotation work�lows  could be implemented  for pilot 
datasets  in a scalable manner.  The Plan should institutionalize validation procedures with 
expert communities, as well as the establishment of  quality control processes , and feedback 
mechanisms to ensure metadata quality . The  annotation patterns and best practices  would 
need to be documented to foster scalability.  
Step 3 is to create technology and incentives so that the community could contribute to 
developing information about data quality , so that data quality  will rapidly improve and 
data usage becomes more informed and rigorous.  This approach could mirror the approach 
developed by user platforms in the private sector, like Yelp, Airbnb, and Uber.  
The resul t could  be that the non- expert scientific research community and the public at 
large can have greater trust in the results.  transform how users find and engage in research 
communities with common research questions and interests and enable them to explore 
data- driven approaches and methodologies based on the use of data. In the long term, the 
approach could be used to  identify challenges in standardizing the ingestion of data extracted from different corpora into a unified dataset usage statistics schema and determine the future  automated  development and validation of these statistics . 
Recommen dation : The Plan could include investments in such technologies as  the 
National Data Platform17 and require that trust measures based on use be discovered, 
accessed and updated through federal funded and supported data repositories ,  
Please do not h esitate to contact me should you have any additional questions.  
Sincerel y, 
Julia  Lane , Ph.D.  
Professor Emerita ,  
NYU Wagner School of Public  Policy  
15 https://www.go -fair.org/fair -principles/   
16 https://www.fcsm.gov/assets/ﬁles/docs/2023- conference- docs/F1.3_Mannshardt.pdf  
17 https://nationaldataplatform.org/   


