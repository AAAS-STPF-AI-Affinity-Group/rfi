Page 1 of 7 March 14, 2025  
TO: Office of Science & Technology Policy and NITRD National Coordination 
Office, National Science Foundation  
FROM:  Cameron F. Kerry and Joshua P. Meltzer*† 
RE: Request for Information on Artificial Intelligence Action Plan  
The Artificial Intelligence Action Plan  is not being written on a  blank sheet because 
executive orders defining AI policy in the  first Trump administration  remain in force .  
With then-CTO of the United States  Michael Kratsios leading the work , Executive 
Order s 13859  (“Maintaining  American Leadership on Artificial Intelligence”) on February 
11, 2019 (the “2019 Order”), and  13960 (“Trustworthy Artificial Intelligence in the United 
States Government”)  on December 3, 2020 (“the “2020 Order”)  put American leadership 
and innovation in AI at the center of U.S. policy.  This focus is consistent with Executive 
Order 14179.  The AI Action Plan should therefore maintain key elements of these  
orders, which recognize d that American leadership and innovation in AI depend on  AI 
that is trustworthy and responsible and on robust international engagement.   
Some five years after the 2019 and 2020 Orders , AI has gone from a nascent 
technology to a transformational  factor in almost every sphere , altering the global AI 
landscape and marketplace  since  that time . In particular, the release of large generative 
AI models has accelerated development, deployment, and capabilities of AI and 
captured public imagination.  The r ecent r elease of China’s DeepSeek R1 demonstrates 
not only the diffusion of knowledge and development, but also the potential for models 
that are fine -tuned by distillation of information from larger models and displaying 
processing steps (or “chain of thought”), possibly enabling  models with wider, more 
efficient, and specific uses and greater  explainability and interpretability of outputs.  
During this time , governments around the world have accelerated their own efforts to 
benefit from the opportunities of AI and  manage its risks through national policies and 
strategies and intergovernmental initiatives.  These include the Hiroshima Process and 
Principles of the G7, the Global Digital Compact adopted by the United Nations, joint 
† Respectively Ann R. Andrew H. Tisch Distinguished Visiting Fellow in Governance Studies, and Senior Fellow 
at The Brookings Institution.  These comments are submitted in individual capacities based on work in the 
field of AI policy and t are not made on b ehalf of Brookings  or any other entity ; the affiliation is listed for 
identification purposes only.  


Page 2 of 7 principles of the African Union and ASEAN,  and legislation enacted by the European 
Union and under way  in Brazil and India among other countries.   
This array of developments in technology and international initiatives around AI 
governance calls for even greater government engagement than in 2020 to support 
innovation,  promote responsible AI adoption , and engage with international partners.  
The previous Trump administration AI orders  
 The 2019 Order  put in place  the American AI Initiativ e with the aim of maintaining 
“[c]ontinued  American Leadership in AI” and “shaping the global evolution of AI.”  To 
accomplish this, the order called for  “a concerted effort to promote advance s in 
technology and innovation, while p rotecting  American technology, economic and 
national security, civil liberties, privacy, and American values and enhancing 
international and industry collaboration with foreign partners and allies.”   It articulated 
five principles to support these goals: (1) driving technological breakthroughs, (2) 
development of technic al standards and “reduc[ing] barriers to safe testing and 
deployment…,” (3) training AI skills, (4) “foster[ing] public trust and confidence” in AI and 
protecting civil liberties, privacy, and American values, and (5) promoting “an 
international environment that supports American AI research and innovation and opens 
markets ….”  These policies and principles remain valid in 2025 . 
The 2020 Order focused on the role of federal agencies in adopting and promoting AI  
and leveraging the impact on the marketplace of federal  agencies as purchasers of AI 
systems.  It  declar ed as a matter of national  policy that federal agencies  should  promote 
and deploy innovative AI “ that fosters public trust , builds confidence  in AI, protects our 
Nation’s values, and remains consistent with all applicable laws, including those relative 
to privacy , civil rights, and civil  liberties .” To these ends, the  order established a set of 
principles for agencies  to apply , which included benefits that “significantly outweigh” 
risks, accuracy, reliability, and effectiveness; safety, security, and resiliency; and 
traceability, human responsibility, monitoring, transparency, and accountability.  
In significant respects, President Biden ’s executive  order of October 30, 2023 buil t on 
these  key elements  of the  2019 and 2020 Orders  even as it shifted priorities and 
established more detailed requirements for federal agencies and AI developers to 
assess AI systems.  The 2023 Order likewise  aimed at  maintaining U.S. leadership and 
leveraging federal agencies to accelerate adoption  and model best practices for the 
marketplace .  It too focused on protecting national security .  In assessing what actions 
taken pursuant to that order should be undone, therefore, the government should retain 
those that buil t on the 2019 and 2020 Orders  
Measures  for USG AI management  and protection of national security  
Chief AI Officers and AI Council  


Page 3 of 7 One key action that  the AI Action Plan should retain is the appointment  Chief AI Officers  
in federal and the AI Council  established  under  Section 10 of the 202 3 Order . Notably , 
the 2023 O rder included among the functions of the se officers  “carrying out the agency 
responsibilities described [in the 2020 Order].”   This is consistent with the requirement 
in the 2020 Order for “responsible official(s)” to carry out these functions, but with the 
effect of elevating the functions commensurate with the enlarged importance of AI .  
Keeping the structure  in place will ensure  the visibility and priority of AI adoption and 
management in federal agencies.   The Chief AI Officers take o n increased importance  
considering  the recent termination of the Presidential Innovation Fellows Program, since 
the 2020 Order provided for PIF experts to work within agencies to design AI adoption.  
Moreover, the termination of  numerous  probationary employees will sweep in many  of 
the more than 200 AI experts hired in the “AI talent surge” under the 2023 Order.  
Eliminating the Chief AI Officers as well would deal a serious setback to AI management 
in federal agencies.   
Modeling a risk -based approach to AI  
The agency processes should continue to include measures to ensure that AI systems 
deployed by federal agencies are responsible, safe, and secure, building on the 
principles and policies of the 2020 Order.  Like both the 2019 Order  and 2023 Order , the 
AI Acti on Plan  should provide for the Office of Management and Budget  (OMB) , in 
consultation with the Office of Science and Technology Policy , the AI and Crypto Czar, 
and the National Security Council , to issue guidance to federal agencies to carry out 
these goals .  This guidance should expand on the OMB guidance  of November 17, 
2020 , including aspects of the further OMB guidance  of March 28, 2024  that reinforce 
the functions described in the 2020 Order .  
The function s in the 2020 Order  include risk management  at the agency level . The 
November 17, 2020  OMB guidance document on AI applications called for encouraging  
both innovation and stewardship , and identified a set of risks “that must be carefully 
assessed and appropriately addressed” with risk assessment and risk management.  
The 2023  National Institute of Standards and Technology AI Risk Management 
Framework provides a n adaptable tool for these purposes, amplified by the principles of 
the 2020 Order and relevant parts of the 2023 Order and OMB guidance issued 
thereunder. .  Just as federal agencies have been required to implement the NIST 
Cybersecurity Risk Management Framework, they should be similarly required to apply 
the AI RMF to assessing their AI risk profiles .    
Standards development  
There  are additional tools in place to manage AI risk and conduct robust evaluations of 
AI systems as result of progress in standards development  in recent years .  The joint 
International Standards Organization and International Electrotechnical committee AI 


Page 4 of 7 standards, ISO/ IEC JTC1/SC42 , has publis hed 34 AI standards , including most 
significantly ISO/IEC 42,001 on risk assessment.  The 202 0 Order called for federal to 
“continue to use voluntary consensus standards developed with industry participation 
where applicable.”  Continuation of this policy would encourage f ederal agencies to use 
standards like ISO/IEC 42,001  as references  to identify and manage risks and promote 
AI that fosters public trust  and builds confidence  in AI as well remains consist ent with 
American laws and values.  
The system of voluntary  consensus standards development has been a bedrock 
principle of U.S. technology policy towards standards .  Like the 2020 Order, the 2019 
also referred to standards development, calling for NIST to develop a plan for federal 
engagement in technical standards, which was issued in August  2019.   The 2023 Order 
in turn called for a coordinated government effort  to work with international partners  “to 
drive the development and implementation  of AI-related consensus standards, 
cooperation, and information sharing.”  This work should continue in support of 
America’s longstanding approach to standards.   For more on standards development 
policy, see “ Small Yards, Big Tents: How to Build Cooperation on Critical International 
Standards ” (Brookings, 2023).  
To reinforce the enlarged  and respected role that NIST plays in AI policy, standards 
development, and pre -standardization research, the AI Action Plan should include 
support for an increase in NIST’s budget request on the order of the 2024  request  for an 
increase of $358.5 million.  
National security risks  
The increased risks associated with generative AI include national security risks. The 
scope of wide range of applications enable d by foundational models is still being 
determined even by the developers of these systems.  Nonetheless, some risks are 
evident, and these include dual -use capabilities that pair beneficial uses with national 
security risks – for example, Nobel -Prize -winning protein folding discoveries that can 
also be used to discover bioweapons.  Both in the development of the 2023 Order and 
in its implementation, the federal government conducted a thoroughgoing study of these 
implications in a range of areas.  This review built on steps in the first Trump 
administration to heighten export controls relating to AI and emerging technologies.  The 
AI Action Plan in turn should build on the controls established under  the 2023 Order 
rather than start on a clean sheet .  It should recognize that, as illustrated by the 
emergence of DeepSeek , the pace of AI development and capabilities is a moving 
target.  Rather than erase the national security aspects of the 2023 Order, the AI Action 
Plan should refresh the review undertaken for the 2023 Order and anticipate continuing 
to so at intervals.  


 
Page 5 of 7 
 International Engagement  
The 2019 Order affirmed the importance of international engagement in maintaining 
U.S. leadership in AI.  Achieving “an international environment that supports American 
AI research and innovation” and opening markets require s active engagement on the 
part of the U.S. Government and other U.S. leaders in AI.   
The first Trump administration had significant achievements on the international front.  It 
negotiated the  OECD Recommendation on AI  – a set of principles very aligned  with 
those in the 2019 Order .  The OECD principles call for  the development of “robust, 
secure and safe” AI systems, the adoption of risk management approaches to mitigate 
risks of AI systems, and international cooperation to promote the development of 
“trustworthy AI”.  The U.S. also joined the launch of the Global Partnership on AI in 2020  
(which now operates in alliance with the OECD, coordinating efforts and platforms while 
broadening the base of countries participating ).  The OECD has been a valuable 
channel for balanced, research -based policy development that incorporates extensive 
stakeholder involvement.  GPAI expands the networks of experts informing global AI 
policy.  
The G7 operates a s a key hub in the networks of AI governance .  It has been an early 
mover in initiatives in like the OECD Princi ples, and the more recent Hiroshima Process 
embodies an approach to AI governance consistent with U.S. policy.  See “ Network 
Architecture for Global AI Policy ” (Brookings, 2025).  Alongside these  initiatives , U.S. 
developers and deployers of AI have taken collaborative steps through commitments to 
the Hiroshima Principles now monitored by the OECD (and building on commitments 
given to the White House in 2023), and  the   Partnership for Global Inclusivity on AI  
between the U.S. government and leading American AI companies to advance AI 
capacity and adoption in the developing world. The AI Action Plan should continue  
participation in such international partnerships , both to sustain U.S. leadership and open 
markets for U.S. companies.  
These partnerships should include the international network of AI safety institutes , which  
emerged from the Bletchley AI Summit .  There , Elon Musk told British Prime Minister 
Rishi Sunak  the government ’s role in AI safety is like a  “referee” and “”when you talk 
about superintelligence...then there is a role for government to play to safeguard the 
interest of the public .”  As discussed above, the risk principles of the 2019 and 2020 
Orders include AI that is “safe, secure, and resilient.”  To this end, the U.S. AI Safety 
Institute has collaborated with foundational model developers and worked bilaterally 
with allies like the U.K. to help address risks such as bioweapon development and taken 
a leadership role by convening an international network  of safety institutes and their 
equivalents  to explore methods of measurement and testing foundational models . 
Keeping abreast of such risks and participating in these networks is essential to 


Page 6 of 7 maintaining American leadership in AI technology and governance.   See M ark 
MacCarthy, “A tec hnical AI government agency plays a vital role in advancing AI 
innovation and trustworthi ness” (Brookings , March 14, 2025).  
The U.S . cannot afford to abdicate the field in international AI governance.  The array of 
international initiatives in this space continue s to grow with or without the United States.  
For example, after the U.S. introduced a resolution on AI in the United Nations General 
Assembly on “safe, secure and trustworthy [AI] systems for sustainable development” 
that was passed unanimously in March of last year, China introduced its own AI 
resolution in July.  The Chinese resolution (which also passed unanimously) covered 
much of the same ground, but with a greater emphasis on state control of outcomes.  If 
the United States does not build alliances in the AI space, China and othe rs will step in 
to lead.  The U.S. needs to remain  an effective  voice in the international discussion of AI 
opportunities and risks.  
Comprehensive privacy protection  
The data aggregated for training AI systems  increasingly encompass everything 
available online, accumulating large volumes and varieties of  personal information  and 
as machine learning datasets .  As a result, the output of these systems can cause 
inadvertent data breaches and privacy intrusions.  In addition, the power and granularity 
of AI increases the power of data analytics to make detailed inferences about 
individuals.  Studies by the Pew Research Ce nter and by KPMG  (among others ) show 
majorities  of consumers  are concerned  that information collected by AI developers may 
be used in ways not originally intended and my comp romise privacy an d data security.   
See IAPP, Consumer  Perspectives of Privacy and Artificial  Intelligence  (March 14, 
2025).  
Protection of individual privacy is therefore a necessary component for trustworthy AI.  
Numerous members of Congress in both parties as well as stakeholders have pointed 
out that comprehensive privacy legislation would establish a foundation for AI policy.  
See “ How privacy legislation can help address AI ” (Brookings, 202 3) and “Protecting 
privacy in an AI -driven world ” (Brookings, 2020) . Bipartisan reports in both the Senate 
and House concluded that federal privacy legislation would establish a baseline of 
protection to support AI development and adoption .  The House task force devoted a 
section of its report to data privacy and reported that “[t]houghtful and effective data 
privacy policies and protections will support consumer confidence in the responsible 
development and deployment of AI systems.” The Senate bipartisan working group 
supported “a strong comprehe nsive federal data privacy law to protect personal 
information ,” addressing data minimization, data security, consumer data rights, consent 
and disclosures, and data brokers.  In the current Congress, a task force of the 


Page 7 of 7 House Committee on Energy & Commerce has initiated  an inquiry on the elements of 
privacy legislation    
In the absence of federal legislation, an increasing number of states are stepping, with 
19 states having adopted varying forms of comprehensive privacy legislation  and 
18adopting legislation regulating AI.  This creates an irregular patchwork of state laws 
creates uneven protections for consumers and complex compliance for businesses.   
The AI Action Plan should include a plan for the Executive Branch to engage with 
Congress and stakeholders toward adoption of comprehensive federal information 
privacy legislation that will establish a national baseline for trustworthy use of personal 
information.  
**  *
Thank you for the opportunity to provide these comments.  We welcome any questions 
or follow -up you may have.   
Respectfully submitted,‡ 
Cameron F. Kerry  
Joshua P. Meltzer  
‡ This document is approved for public dissemination and contains no business -proprietary or confidential 
information.  The contents may be reused by the   government in developing the AI Action Plan and associated 
documents without attribution.  


