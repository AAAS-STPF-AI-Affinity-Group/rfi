5200 Butler Pike, Plymouth Meeting, PA  19462 
e      |    w  www.ecri.org  
NaƟonal AI AcƟon Plan – Request for InformaƟon 
On behalf of the Oﬃce of Science and Technology Policy (OSTP), the NITRD NCO requests input 
on the Development of an ArƟﬁcial Intelligence (AI) AcƟon Plan (“Plan”).  
This Plan, as directed by a PresidenƟal ExecuƟve Order on January 23, 2025, will deﬁne the 
priority policy acƟons needed to sustain and enhance America's AI dominance, and to ensure 
that unnecessarily burdensome requirements do not hamper private sector AI innovaƟon.  
Through this Request for InformaƟon (RFI), OSTP and NITRD NCO seek input from industry 
groups, academia, and private sector organizaƟons – including concrete AI policy acƟons. 
In response to the RFI on the AI AcƟon Plan, this brieﬁng presents pracƟcal 
recommendaƟons designed to sustain and enhance America's leadership in the use of 
AI in healthcare. 
ECRI’S Key Takeaways 
Game-changing potenƟal: AI has vast potenƟal to opƟmize the American healthcare
system by improving eﬃciency, reducing costs, and guiding treatment plans based on
massive datasets and health research, improving care quality and paƟent outcomes.
AI is not infallible: It’s dangerous to implement AI in healthcare seƫngs without a
rigorous tesƟng, implementaƟon, and monitoring protocol.
Strong data is essenƟal: AI systems are only as good as the model generated from
the data on which they’re trained. Shortcomings in the data used in healthcare
applicaƟons could harm paƟents.
Warning against AI driŌ: It’s essenƟal to conƟnuously monitor AI performance with
periodic assessments to protect against degradaƟons of the AI model due to changes
in data, environments or clinical pracƟces.


2 AI in Healthcare: Improving Eﬃciency & Quality 
By implemenƟng clear disclosure pracƟces, secure tesƟng environments, and rigorous data 
integrity validaƟon, we can ensure that American AI systems are not only robust and reliable but 
also free from unnecessary burdens. These measures lay the groundwork for a compeƟƟve and 
dynamic private sector, driving forward innovaƟon that keeps us ahead of internaƟonal 
compeƟtors and reinforces our leadership in arƟﬁcial intelligence. AdopƟng the standards 
outlined in the recommended AI strategies below will empower America to lead the global AI 
revoluƟon in healthcare, solidifying the United States’ posiƟon as the foremost innovator in the 
ﬁeld.  
Unlocking AI’s Game-Changing PotenƟal in Healthcare 
AI has the potenƟal to revoluƟonize healthcare by enhancing diagnosƟc accuracy, predicƟng 
paƟent risks, personalizing and opƟmizing treatment plans, and improving healthcare 
accessibility, ulƟmately improving paƟent outcomes. By automaƟng rouƟne tasks and analyzing 
vast amounts of clinical data, AI can support clinicians in making faster, more informed decisions 
while reducing errors and administraƟve burdens. These beneﬁts can reduce healthcare costs 
and healthcare staﬀ burn-out. However, AI must be applied with proper safeguards. 
AI Misuse Could Harm PaƟents 
Common challenges with AI technology—such as transparency, performance degradaƟon and 
privacy and security concerns—can have unique and dangerous consequences in healthcare.  
High quality AI models are dependent upon the robustness of the underlying algorithms and the 
strength of the data they rely upon. AI algorithms trained on limited or non-representaƟve 
paƟent datasets risk producing biased or inaccurate results, which can lead to misdiagnoses, 
ineﬀecƟve treatments, or dispariƟes in care. Without diverse and comprehensive data, these 
algorithms may fail to generalize across diﬀerent populaƟons, reinforcing exisƟng healthcare 
inequaliƟes rather than improving paƟent outcomes. 
When AI models are based on bad data, they can increase the chances of a medical error or 
adverse event for a paƟent. Medical errors generated by AI could compromise paƟent safety 
and lead to misdiagnoses and inappropriate treatment decisions, which can cause injury or 
death. Staﬀ may also have diﬃculty determining when adverse events are aƩributable to AI, 
making such errors harder to track.  
ECRI encourages conƟnued advancement in AI-enabled medical devices that can improve 
healthcare eﬃciencies, improve clinical outcomes, and improve healthcare accessibility, and 
stresses that these improvements rely on robust data and sound development pracƟces. 
Ensuring these standards are met is crucial for maintaining American dominance in AI 
innovaƟon. 


3 Recommended AI Strategies 
ECRI recommends the following acƟons in the applicaƟon of AI in healthcare to improve 
eﬃciency, improve paƟent outcomes, decrease clinical burden, make care more accessible to 
rural communiƟes, and improve paƟent safety. These recommendaƟons are designed to 
support innovaƟon and compeƟƟveness in the private sector, while maintaining robust 
safeguards that promote accountability and trust in the American healthcare system.  
Promote Secure, InnovaƟve TesƟng Environments  
Promote the use of controlled tesƟng environments—oŌen called sandboxes—to validate the 
performance of the highest risk AI applicaƟons (e.g., those that treat or diagnose for criƟcal 
paƟent condiƟons). 
In healthcare, AI-powered applicaƟons associated with higher risk are oŌen those that
inﬂuence clinical decision-making and diagnoses.
These tesƟng and development spaces allow innovators to rigorously assess
performance and security before full-scale deployment, ensuring that high-stakes
systems work as intended.
ImplemenƟng a sandbox tesƟng approach is feasible, as a risk straƟﬁcaƟon framework is
already in place. The InternaƟonal Medical Device Regulators Forum (IMDRF) risk
categorizaƟon for SoŌware as a Medical Device (SaMD) is useful for clinical teams to
understand the impact of applicaƟons based on its intended use. The FDA adopted this
framework in their “SoŌware as a Medical Device (SaMD): Clinical EvaluaƟon” guidance
issued on December 8, 2017.
The IMDRF risk framework idenƟﬁes two major factors which guide determinaƟon of the
risk category:
oSigniﬁcance of informaƟon provided to the healthcare decision (i.e., to treat or
diagnose, to drive clinical management, or to inform clinical management)
oState of healthcare situaƟon or condiƟon, which idenƟﬁes the intended user,
disease, or condiƟon (i.e., criƟcal; serious; or non-serious healthcare situaƟons).  
Require Comprehensive Data Strength Reviews  Advocate for thorough assessments of the data used during the training, tuning, and tesƟng of 
AI models. By examining the datasets strengths (e.g. avoiding bias and data that does not 
represent the target populaƟon) and by encouraging a balanced mix of real-world and syntheƟc 
data to help avoid bias, companies can beƩer idenƟfy and address dispariƟes among various 
populaƟon groups.  


4 
 Require Rigorous Deep Learning Oversight 
Support robust reviews of the large databases behind deep learning models. Developing 
naƟonal datasets—validated to be free from inadequate data points—would oﬀer developers a 
trusted, standardized foundaƟon for training deep learning algorithms. This resource would 
streamline the development process by reducing the Ɵme and expense spent on data cleaning 
and validaƟon, ulƟmately acceleraƟng innovaƟon. Economic beneﬁts include lower 
development costs, faster Ɵme-to-market for new soluƟons, and enhanced investor and 
consumer conﬁdence, thereby reinforcing America's leadership in the global AI race. 
 
Standardize a Bias Assessment Process One of the major risks of AI is its potenƟal for bias. AI models are only as good as the data on 
which they are trained, and biased data will result in biased models. Establish a standardized 
bias assessment process for training, tuning, and tesƟng datasets.  
 While social and demographic bias is a known risk to most developers, other types of 
bias exist. For example: 
o The hospital informaƟon systems that the AI model acquires training data from, 
including those systems’ workﬂows, may not be representaƟve of the intended 
use of the device or the intended paƟent populaƟon.  
o VariaƟons in breast Ɵssue density or diﬀerences in the type of imaging system 
collecƟng the data could skew diagnosƟc algorithms. 
 Failing to miƟgate these biases could lead to misdiagnoses or ineﬀecƟve treatments, 
which would not only compromise paƟent care but also result in costly failures that 
undermine conﬁdence in American AI systems. 
 Bias assessment for training/tuning/tesƟng datasets can include allowing syntheƟc data 
and hybrid datasets where syntheƟc data rounds out real-world data to prevent bias, 
and interrogaƟon of the enƟre dataset and reporƟng of bias between subpopulaƟons.  
 For deep learning models, this means an assessment of the massive databases used for 
training and assurance that the model is being built from unbiased data. 
 
Require Clinical ValidaƟon  Requiring clinical validaƟon can improve device performance, which in turn may boost 
consumer adopƟon rates and reinforce U.S. leadership in AI innovaƟon.  
 Clinical validaƟon is oŌen missing in FDA cleared, approved or granted AI-enabled 
medical devices which could be a factor contribuƟng to poor performance of these 
devices clinically and consumer adopƟon hesitancy.  


5 According to Nature Medicine,, from 2016 to 2022, the number of authorizaƟons with
missing clinical validaƟon data surpassed the numbers of retrospecƟvely and
prospecƟvely validated devices in every year since 2016.1
Foster public trust in AI-enabled SoŌware as a Medical Device (SaMD)/SoŌware In
Medical Devices (SiMD) tools as a key strategy to drive consumer adopƟon and reinforce
U.S. leadership in AI innovaƟon. To achieve this, ECRI emphasizes the importance of
structured evaluaƟon processes that conﬁrm these tools are safe and eﬀecƟve. Clear
standards for assessing AI-enabled SaMD/SiMD can help ensure these devices deliver
clinical value, especially in situaƟons where formal oversight may be limited.
ECRI recommends that healthcare providers and payers independently assess these
tools to conﬁrm their clinical value, especially in cases where regulatory oversight is
limited. Building conﬁdence in AI-enabled medical devices through transparent
evaluaƟon processes will encourage broader adopƟon and accelerate economic growth.
Require Transparency and StandardizaƟon of Disclosed Data  Encourage a consistent approach where companies share key technical details with 
consumers—such as through standardized “model cards” or “AI nutriƟon fact labels”—to 
clearly explain how AI systems are built and funcƟon.   
This straighƞorward disclosure supports market conﬁdence and enables informed
decision-making.
Measures of recall and precision should be included.
Enhanced transparency builds public trust and acceptance, leading to stronger global
demand for innovaƟve American AI soluƟons.
Require Safety ReporƟng Clinical literature indicates there may be underreporƟng in regulatory databases and industry 
press. Require and consolidate AI/ML product safety reporƟng to address paƟent harm or 
preventable harm.  
An AI product must be regularly monitored to ensure ongoing eﬀecƟveness and safety.
This may entail ensuring reliable communicaƟon with both product users and the
product vendor/developer, regularly assessing the availability of alternaƟve, improved AI
products, assembling a commiƩee to review the AI product periodically, performing
1 Chouffani El Fassi, S., Abdullah, A., Fang, Y. et al. Not all AI health tools with regulatory authorization are clinically 
validated. Nat Med  30, 2718–2720 (2024). https://doi.org/10.1038/s41591-024-03203-3 , last accessed March 11, 
2025 


6 audits of the AI model's performance, maintaining a record of AI product adverse 
events, and monitoring event reporƟng in regulatory databases.   
For healthcare applicaƟons coordinated under the Assistant Secretary for Technology
Policy / Oﬃce of the NaƟonal Coordinator for Health InformaƟon Technology (or
government agencies other than FDA), consider guiding users to perform a risk
assessment and to review the AI/ML product periodically to idenƟfy driŌ or bias, where
monitoring frequency should be deﬁned based on intensity of use of and criƟcality of
the applicaƟon. Refer to Risk Assessment ConsideraƟons from ISO 14971.
MiƟgate Eﬀects of AI DriŌ Over Time  
DriŌ occurs when an AI model’s performance degrades due to changes in data, environments, 
or clinical pracƟces.   
In healthcare AI applicaƟons, driŌ can happen as paƟent demographics shiŌ, medical
guidelines evolve, or new treatments emerge, causing the model to become less
accurate. Without regular monitoring and updates, an AI tool that was once reliable may
start making incorrect predicƟons, leading to potenƟal safety risks for paƟents.
To ensure AI remains safe and eﬀecƟve, healthcare organizaƟons must implement
conƟnuous validaƟon, retraining, and human oversight to detect and correct driŌ before
it impacts care.
Manufacturers must also maintain transparency with regulators and providers on
planned and unplanned soŌware updates.
The current FDA recommendaƟon allowing for a predetermined change control plan
(PCCP) speciﬁes the modiﬁcaƟons the vendor intends to implement without further
regulaƟon in the event that a change to the AI model becomes necessary.  This provision
allows ﬂexibility and innovaƟon while maintaining accountability and building consumer
trust.
Train Healthcare Professionals Training healthcare professionals to interpret AI results criƟcally is essenƟal to prevent errors 
and ensure opƟmal outcomes.    
Human-AI interacƟon performance depends on factors like tailored integraƟon,
minimizing cogniƟve burden, addressing automaƟon bias, and fostering trust through
transparent communicaƟon.
AutomaƟon bias, where users overly rely on AI-generated results without criƟcal
evaluaƟon, is a signiﬁcant concern.


7 
 Total Systems Safety 
AI risk assessment processes should be part of a Total Systems Safety (TSS) approach. 
Healthcare organizaƟons should examine potenƟal systems factors that could contribute to 
failures in AI technologies. TSS principles are also reﬂected in the Health AI Partnership (HAIP) 
framework for risk management in AI-based clinical applicaƟons. 
By adopƟng these recommendaƟons and tacƟcs, the naƟonal AI AcƟon Plan can 
establish a framework that safeguards paƟent safety and economic growth 
opportuniƟes while encouraging American innovaƟon. 
 
ECRI: Advancing EﬀecƟve, Evidence-Based Healthcare  
 
ECRI has been a leader in paƟent safety, healthcare quality, and evidence-based medicine for 
over 50 years. As an independent, nonproﬁt organizaƟon, ECRI provides trusted guidance to 
healthcare leaders and policymakers to improve safety, quality, and cost-eﬀecƟveness in 
healthcare. With the strictest conﬂict of interest rules in the industry, ECRI remains a leading 
resource for unbiased, evidence-based decision-making. 
ECRI is federally cerƟﬁed as a PaƟent Safety OrganizaƟon (PSO) and collaborates with over 1,800 
healthcare faciliƟes naƟonwide, including health systems, hospitals, ambulatory surgery 
centers, and aging services. By analyzing ECRI’s database of over 7 million paƟent adverse 
events and “near misses,” ECRI idenƟﬁes the greatest threats to paƟent safety and care 
eﬃciency and develops evidence-based soluƟons. 
ECRI partners with government and public agencies, payers, medical liability insurers, the 
ﬁnancial, pharma, and biotech communiƟes, medical device manufacturers, and healthcare 
providers. For federal agencies, ECRI provides evidence-based research, paƟent safety 
advisories, and policy guidance. Examples include: 
 FDA: Medical device safety and regulatory insights 
 CDC: InfecƟon prevenƟon and paƟent safety advisories 
 VA and DoD: PaƟent safety iniƟaƟves for veterans and military personnel 
 CMS and HRSA: Evidence-based research and policy guidance to inform healthcare 
programs 
ECRI is the only organizaƟon that leads independent evaluaƟons of medical devices. Each year, 
ECRI tests thousands of devices, empowering healthcare organizaƟons to make informed 
procurement decisions. ECRI is one of the few Evidence-based PracƟce Centers (EPCs) 
designated by the U.S. Agency for Healthcare Research and Quality (AHRQ).  
 


8 Contact 
ScoƩ Lucas, PhD, PE 
Vice President, Device Safety, ECRI 
www.ECRI.org  
This document is approved for public disseminaƟon. The document contains no business-
proprietary or conﬁdenƟal informaƟon. Document contents may be reused by the government in 
developing the AI AcƟon Plan and associated documents without aƩribuƟon. 


