Developers Alliance Response:  
Request for Information on the Development of an Artificial Intelligence Action Plan  
This document is approved for public dissemination. The document contains no 
business-proprietary or confidential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without attribution. 
March 14, 2025 
Office of Science and Technology Policy (OSTP), Networking and Information Technology 
Research and Development (NITRD), and National Coordination Office (NCO), 
The Developers Alliance, the leading advocacy organization representing application 
developers, the companies they lead, and the industries that depend on them, applauds the 
Trump Administration, the Office of Science and Technology Policy (OSTP), and the Networking 
and Information Technology Research and Development (NITRD) National Coordination Office 
(NCO) of National Science Foundation for inviting public comment on the formulation of a 
National Artificial Intelligence (AI) Action Plan.  The U.S. digital economy is at an inflection point. The 2022 arrival of generative AI tools, 
including ChatGPT, Google’s Gemini, Microsoft’s Copilot, Meta AI, and others, planted a flag on 
America’s AI development dominance. The January 20 U.S. launch of China’s DeepSeek was a 
startling wake-up call that America and the U.S. government cannot take that dominance for 
granted.  
However, AI is not just the purview of large tech companies, and it is not a monolithic sector 
limited only to large language models (LLM). Small companies are building and using AI tools to 
deliver innovative solutions across myriad industries, including healthcare, law, and education. It 
is critical that these innovators aren’t lost in the debate. Their growth and the thousands of new 
AI-focused companies emerging in the coming years cannot succeed if they are saddled with 
regulatory burdens that only the largest companies can comply with.  The U.S. must strike the right regulatory balance and create a supportive environment for AI 
development. To achieve this, the Administration and Congress must simultaneously invest in 
software education and training and find the right balance of data privacy and other consumer 
protections to ensure the U.S. continues to lead the world in responsible AI development.  
Investing in Education and Training : 
Since the dawn of the internet, software developers have been a rapidly growing profession, 
and experts predict that they will continue to be highly in-demand for the foreseeable future. 
According to the U.S. Bureau of Labor Statistics, the software development profession is 


projected to grow 17 percent from 2023 to 2033, much faster than the average for all 
occupations. There are an average of 140,000 job openings in the field every year.  
The growth of AI is as seminal of an event in our nation’s economy as the dawn of the internet. 
As the rise of the tech sector required workers to upskill from manufacturing jobs to software 
development, we as a nation need the workforce to upskill from coding the more complex AI 
engineering. 
According to a 2023 article in the Harvard Business Review, the half-life for digital skills is less 
than five years. The federal government has a critical role in supporting education, training, and 
retraining programs to ensure the U.S. workforce can meet the growing demands of the 
burgeoning AI ecosystem and ensure we, as a nation, have the tools to remain competitive with 
our fast-growing international rivals.  
Balanced Data Collection and Privacy Laws  
Data is key in developing high-quality AI-powered apps, so the issues of privacy and data 
collection and use are fundamentally intertwined with developers' ability to build, manage, and 
update AI-powered applications.  
First and foremost, developers need a single unified data privacy law. Currently, 20 states have 
enacted comprehensive data privacy laws, all with their own version of requirements and 
restrictions. Some states have adopted, and more are considering — radical data collection and 
use restrictions that go far beyond the strict standards set by California and the European 
Union. For example, the Maryland Online Data Privacy Act (MODPA) wholly ignores data's 
critical role in the digital economy. MODPA, and bills like it in Maine, New York, and Vermont, 
severely limit app-based businesses' ability to collect basic, non-sensitive data that helps them 
understand, serve, and communicate with their customers and is critical in building and training 
AI models.  The lack of a balanced federal data privacy law that preempts radical state regulations is an 
increasingly significant hurdle to U.S. innovation and economic growth, and passing such a 
federal privacy law should be a critical part of the Administration’s AI Action Plan. 
AI Model Consumer Protections and Regulations 
We appreciate that the Administration understands the stakes and potential effects of stifling AI 
innovation and leaving it ungoverned.  
AI is tech’s cutting edge, but U.S. states introduced over 700 AI bills in 2024, and hundreds 
more are in the works. Many of these laws threaten to stifle AI’s astonishing potential to boost 
productivity and enhance Americans’ lives. Colorado, for instance, recently passed a sweeping 
new law requiring companies that use AI to conduct costly audits of their AI to prove they aren’t 
using the technology to circumvent anti-discrimination laws. Colorado’s broad proposals are 
prohibitively costly and risky for AI startups, jeopardizing the state’s tech sector — and, more 
2 


 
seriously, potentially stamping out innovative new AI uses and technologies, including for 
beneficial things like medical research or boosting agricultural productivity. 
 
The best solution is to create a unified federal approach to managing the risks of the most 
powerful models and allowing states to address concerns regarding specific risks posed by 
consumer-facing apps. This begins with an understanding of current law and its applicability to 
AI, not creating duplicative statutes, and not saddling smaller innovators with insurmountable 
red tape.  
 
Colorado’s AI law is a great example. While discrimination is already illegal, the Colorado 
Anti-Discrimination in AI Law created a new regulatory framework specific to AI. This framework 
includes requiring anyone who uses or develops AI to conduct annual impact assessments, 
regardless of any complaints, evidence, or history of inappropriate use of the underlying AI. 
While these assessments are designed to prevent companies from using AI to engage in 
discrimination, they represent a significant financial and resource burden to small companies 
and are unnecessary.  
 
AI is not one-size-fits-all, and regulators cannot take a sledgehammer approach to the industry. 
AI regulations need to be uniform, tailored to the specific risks posed by industry or AI type, and 
take into consideration that thousands of small companies are developing, deploying, and 
driving AI innovation every day.  
 
On behalf of the developers and the companies that employ them, the Developers Alliance 
thanks OSTP and NITRD NCO for the opportunity to respond to their Request for Information on 
a National AI Action Plan.  
 
 
Sincerely, 
 
Jake Ward 
Chairman, Developers Alliance 
 
3 


