Faisal D’Souza, NCO  
Office of Science and Technology Policy  
Executive Office of the President  
2415 Eisenhower Avenue  
Alexandria, V A 22314  
Request for Information (RFI) on the Dev elopment of an Artificial 
Intelligence (AI) Action Plan
The Oxford Martin AI Governance Initiative (AIGi) welcomes the opportunity to submit this comment for 
consideration in the development of the United States' AI Action Plan, as called for in Executive Order 
14179. AIGi supports the Administration's vision of American leadership in artificial intelligence as a driver 
of economic prosperity and national security. We propose a policy framework to solidify the U.S. position as 
the global leader in AI while addressing a restricted set of security risks to American citizens and interests. 
This comment focuses on three key areas where targeted government action can have the greatest impact: 
1.Strengthening US AI Leadership and Competitiveness through strategic investments in critical
infrastructure, compute capacity, and research priorities
2.Protecting US Citizens and National Security by eschewing premature, innovation-dampening
regulation in favor of targeted, capability-based security governance and robust measures against AI
system vulnerabilities
3.Managing Access to Strategic AI Capabilities through effective export controls and thoughtful
approaches to open-source AI development
About the AI Governance Initiative: The Oxford Martin AI Governance Initiative (AIGi) is an 
interdisciplinary research center dedicated to understanding and anticipating lasting risks from AI through 
rigorous research. Housed at the University of Oxford's Martin School, we combine technical expertise 
with policy analysis to support decision-makers across industry, government, and civil society in mitigating 
AI's challenges while realizing its benefits.  
As a UK institution, we strongly support American leadership in AI development, recognizing our share d 
values and commitment to ensuring advanced AI systems are built and governed by nations that uphold 
free speech, human rights, and transparent governance. The transatlantic partnership on AI governance is 
essential to ensure these technologies serve humanity's best interests. 
By Charles Martinet, Ben Harack, and Sumay a Nur Adan, research affiliates at the AI Governance Initiative. The views 
expressed in this comment do not necessarily reflect the positions of individual authors. Correspondence with 
1 


Introduction 
The development of artificial intelligence has entered a phase of extraordinary acceleration and global 
diffusion, creating unprecedented opportunities and challenges for American economic prosperity and 
national security. The U.S. AI Action Plan represents a critical opportunity to shape the trajectory of this 
transformative technology in ways that serve American interests and values. 
Recent developments create three critical imperatives for U.S. policy. Infrastructure bottlenecks—from 
energy supply to specialized semiconductor availability—threaten to constrain America's development 
capacity precisely when computational resources have become a strategic asset. Novel security vulnerabilities 
unique to AI systems require targeted governance measures that scale with system capabilities without 
hampering innovation. And the global proliferation of AI technologies necessitates managing global access to 
the most sensitive capabilities. 
Without strategic government action addressing these interconnected challenges, the United States risks 
losing both its technological edge and its ability to shape how these powerful systems evolve. The 
recommendations that follow provide a framework for securing U.S. economic and security leadership in AI. 
1. Strengthening US AI Leadership and Competitiveness
a. Critical Energy Infrastructure for AI Leadership: Streamline approvals and incentivize development
of abundant, reliable energy infrastructure to power America's AI revolution.
Access to reliable power is becoming strategically important in AI leadership. The demand for computing 
power required for advanced AI systems is projected to increase dramatically as demand for increasingly 
capable models grows. Matching this demand will require much more power than current infrastructure can 
provide—and projected growth in energy production is not expected to match the growth in demand. 
Ensuring abundant, cost-effective electric power supply will allow American AI developers to continue to 
hold a crucial competitive advantage over their international rivals. Targeted tax incentives and streamlined 
approvals for energy providers could accelerate private investment in necessary infrastructure, creating a 
foundation for robust AI development at scale and preventing energy shortages from becoming a bottleneck 
that constrains America's technological leadership. Recommendations: 
➢Create expedited approval pathways and tax incentives for energy projects supporting
AI-specialized data centers.
➢Establish a federal task force to coordinate energy planning for high-priority AI development zones
2 


b. Data Center Development: Ensure that America remains the  leading location for AI compute through
strategic incentives, expedited permitting, and infrastructure investments.
Domestic data center capacity represents one of the most significant potential bottlenecks in advancing 
American AI leadership. The federal government should ensure that specialized facilities required for 
large-scale AI training do not face complex regulatory hurdles and lengthy approval processes that impede 
rapid expansion. Without deliberate policy support, data center construction timelines and costs may place 
American AI developers at a disadvantage compared to international competitors who benefit from 
streamlined approval processes and government support.  
Expedited permitting could dramatically accelerate deployment of necessary computing resources, ensuring 
that the United States maintains its position at the forefront of AI compute capacity. Prioritizing these 
facilities in regions with abundant renewable energy would create AI development hubs that maximize both 
economic and environmental benefits while strengthening America's technological leadership. Recommendations: 
➢Streamline review processes and create tax incentives for high-capacity AI data centers serving
national security or strategic economic purposes (for example through “ AI Compute Zones”).
➢Establish matching grants for data center developments in regions with abundant renewable
energy resources.
c. Strategic Semiconductor Supply Chains: Secure preferential access to advanced AI chips through
deepened partnerships with allies while strengthening targeted domestic capabilities.
Advanced AI-specific semiconductors represent a critical vulnerability in America's AI supply chain, with 
global production concentrated in few, often geopolitically sensitive regions. The technical complexity and 
capital-intensive nature of leading-edge semiconductor manufacturing make complete domestic production 
infeasible in the near term. The U.S. should grow onshore semiconductor capacity through tax incentives, 
streamline regulation, and public-private partnerships. In addition, because increasing domestic capacity is a 
long-term project, the United States should leverage America's design expertise while securing reliable access 
to manufacturing capacity through strategic international partnerships. 
The US should pursue deals to strengthen AI supply chain resilience, in particular agreements to secure 
preferential access to critical components. By receiving the majority of global AI-specialized chips, the US can 
maintain its leadership position in AI development. This strategy complements domestic data center 
development, which should remain a priority regardless of semiconductor sourcing decisions, creating a 
comprehensive approach to compute infrastructure security. Recommendations: 
3 


➢ Implement policies ensuring American companies receive preferential access to advanced AI chips 
produced globally. 
➢ Deepen partnerships with trusted semiconductor manufacturing nations to secure supply chain 
stability. Work with Japan, South Korea, Taiwan, and European partners to 1) coordinate R&D of 
next-generation semiconductor designs and materials and 2) pursue a "solidarity pledge" ensuring 
no party will cut off others from critical equipment, design IP, or scarce AI-specialized chips. ➢ Enhance capacity for packaging, testing, and assembling advanced semiconductors within the 
United States. 
 
d. Strategic AI Research & Development Initiatives: Fund targeted R&D programs that enhance U.S. 
security interests while developing targeted governance tools and technical standards. 
Many current AI governance approaches lack robust technical foundations; without rigorous evaluation 
methods, standardized testing protocols, and technically-validated security measures, they may lead to 
regulatory regimes that create security vulnerabilities and inhibit innovation. The U.S. government should 
strategically invest in R&D initiatives that simultaneously strengthen America's competitive advantage and 
enable evidence-based policy with respect to targeted security risks. By supporting fundamental research in 
areas such as model evaluation, security mechanisms, and next-generation hardware, the government can 
address market failures where private industry lacks sufficient incentives to invest.  Such efforts can also 
address areas where industry players are prohibited by antitrust rules from collaborating on valuable 
industry-wide initiatives. These investments will position the U.S. to shape global technical standards and 
help identify and mitigate emerging security vulnerabilities before they can be exploited by adversaries.  Recommendations:  
➢ Allocate funding for the research and development of: 
● 1) rigorous, standardized model security evaluation techniques; 
● 2) hardware security mechanisms to verify computational resource usage, facilitate 
privacy-preserving access to datasets, and enable reliable attestation of evaluation results, 
helping ensure that high-end AI hardware is used in accordance with national security 
interests; ● 3) adversarial testing and resilience measures against manipulation, theft, and misuse of 
advanced AI systems; 
● 4) technical guardrails for AI systems that reduce security risks without compromising 
America's leadership in innovation; and 
● 5) next-generation AI hardware research to ensure US leadership in AI infrastructure. 
● See Reuel, Bucknall, et al., 2024 for key R&D priorities. 
➢ Ensure these R&D initiatives directly inform the development of practical governance tools and 
standards that can be operationalized by industry and government alike. 
4 


➢Create shared research infrastructure enabling secure, controlled access to models and datasets for
legitimate security research while protecting intellectual property.
➢Expand partnerships with key allies to jointly fund and develop AI security research, ensuring safe
technological advancements in AI governance and security evaluation.
e. A US Lead in Secure and Verifiable AI: Provide market-creating standards that help American
businesses lead in providing security-sensitive AI services to people, companies, and states.
Absent regulatory effort, uptake of AI will lag in security-sensitive domains such as medicine, government 
services, and militaries. Organizations in these sectors require verifiable assurances that AI systems meet 
stringent security and reliability standards before deployment. Technological stacks already exist that can 
robustly verify that AI models and their infrastructure follow desired rules—but the trustworthiness of these 
assurance systems is limited by the lack of a neutral overseer who can validate security claims and ensure 
compliance with technical requirements. 
Federal regulation can enable this market in at least two major ways. First, it can provide an institutional 
framework for strong, independent, and neutral oversight of secure and verifiable AI stacks, thus enabling 
American corporations to robustly demonstrate their cutting-edge offerings in this domain. Second, it can 
minimize compliance costs by facilitating the creation of targeted industry standards for secure and verifiable 
AI. Compliance costs can be reduced by ensuring that all federal rules about technical aspects of AI systems 
are compatible with and do not overreach the new standard. Broad use of this standard would help mitigate 
the potential costs for AI producers from regulatory fragmentation and existing regulatory overreach. 
Rapid progress on the standard and its rollout can also enable US-based AI infrastructure providers to lead in 
this emerging domain on the international level. Many states will desire domestic data centers that meet 
security standards strict enough that they can be allowed to locally host sensitive resources such as models 
exported from the USA. By establishing the technical standards and certification processes for these secure 
facilities, American companies can capture a significant share of this growing market while advancing U.S. 
security interests. American leadership in this domain will provide another competitive edge in the emerging 
AI ecosystem, positioning U.S. companies as the premier providers of secure AI solutions globally. Recommendations: 
➢Direct NIST to develop a Secure and Verifiable AI Framework (SV AF) in close partnership with
industry leaders, establishing how an organization can make credible technical claims about its
infrastructure and software such that they can be securely verified.➢Create a neutral oversight body that can verify claims in accordance with the SV AF, thus allowing
AI producers to robustly demonstrate the security and verifiability of their offerings.
5 


➢Extend SV AF rules to enable foreign data centers and AI industry players who want to
demonstrate that they meet American standards for security and verifiability as part of their
eligibility for U.S. government contracts or hosting models exported by the U.S.
➢Collaborate with key allies to ensure international recognition of US verification standards,
securing reciprocal agreements for market access based on comparable security assurances.
2. Protecting US Citizens and National Security
a. An AI Security Governance Framework Based on Capability Tiers: Develop and implement targeted
governance requirements that scale with AI capabilities rather than applying blanket regulations that could
hamper America's competitive edge.
The accelerating pace of AI development demands a governance approach that addresses genuine risks while 
preserving the innovation ecosystem that fuels American leadership. Applying uniform regulations across all 
AI systems regardless of their capabilities and risk profiles would hamper innovation and competitiveness. 
Instead, a framework based on evidence-based capability tiers would match governance requirements to the 
specific technological capabilities and risk profiles of AI systems, focusing on those with the greatest security 
impacts. 
Without capability-based distinctions, regulators risk either imposing excessive burdens on low-risk systems 
or failing to adequately oversee high-capability applications that could affect national security—potentially 
driving innovation offshore as developers seek jurisdictions with less blunt regulatory approaches. Instead, 
requirements should apply only to systems that demonstrate specific security-related capabilities of concern, 
creating regulatory predictability for developers and allowing them to anticipate compliance requirements as 
their systems advance. 
Leading AI developers have already implemented voluntary security and safety measures that can form the 
foundation for a more formal capability-based framework. These include conducting internal and external 
red-teaming to identify severe threats, establishing information sharing protocols to alert others to emerging 
vulnerabilities, implementing cybersecurity and insider threat safeguards to protect model weights, enabling 
third-party vulnerability reporting, and other best practices that help reduce security risks and enable uptake 
of AI solutions.  
The federal government should foster industry consensus on these practices, for example through the US AI 
Safety Institute Consortium, to establish baseline expectations appropriate for systems of different capability 
levels. It should also lead efforts to internationalize these security practices through both diplomatic and 
regulatory channels to prevent regulatory arbitrage while also expanding market opportunities for American 
AI products. By establishing clear, capability-based requirements that are adopted by key international 
partners, the U.S. can create a coherent framework that enables responsible innovation while addressing 
security concerns. 6 


Recommendations: 
➢Build government capacity to independently monitor and evaluate AI capabilities, trends, and
emerging risks through appropriate agencies and departments (see our recommendation for the AI
Safety Institute), with the view of leveraging this capability to develop a framework based on
capability tiers.➢Task an inter-agency taskforce with developing a tiered governance structure based on clear
technological thresholds or capability tiers that, when reached, would trigger specific policy
responses, focusing oversight on systems with the greatest potential for misuse and loss of control
risks. The taskforce should define specific milestones and the broad contours of the policies to be
implemented after each milestone.➢Require companies developing advanced AI systems to publish or report to the government
appropriate documentation of their capability assessment frameworks and risk mitigation
strategies, helping the federal government develop appropriate targeted requirements.➢Internationalize capability-based security standards to ensure a global level playing field by leading
development of technical specifications through bodies like ISO and NIST, embedding these
standards in trade agreements with key partners, and creating certification mechanisms that verify
compliance—potentially limiting market access for non-compliant AI systems in critical sectors.
b. Public-Private Partnerships & Information Sharing: Establish robust communication channels
between government and industry to address evolving AI security challenges.
The rapid evolution of AI capabilities and potential threats requires unprecedented coordination between 
government and industry. The technical complexity of these systems and the specialized expertise required to 
evaluate them necessitates close collaboration between those developing the technology and those responsible 
for securing national interests. Government agencies lack the technical depth to independently evaluate 
cutting-edge AI systems without industry cooperation, while private developers may be unaware of emerging 
threat vectors identified through intelligence channels. Without formalized information sharing, both sectors 
will face critical knowledge gaps and potential security blind spots that could be exploited by adversaries 
seeking to compromise these systems. 
These communication channels would help build institutional capacity within government while enabling a 
coordinated response to emerging threats, strengthening the United States' ability to identify and mitigate AI 
security risks before they materialize. Establishing trusted channels for sharing sensitive information about 
vulnerabilities, incidents, and mitigation strategies would create a more resilient AI ecosystem capable of 
withstanding sophisticated attacks and addressing security challenges as they emerge. Recommendations: 
7 


➢Establish formal communication channels with AI developers, similar to Information Sharing and
Analysis Centers (ISACs) in other critical industries.
➢Create both classified and unclassified channels to share threat information and security practices
and include intelligence agencies as and when appropriate in information-sharing frameworks.
➢Develop secure government infrastructure for assessing sensitive AI technologies that may have
national security implications, enabling thorough assessment of potentially sensitive AI
technologies without exposing evaluation methodologies or sensitive capabilities to potential
adversaries
c. Crash Program for AI Model Security: Launch a coordinated national effort to secure the most capable
AI systems across their entire lifecycle—from training environments to deployed models—against attack,
theft, and manipulation.
As AI systems become increasingly integrated into critical infrastructure and sensitive applications, they 
present novel and potentially severe security vulnerabilities. The unique characteristics of these systems create 
security challenges that existing cybersecurity frameworks are ill-equipped to address. Inadequate security of 
AI assets—from physical data centers to model weights—represents one of the few vulnerabilities that could 
rapidly erode America's current AI leadership position. Existing security approaches will prove insufficient 
for protecting these systems from sophisticated attacks designed to compromise their integrity or extract 
sensitive intellectual property. 
Security vulnerabilities exist across multiple levels, including physical infrastructure (data centers, power 
systems), hardware (specialized chips), training data, model weights, algorithms, and deployment 
environments. These systems face diverse threat vectors including cyber intrusions targeting model weights 
or training data, physical security breaches at AI infrastructure facilities, insider threats from personnel, 
compromises of AI hardware components, and advanced adversarial attacks unique to machine learning 
systems. This multifaceted attack surface requires a comprehensive security approach that addresses 
vulnerabilities at each level.  
Market forces alone cannot adequately address these security challenges due to information asymmetries 
between developers and users (where developers have greater knowledge about vulnerabilities but limited 
incentives to disclose them) and antitrust constraints limiting necessary industry-wide coordination. 
Individual companies lack sufficient incentives to invest in security measures that provide collective benefits, 
particularly when competitive pressures encourage rapid deployment over thorough security validation. 
Without coordinated federal action to establish security standards and practices, the commercial AI 
ecosystem cannot achieve the necessary security posture to withstand determined adversaries. 
A national initiative should involve collaboration between national security agencies, research institutions, 
and private sector AI developers to rapidly identify and address emerging vulnerabilities. This approach 
would leverage the federal government's security expertise and research capabilities as well as industry's 8 


technical knowledge to create robust security standards and practices appropriate for systems of different 
capability levels. Establishing American leadership in AI security would create both national security 
advantages and commercial opportunities in secure AI applications, positioning U.S. companies as trusted 
providers in domains where security is paramount. 
Recommendations: 
➢Fund targeted research into novel vulnerability types unique to the most capable models,
particularly those that could enable model weight theft or compromise.
➢Develop testing methodologies to evaluate model security against adversarial attacks and
unauthorized access across different capability tiers, ensuring that cyber and physical security
measures scale appropriately with AI system capabilities (see our recommendation on a
capability-based security governance framework).
➢Establish joint efforts between government agencies and leading AI companies to rapidly identify
baseline security standards appropriate for systems of different capability levels and address
emerging security vulnerabilities at all levels.
➢Require security assessments proportional to capability levels before deployment and, in some
cases, development of the most capable systems.
d. The AI Security Institute: Transform the existing AI Safety Institute into a center of technical
excellence that enhances government capacity to evaluate, secure, and verify advanced AI systems while
coordinating international security standards with trusted partners. The institute should be renamed the AI
Security Institute (AISI), and focus exclusively on national security concerns. This approach will safeguard
critical resources, keeping them focused exclusively on essential security imperatives.
The federal government currently lacks sufficient technical capacity to evaluate emerging AI systems, assess 
their security implications, and develop appropriate policy responses. This capability gap undermines the 
government's ability to make informed decisions. Without robust internal expertise, the government will be 
unable to effectively respond to potential threats as they emerge, potentially leaving critical vulnerabilities 
unaddressed until after they are exploited. 
The institute should develop rigorous testing methodologies, evaluate both domestic and foreign AI models 
in terms of capabilities, risks, and vulnerabilities, and provide technical guidance to government agencies. It 
must function as a critical node within a broader network of federal offices to maximize economic and 
national security benefits, coordinating with defense, intelligence, and civilian agencies to ensure a coherent 
approach to AI security. These activities would create a foundation of technical expertise within government 
capable of understanding emerging capabilities and their security implications.  
Building this capacity will provide the U.S. government with crucial situational awareness regarding AI 
capabilities being developed domestically and abroad while strengthening American influence over the global 9 


direction of AI governance. Without American leadership in international standards development, 
frameworks developed by other nations may disadvantage U.S. companies or prove inadequate for addressing 
security concerns that impact American interests. To ensure such standards reflect American security 
priorities, the Institute needs to develop credible technical expertise and evaluation methodologies.  
Recommendations: 
➢ Reframe the AISI's mission to focus on building government capacity to evaluate AI systems, 
including those developed by potential adversaries. 
➢ Secure appropriate funding from Congress to attract top technical talent and build the 
infrastructure necessary for advanced testing and verification processes. 
➢ Develop comprehensive model weight security protocols and verification methodologies to protect 
critical AI assets from theft or exploitation. 
➢ Coordinate testing and evaluation methodologies with similar institutions in allied nations to 
establish consistent and scalable international approaches to AI security that will minimize 
compliance costs across markets. 
➢ Share appropriate insights on emerging capabilities and risks with trusted partners to strengthen 
the collective security posture of allied nations. 
➢ Coordinate technical assessment efforts across relevant federal agencies to optimize threat response 
capabilities and establish clear coordination mechanisms with national security authorities, 
including dedicated liaison positions. 
 
e. Minimum and Maximum Federal Standards: Establish clear federal parameters for AI regulation that 
provide consistency for developers and mitigate the danger of a harmful patchwork of conflicting state 
requirements. 
The emergence of inconsistent state-level AI regulations could create compliance challenges for developers 
and potentially hamper innovation. As states develop their own approaches to AI governance, developers 
may face contradictory requirements across different jurisdictions, increasing compliance costs and creating 
regulatory uncertainty that discourages investment. Without federal leadership to establish coherent national 
standards, this regulatory fragmentation could place American developers at a disadvantage compared to 
international competitors operating under more coherent frameworks. 
A strategically designed federal framework would provide the regulatory certainty needed for long-term 
investment while ensuring that essential safety guardrails remain consistent nationwide. By establishing clear 
parameters for state regulation, federal policy can prevent harmful inconsistencies while preserving 
appropriate flexibility for state innovation. Such a framework should recognize that some domains benefit 
from regulatory uniformity (maximum harmonization preventing states from imposing stricter 
requirements), while others warrant flexibility for states (minimum harmonization establishing federal floors 
rather than ceilings). 10 


Federal policymakers can use maximum harmonization to shield lower-risk AI innovations from a patchwork 
of conflicting state regulations, while using minimum harmonization to ensure baseline protections for 
higher-risk systems. Furthermore, the very structure of state regulation might be subjected to minimum 
harmonization, where federal rules could ensure that state regulation integrates cleanly with the established 
federal rule structures for AI , such as the SV AF described above—thus ensuring that compliance processes 
are scalable and cost-effective for industry even if many states implement their own requirements. This 
approach recognizes legitimate state interests and the vital role they play as policy laboratories where 
regulatory innovations can be tested and refined, while preventing harmful fragmentation that would place 
American companies at a disadvantage against international competitors operating under more unified 
regulatory frameworks. 
Recommendations: 
➢Work with Congress to enact legislation that clearly designates areas where states are preempted
from imposing additional regulations (maximum harmonization) and areas where federal
standards serve as a floor that states may build upon (minimum harmonization).
➢Prioritize maximum harmonization for lower-risk AI applications to create a unified national
market for responsible innovation, while employing minimum harmonization for applications
with potential for significant harm.➢Particular attention should also be paid to the scalability and interoperability of rules through
standards such as the SV AF so that industry can cost-effectively meet its obligations to both states
and the federal government.
3. Managing Access to Strategic AI Capabilities
a. Strategic Computing Access Framework: Strengthen America's approach to controlling advanced AI
technology transfer while maintaining global leadership through selective engagement.
The existing Framework for AI Diffusion (BIS-2025-0001) established important groundwork for managing 
access to sensitive AI technologies, but requires substantial enhancement to address emerging vulnerabilities 
across both hardware and cloud domains. 
Unlike semiconductor chips that cannot be recalled once shipped, cloud access can be monitored in 
real-time, throttled depending on usage patterns, and revoked entirely if necessary. This dynamic nature 
allows the United States to implement more precise restrictions while maintaining the ability to quickly 
respond to changing security concerns. Cloud controls can be calibrated based on specific use cases, specific 
capabilities being accessed, and evolving risk assessments, creating a more adaptive security framework than 
approaches focused on hardware exports alone. 
11 


A robust, multi-layered system of controls is essential to prevent adversaries from accessing sensitive AI 
capabilities while enabling appropriate collaboration with trusted partners and allies. Strategic governance 
must encompass both hardware exports and cloud compute access, recognizing these as interconnected 
vectors through which critical capabilities can flow. Without cohesive coordination between these 
complementary control mechanisms, sophisticated actors will inevitably exploit regulatory gaps to 
circumvent restrictions, rendering individual control mechanisms less effective, and redundant regulations 
risk imposing unnecessary economic costs. 
Maintaining strong international cooperation on hardware export controls must remain a top priority, as the 
effectiveness of US-led restrictions depends on alignment with key allies to prevent circumvention through 
alternative supply chains. While significant progress has been made in coordinating hardware export controls 
with Japan and the Netherlands, equivalent frameworks for cloud-based AI compute with key countries 
remains underdeveloped. Domestic data centers pose less concern due to direct US regulatory oversight, and 
trusted Tier 1 allies generally maintain compatible security standards; but restricted entities can still bypass 
restrictions by accessing equivalent capabilities through foreign cloud providers operating under less 
stringent regulatory frameworks. Harmonizing these approaches is essential to creating a coherent 
international control regime that prevents capability leakage while maintaining America's technological 
leadership. In this regard, strategic engagement with key developing economies represents both an 
opportunity to maintain American technological reach and a necessity to counter China's growing digital 
influence in these regions. By establishing structured technology partnerships with select developing nations, 
the U.S. can expand its commercial footprint while ensuring these countries are not driven toward alternative 
technology ecosystems that could undermine American interests. Recommendations: 
➢Further differentiate access tiers in the Framework for AI Diffusion based on comprehensive
security and economic assessments:
●Deepen technology sharing with closest allies (Tier 1) who maintain equivalent control
systems
●Establish conditional access for other partners (Tier 2) based on verifiable commitments to
1) responsible use, 2) enforcement of anti-chip smuggling rules and U.S. export control
laws, and 3) implementation of robust datacenter governance standards that align with
future US-led efforts to establish interoperable international frameworks for cloud
compute oversight●Strengthen capacity to monitor the effectiveness of restrictions for countries of concern
(Tier 3)
●Develop clear criteria and processes for countries to navigate between tiers through specific
policy reforms and security commitments, with the understanding that Tier 2 may be
further subdivided as the control framework matures
➢Strengthen end-user verification protocols to track both physical hardware transfers and cloud
resource usage to eliminate regulatory blind spots.
12 


➢Work with key technology partners to develop a comprehensive framework that addresses both
cloud computing access and semiconductor export controls as complementary but distinct
mechanisms, implementing consistent thresholds and monitoring protocols to close regulatory
gaps and ensure that American cloud providers are not disadvantaged against allied competitors
operating under lighter regulatory standards.
b. Strategic Framework for Securing Open Source AI: Develop a nuanced, capability-based approach to
open-source AI that balances innovation, security, and America's global leadership position.
Once the weights of an AI model are made widely available, this action is irreversible—they cannot be 
recalled, modified, or meaningfully controlled in their spread. This irreversibility represents a unique security 
challenge. Unlike traditional dual-use technologies where export controls can restrict physical transfers, 
digital AI models can proliferate globally once released, making initial release decisions critically important 
for systems with significant capabilities. 
Technical approaches to prevent misuse of open-weight models have often failed against determined actors. 
Security filters, output restrictions, and other measures implemented at the model level can be circumvented 
through techniques like prompt engineering, fine-tuning, or direct manipulation of model weights. 
Currently, security measures in open-weight models are easily circumvented and releases cannot be undone; 
open source models can be exploited by malicious actors.  
Yet, open source models can be much more quickly incorporated into economic activities. DeepSeek’s 
models have been rapidly incorporated into many popular applications in China, for instance. We can expect 
such open source models, and the applications that make use of them, to spread quickly around the world.  
Control of the open source ecosystem by foreign actors creates a security vulnerability for the United States. Such 
models could include backdoors that are virtually undetectable, but triggered in contexts that are detrimental 
to U.S. security such as facilitating cyber security vulnerabilities. 
The capability gap between open and closed models is surprisingly narrow, with recently released models 
demonstrating that open-weights AI can achieve capabilities similar to recently released proprietary 
systems—including potentially dangerous capabilities such as those relevant to chemical, biological, 
radiological, and nuclear threats.  
Without a coherent framework, the U.S. risks allowing advanced systems and capabilities to proliferate 
without appropriate safeguards, enabling sophisticated threat actors—both non-state actors and foreign 
adversaries—to leverage American AI systems against American interests.  The absence of clear guidelines 
about which capabilities should remain restricted and which can be openly shared creates uncertainty for 
developers while potentially allowing sensitive capabilities to proliferate unchecked. Indiscriminate release of 
unsecured models could provide adversaries with advanced technological capabilities they could not 
otherwise develop independently.  13 


The United States should leverage its leading position in AI to shape the global open source ecosystem. It can 
promote open source with security guardrails, ensuring that default models used in the open source 
ecosystem do not undermine U.S. security interests. This means investing strategically in research on 
techniques to create open weight models that are more difficult to misuse, with particular attention to 
preventing the development of dangerous CBRN capabilities in open-weights models.  
Open-source development remains extremely valuable and should be actively encouraged in numerous AI 
domains, including security techniques, evaluation benchmarks, model architectures, training 
methodologies, and specialized non-general purpose systems that present minimal misuse potential. These 
areas benefit from collaborative development while posing minimal security risks, allowing the innovation 
benefits of open approaches without the potential harms associated with unrestricted release of 
high-capability general-purpose models. Furthermore, many of the benefits commonly attributed to 
open-source AI—including external oversight, innovation acceleration, and democratic access—can be fully 
realized without the security risks of full release of model weights, such as through structured access 
programs and targeted API access (Seger et al., 2023). These approaches would preserve the benefits of 
transparency and accessibility for legitimate researchers and developers while maintaining appropriate 
controls on the most sensitive capabilities. Recommendations: 
➢Partner with U.S. open source model providers to ensure that the open source ecosystem is not
dominated by foreign actors.
➢Invest in research to mitigate security-related misuse of open weight models.
➢Create structured access frameworks for research and auditing of more capable models that
preserve many benefits of openness (innovation, oversight, adaptation) while reducing
proliferation risks.
➢Restrict the release of model weights for highly capable systems that could enable chemical,
biological, radiological, and nuclear (CBRN) threats or other catastrophic misuse. If such models
are legal, such as white hat cyber tools, they should be made accessible only via secured APIs with
appropriate safeguards and monitoring.➢Provide funding and resources for security research on open-source models to proactively identify
and address vulnerabilities before they can be exploited, recognizing that market forces will not
supply such research by default and that most adopting organizations lack the expertise and
computing resources to evaluate these tools despite depending on them.
➢Support the development of industry standards for evaluating when model components (e.g.
weights) should be released publicly, based on specific capabilities and potential for misuse.
Conclusion 
14 


The Oxford Martin AI Governance Initiative believes that the recommendations outlined in this comment 
would help achieve the vision set forth in Executive Order 14179: removing barriers to American leadership 
in AI while promoting human flourishing, economic competitiveness, and national security. We appreciate 
the opportunity to contribute to this important discussion and stand ready to provide any additional 
information that might be useful in the development of the AI Action Plan.  
This document is approved for public dissemination. The document contains no business-proprietary or 
confidential information. Document contents may be reused by the government in developing the AI Action 
Plan and associated documents without attribution. 
15 


