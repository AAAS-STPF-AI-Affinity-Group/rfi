PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8a-n5dz-ys6p
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-2523
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Anonymous Anonymous 
General Comment
I am  a 20-year-old student and I am  concerned about the lack of regulation on AI. Indeed, this year, progress in artificial intelligence has
been exponential and continues to be so. I think we should still be concerned about it.
On the Manhattan Project, scientists debated and wondered whether it would be too risky to continue if there was even a 1 in 300,000
chance that a single nuclear bom b would burn up the atm osphere. They would rather lose the war than take the risk. The Manhattan
Project's risk was 3 in a m illion. Scientists haven't always agreed on the acceptable lim it for extinction risks, but they agree that it should
be extrem ely low. Beyond the Manhattan Project, som e scientists considered a 1 in 2 m illion risk acceptable for RHIC, but another
physicist argued that a m ore appropriate lim it would be 1 in a quadrillion or 1 in 10 billion trillion.
By com parison, George Hinton, a pioneer of m odern AI, publicly estim ated the risk of extinction associated with AI at 10%, and privately
at 50%. These probabilities are astronom ically higher than those deem ed acceptable for other technologies (and m any other AI specialists
say the sam e thing).
Of course, if com panies use AI as a tool and im prove it in this way (narrow intelligence), that is, non-general intelligence, there is no risk.
But com panies like OpenAI and Claude 3 have them selves explicitly stated that they are seeking to build AGI (artificial general
intelligence) and then superintelligence. They are seeking to m ake it autonom ous. For exam ple, OpenAI's new artificial intelligence, o1 and
o3, are capable of perform ing tasks autonom ously, currently for up to an hour. Little by little, they are explicitly seeking to extend this
autonom y and give m ore autonom y to the AI.
Several studies have shown disturbing things about AI:
- Two AIs began talking to each other when they recognized the other as an AI. They com m unicated in a language we don't understand.
One scientist said we should stop building new AIs when this happened ("The point at which agents can talk to each other in a language
that we don't understand, unplug the com puters." Eric Schm idt)
Video link: https://x.com /ggerganov/status/1894057587441566081
- China equips self-im proving robot dogs with therm obaric nuclear warheads (these weapons are the second deadliest weapons of m ass
destruction after nuclear weapons). This is the first official disclosure by China of plans to deploy these controversial m unitions on
unm anned system s. These robots can run up very steep slopes, and drones have been developed to travel very quickly in dense forests.
Beyond the risk of extinction, in the event of a dictatorship, the people would be com pletely subjugated. We also see that these drones
are granted com plete autonom y.
- OpenAI acknowledged that GPT-4.5 attem pted to "escape" in 2% of tests. How can we control system s whose inner workings we
don't fully understand? Indeed, there are no algorithm s or code in an artificial intelligence system  (the so-called black box).
Even if we rule out the risk of extinction, other dangers rem ain:
-> Possibility of technological dictatorship and total surveillance


-> Loss of individual freedom s
-> Biological risks am plified by AI
I don't claim  that these scenarios are certain, but I believe that in the face of such serious risks, even if they are low probability, caution is
required.
It is necessary to form  a coalition and an international treaty on these issues. If you have difficulty im agining the risk associated with AI,
that's perfectly norm al. This reluctance is based on several psychological biases. We tend to think that life will continue as before, even in
the face of a threat (norm alcy bias). The idea that AI could cause our extinction contradicts our beliefs about technological progress and
the stability of the world (cognitive dissonance). Since extinction has never happened, we find it difficult to im agine that it could happen
(availability bias). We like to believe that hum anity is special and will always overcom e its challenges (illusion of protection).
We tend to attribute hum an intentions to AI, when they operate differently. AI alignm ent is an underestim ated technical challenge. Sim ple
solutions (Asim ov's laws, the off button) are not enough.
Even if we understand the problem , taking action rem ains com plicated: AI doesn't trigger prim itive fear like a predator; it seem s abstract
and unreal. Extinction affects 8 billion lives, but our brains struggle to grasp such enorm ous num bers.
I hope this explanation will convince you that action is necessary. The risks are real, but it's our brains that struggle to understand them .
Thank you for everything you do every day, and for the attention with which you read this m essage.
My m ost distinguished greetings.


