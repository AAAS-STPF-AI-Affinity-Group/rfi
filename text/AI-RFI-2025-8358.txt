PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8b-2j9a-w6ke
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-8358
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Steven 
Kaas   Email: 
General Comment
My m ain hope for the AI Action Plan is that it gives serious attention to the risks of losing control over future, highly advanced AI system s
— risks that could lead to catastrophic or even existential consequences. Many subm issions likely outline concrete policy proposals to
address this. I don’t claim  special expertise in that area, but I want to highlight one approach that m ay not yet have been raised.
AI experts differ sharply on the severity of existential risk from  m isaligned AI. Researchers like Geoffrey Hinton, Yoshua Bengio, and
Stuart Russell have warned that current AI advances are placing us on a highly dangerous trajectory. Others, like Yann LeCun and
Andrew Ng, argue the risk is overstated.
Sound policy depends on rigorously assessing how serious an extinction-level threat AI poses, understanding the nature of that threat, and
identifying how it can be addressed.
To that end, I suggest supporting structured, public dialogues between leading researchers on AI and its risks — perhaps m odeled after
the Dutch governm ent’s “Clim ate Dialogue” project (2012–2015) (https://m wenb.nl/clim ate-dialogue/), which brought together clim ate
scientists with opposing views. Such a forum  could foster m ore transparent, system atic, and constructive expert conversations, grounding
AI policy in well-vetted ideas and m aking them  accessible for public scrutiny.


