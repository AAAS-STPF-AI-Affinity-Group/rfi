PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8b-2xu3-ziox
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-8973
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Mehmet Kayaalp  
General Comment
Mehmet M. Kayaalp, M.D., 
Ph.D.  
March 15, 2025
National Science Fo undation
2415 Eisenhower Avenue
Subject: S ubmission in Response to t he Request for Information on t he Development of an Artificial Intelligence (AI) Action Plan
Dear Nat ional Science Fo undation Represe ntative,
I am p leased to s ubmit my comments in respo nse to t he Nat ional Science Fo undation’s Request for Information (RFI) on t he 
Development of an Artificial Intelligence (AI) Action Plan, as p ublished in the Federal Register. As an e xpert in artificial intelligence, 
machine learning, clinical dec ision-making, and data pr ivacy, I have care fully considered t he challenges and opport unities prese nted by t he 
AI revolution. My e nclosed doc ument, On t he Elements of t he AI Action Plan, provides spec ific reco mmendations to e nsure the 
respo nsible development, dep loyment, and governance of AI technologies for the benefit of soc iety.
In my respo nse, I address key issues such as tra ining data tra nspare ncy, intellectual property r ights, eq uitable eco nomic models, eco logical 
respo nsibility, and the necess ity for national and international regulatory frameworks. As AI-driven automation increas ingly shapes 
industries and eco nomic structures, it is imperative that policies proact ively mitigate risks assoc iated with workforce d isplacement, wealth 
concentration, and regulatory gaps.
The reco mmendations outlined in my submission highlight the urgency of de veloping enforceab le standards for AI acco untability, fair 
compensation for data co ntributors, a nd sustainable AI access ibility models. Additionally, I emphasize the necess ity of federal legislation 
to ensure the United States maintains both global leaders hip and soc ietal res ilience in the AI era.
I apprec iate the NSF’s e fforts in gathering expert insights on AI policy and strate gy. I hope that my submission co ntributes meaningfully to 
the ongoing discourse and informs policy de velopment that fosters an eq uitable and sustainable AI-driven future.
Please feel free to co ntact me should you req uire any further information or c larification regarding my submission.
Sincerely,


Mehm et M. Kayaalp, M.D., Ph.D.
Attachments
On the Elem ents of AI Action Plan - by Mehm et Kayaalp MD PhD - 2025-03-15 - Maryland


On the Elements  of the AI Action  Plan  
by 
Mehmet Kayaalp, M.D., Ph.D. 
Executive Summary 
We are at the dawn of the AI revolution. While it's easy to focus solely on technological 
progress, overlooking its social implications could create severe long-term consequences. Many 
countries are closely matched in AI technology, but those that will benefit the most are those that 
will proactively manage social impacts before they become overwhelming. Current AI policies, 
including the EU AI Act, are often too vague or unenforcea ble. Here , I offer specific, actionable 
recommendations base d on my expertise in AI, clinical decision-making, and privacy. 
My key proposals  include:  
•Transparent  and auditable  AI training  processes,  especially  for high-risk applications.
•Clear  documentation  of AI systems,  ensuring  accountability  and intellectual  property
rights protection.
•Equitable  compensation  for individuals  and entities  whose  data supports  AI system
development.
•Ecological  accountability  from  AI companies,  ensuring  they minimize  their carbon
footprint.
•Strong  human -in-the-loop regulations,  especially  in critical  sectors  like medicine  and
security.
•Translating  global  cultural  heritage  into accessible  datasets  to enhance  AI models.
To sustain  a healthy  society  and economy,  we must  address  these  aspects  urgently  rather  than 
reactively. The US currently lacks comprehensive federal  AI regulations, and it’s critical to 
establish them now for both global leadership and long -term social stability.  
Introduction  
We stand  at the beginning  of the AI revolution.  Focusing  exclusively  on technological 
advancement  without  adequately  addressing  the social  ramifications  could  be profoundly 
detrimental.  As we have seen in recent months, other nations are not far behind in the  


technological aspects of this revolution. The emergence of models like Deepseek R1 has 
demonstrated China’s prowess in this field, evoking a sentiment akin to the Sputnik moment. 
However, technological superiority alone will be meaningless if societies fail to proactively 
adapt  to the new era. Nations  that will be able to reap the gains  from  this revolution  will be those 
that proactively adjust their social systems before the adverse effects arrive in tsunamis —at 
which point, responding may no longer be feasible.  
In reviewing  international  AI policies —including  those of the EU and various U.S. states — 
using  AI-assisted methods, I have identified that most existing regulations and frameworks are 
abstract,  high-level,  and challenging  to enforce  practically.  Here,  I provide  concrete,  actionable 
recommendations. However, please interpret numeric examples (e.g., revenue -sharing 
percentages) as illustrative rather than prescriptive.  
My recommendations  draw  specifically  from  my professional  expertise  in AI, machine 
learning, clinical decision -making, and data privacy.  
 
Training  Data  and Process  Transparency  
AI systems'  performance  heavily  depends  on training  data,  which  encompasses  foundational 
training,  safety  tuning,  reinforcement  learning  from  human  feedback,  fine-tuning,  and retrieval - 
augmented generation. Negligent or unethical practices in curating training data can lead to 
harmful outcomes in critical sectors.  
AI developers must adhere to rigorously documented protocols. Deviations from these 
protocols  must  be transparently  communicated  to end-users  and regulators.  Consequences  for 
undisclosed deviations should align proportionally with domain risk:  
• High -risk (e.g.,  homeland  security,  nuclear  controls):  Severe  penalties.  
• Moderate -risk (e.g.,  clinical  decision -making,  robotic  assistance  to elderly):  Significant 
penalties.  
• Low -risk (e.g.,  word  processing,  video  gaming):  Lesser  penalties.  
For example,  in intelligent  clinical  decision -making,  failure  to disclose  omitted  validation 
steps could jeopardize patient safety and thus warrant significant regulatory action.  
Several governments have introduced varying transparency mandates. Colorado[1] and 
California[2]  have  high-level  requirements  for generative  AI systems,  while  the EU[3]  mandates 
comprehensive disclosures, including data provenance, cleaning methods, bias mitigation 
practices, and validation details.  


I recommend  combining  both approaches.  Specifically,  for high-risk applications,  developers 
must provide comprehensive documentation —including random seed values —to enable 
independent replication, especially critical when claims are legally contested.  
 
Intellectual  Property  (IP) Rights  and Fair Compensation  
AI-generated outputs must clearly reference underlying data sources to respect intellectual 
property  rights.  Developers  of commercial  AI tools  should  financially  compensate  entities  and 
individuals whose data they utilize. The distribution could be structured as follows:  
• Healthcare  Data  (e.g.,  Electronic  Health  Record  or Clinical  Trials  data):  
o Patients  whose  data is used:  majority  share  (e.g.,  90%)  
o Healthcare  providers  facilitating  data and revenue  exchange:  modest  share  (e.g., 
9%) 
o Governmental  oversight  and administrative  costs:  minimal  share  (e.g.,  1%) 
• News  or literary  sources:  
o Individual  authors  or original  content  creators:  majority  share  (e.g.,  90%)  
o Publishers  or curators:  moderate  share  (e.g.,  9%) 
o Regulatory  bodies:  minimal  share  (e.g.,  1%) 
For instance, if an  AI-generated book analysis references copyrighted literary works, the 
authors or rights holders should proportionally benefit. Table 1  presents an  AI-based analysis of 
the Orson Card’s novel Ender’s Game . Had it been written by a generative  AI system and 
commercialized,  the revenues  should  have  been  shared  with the copyright  holders  of the previous 
works.  


Table  1: Relating  to Earlier  Works  by Other  Authors  
 
Theme  in Earlier  Work  It Resembles  Connection  
Military  training  as a 
social duty  Starship  Troopers  
(Heinlein)  Heinlein  advocates  military  citizenship; 
Card questions ethical implications.  
Strategic  deception  The Art of War (Sun Tzu) Warfare  as deception  directly  influences 
Ender's training methods.  
Manipulation and 
societal  conditioning  Brave  New World  
(Huxley),  1984  (Orwell)  State  manipulation  parallels  Battle 
School’s controlled environment.  
Gifted,  isolated  leader  Dune  (Herbert)  Both  Ender  and Paul Atreides  face heavy 
burdens imposed externally.  
The psychological 
burden  of command  Crime  and Punishment  
(Dostoevsky)  Ender  shares  Raskolnikov’s  internal  moral 
conflict  after decisive,  irreversible  actions.  
"Chosen  One" 
narrative  Hamlet  (Shakespeare)  Ender, like Hamlet, struggles under 
manipulation  toward  tragic  outcomes.  
 
An AI system  using  these  works  must  explicitly  cite them  and, if applicable,  distribute 
royalties proportionally.  
 
Ecological  Accountability  
AI developers  must  internalize  environmental  costs,  moving  beyond  current  practices  where 
ecological harms are externalized to society. Companies should implement measures like heat 
capture in data centers and recycling thermal energy for heating or power generation, thus 
minimizing overall carbon emissions.  
 
Human -in-the-Loop  Safeguards  
Overreliance on  AI in critical domains such as healthcare, aviation, and national security 
should be curtailed through strict regulations mandating human oversight. For example, 
Intelligent Clinical Decision -Making (ICDM) systems in healthcare should deliberately and 
periodically insert simulated errors unbeknownst to clinicians under controlled conditions and 
expect the clinician to respond with corrective feedback. If clinicians fail to detect these 
intentional inaccuracies, the system should alert and educate them to foster vigilance and ensure 
robust  oversight  of AI system  performance.  This reinforcement  feedback  process  should  never  be 
applied in urgent and emergency care and the controlled conditions must ensure that patients 
cannot be harmed during this process.  


Expanding  AI Training  Data  
English is the predominant language on the Internet and for training  AI systems, yet it 
represents only a portion  of global cultural heritage.  To maximize  the capabilities of  AI systems, 
it is essential to systematically translate legally accessible global knowledge resources (e.g., 
Chinese, Russian, French, Japanese,  Arabic, etc.) into English datasets. For instance, translating 
non-English  medical  texts  or ancient  philosophical  works  significantly  enriches  training  data and 
enhances AI's analytical depth.  
 
Public  AI Accessibility  and Subscription -Based  Redistribution  
A portion of fees paid by subscribers using powerful  AI models should be allocated to 
providing  free AI services  to the public  using  previous -generation  models.  While  this approach  is 
currently practiced by  AI companies, it remains a fragile, profit -driven system that may not 
persist indefinitely. This arrangement should therefore be legislated to ensure continued public 
access to  AI tools.  At present,  AI companies rely on free -tier users to improve their models 
through Reinforcement Learning from Human Feedback (RLHF). However, once this exchange 
ceases to be profitable, these companies, bound by fiduciary obligations to their shareholders, 
may discontinue free services. Unless legally required to serve the public interest,  AI firms will 
likely prioritize short -term financial returns over long -term accessibility, making government 
intervention crucial in maintaining this system for public benefit.  
 
Conclusions  
Just as businesses must identify sustainable models to generate revenue, societies must 
establish frameworks to equitably distribute wealth and resources.  As AI-driven automation 
increasingly  replaces  human  labor  with robots,  a larger  share  of AI-generated  revenues  must  be 
allocated to society at large.  
Superintelligent  software  and robotic  applications  no longer  belong  to science  fiction —they 
are becoming intrinsic to our near future. If we fail to establish a new economic and social 
framework before  AI-driven automation erodes traditional labor markets, vast segments of 
society may find themselves economically obsolete. This is not a distant possibility but an 
imminent threat. If we continue to delay necessary policy interventions —just as we have done 
with climate change —we may be unable to prevent widespread societal collapse.  
The AI revolution is built on centuries of human knowledge encoded in literature, science, 
philosophy, and the arts. This cultural heritage does not belong to a handful of corporations but 
to humanity  as a whole.  As AI harnesses  this collective  inheritance  more  effectively  than ever,  its 
benefits must be equitably distributed.  


Superintelligent  software  and robotic  applications  represent  imminent  changes  to societal 
structures,  employment,  and the economy.  To avoid  societal  instability,  we must  proactively 
develop  equitable  economic  models  to ensure  that the wealth  generated  by AI advancements 
benefits humanity broadly, not just corporate shareholders.  
Currently, the United States lacks comprehensive federal legislation addressing these issues, 
potentially jeopardizing long -term stability. While some states have begun legislating on  AI, 
comprehensive  national  standards  are necessary  for global  leadership  and societal  resilience.  The 
U.S. must  balance  national  interests  with ethical  and cooperative  international  engagement, 
recognizing  AI technologies as global public goods rather than proprietary national assets.  
Currently, the United States lacks comprehensive federal legislation addressing these issues, 
potentially jeopardizing long -term stability. While some states have begun legislating on  AI, 
comprehensive  national  standards  are necessary  for global  leadership  and societal  resilience.  The 
U.S. must  balance  national  interests  with ethical  and cooperative  international  engagement, 
recognizing  AI technologies as global public goods rather than proprietary national assets.  
 
Disclaimer  
These  recommendations  reflect  my individual  perspective  as an AI expert  active  since  1990, 
particularly in  AI applications in medicine. Numeric examples are conceptual and require 
refinement through further expert analysis.  Although I am a federal employee, my input is 
submitted as a private U.S. citizen due to procedural limitations.  
 
References  
1. State  of Colorado,  SB24 -205 (2024).  
2. State  of California,  AB-2013  (2024).  
3. EU Artificial  Intelligence  Act, Article  53 (2024).  
4. National  Science  Foundation,  Federal  Register  Vol.90  (2025):  9088 -9089.  
 
Digitally  signed  by Mehmet  M. Kayaalp  -S 
Date: 2025.03.15 23:16:18 -04'00'  Mehmet  M. Kayaalp  -S 


