3/13/2025  via FDMS  
Nathan Metzger,  
I am a senior test automation developer working at a US DOD contractor. I am an AI enthusiast 
and te chno-optimist, and I have been deeply following AI technologies for the past 3 years. I 
have included expert and academic sources at the bottom of this comment to substantiate my statements. The good this kind of technology can bring is undeniable. US companies are creating very powerful AI systems in the coming years. It is recognition of this trajectory that leads me to be deeply concerned about the risk of loss of control of future AI systems. Most scientists take this risk seriously: 86% of AI researchers[1] believe the AI control problem is real and important. Until recently, these risks were theoretical. However, today, there is a large and growing pile of evidence of naturally emergent power -seeking behaviors in current general-
purpose AI systems. These include strategic deception[2], cheating[3], scheming[4], sandbagging[4], self- preservation[5], attempted self- improvement[6], and attempted self-
exfiltr ation[7]. These are only a sampling of the empirical results of misalignment, disloyalty, 
and dangerous tendencies in AI systems. Crucially, no one on earth knows how to prevent these behaviors.[8] As the US leads on this technology and creates more powerful, more capable, and more autonomous AI systems, the behaviors being demonstrated will become increasingly likely to lead to real-world harm, potentially of the kind that cannot be undone. It is crucial not to lose sight of the fact that most leading expe rts say there is a significant chance of total human 
extinction from AI.[9] The US must lead on this technology, and it cannot do so if the technology is disloyally scheming to lead us instead. We must cooperate with other nations to reign in disloyal AI systems and ensure AI remains under our control. A global treaty will be necessary to fully mitigate these risks. Thank you for taking these academic and expert sources into account. Nathan Metzger [1]: Researchers take these risks seriously - https://wiki.aiimpacts.org/ai_timelines/predictions_of_human -
level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai [2]: Deception - https://www.cell.com/patterns/fulltext/S2666 -3899(24)00103-X [3]: Cheating - 
https://palisaderesearch.org/blog/specification -gaming [4]: Scheming / Sandbagging - 
https://www.apolloresearch.ai/research/scheming -reasoning -evaluations [5]: Self-preservation - 
https://arxiv.org/abs/2307.00787 [6]: Attempted unauthorized self- improvement - 
https://sakana.ai/ai -scientist/#:~:text=The%20AI%20Scientist%20Bloopers [7]: Attempted 
unauthorized self- exfiltration - https://arxiv.org/abs/2412.14093 [8]: 200+ unresolved research 
questions - https://llm- safety -challenges.github.io/ [9]: Statement on AI risk - 
https://www.safe.ai/work/s tatement -on-ai-risk 
 


