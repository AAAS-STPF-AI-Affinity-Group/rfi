PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8b-2724-za7y
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-8059
Com m ent on FR Doc # 2025-02305
Submitter Information
Nam e: Kenisha Prout
Em ail:  
General Comment
The AI system s m ade by Big Tech com panies like OpenAI (Microsoft) and Google threaten to destroy thousands of Am erican sm all
businesses with their recent dem and to create special carve outs in copyright law. The purpose of Am erican copyright law is to protect the
incentive to create and innovate. What is the incentive to create when big tech corporations will be able to freely help them selves to the
work of others for their own financial gain? If the business m odel does not work without theft, it is a bad business m odel. Full stop.
Please, do not create new copyright exem ptions that allow Big Tech com panies to exploit and steal from  creators and everyday
Am ericans without perm ission, com pensation, or transparency.
This adm inistrationâ€™s AI Action Plan should focus not on giving away creator content to Big Tech com panies, but rather on ensuring a fair
m arketplace with com petition:
First, the governm ent should ensure that creators and everyday Am ericans give effective consent, so that we can decide when and where
our work is used by AI system s.
Second, the AI Action Plan should encourage a robust licensing m arketplace, so that the incentive to create for sm all businesses is
preserved. Our work has im m ense econom ic value, so the value generated by that work should accrue to the original creators, not just
Big Tech.
Finally, the AI Action Plan should require transparency from  Big Tech com panies, requiring them  to disclose what m aterial is in their
training datasets, and label what content is AI generated.
Thank you for your tim e.


