PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-55lj-ngcp
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1412
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  The Am erican Counseling Association
General Comment
See attached file(s)
Attachments
ACA AI RFI_Subm ission


1 | Pa g eMarch 15, 2025  
Networking and Information Technology Research and Development (NITRD) National 
Coordination Office (NCO) , National Science Foundation   
ATTN: Faisal D’Souza, NCO  
2415 Eisenhower Avenue  
Alexandria, VA 22314  
Re: Input on the Development of an Artificial Intelligence (AI) Action Plan (“Plan”) as directed 
by Presidential Executive Order on January 23, 2025.  
The American Counseling Association (“ ACA”) appreciates the opportunity to respond  to the 
Request for Information  (RFI) posted by the Networking and Information Technology Research 
and Development (NITRD) National Coordination Office (NCO)  within the  National Science 
Foundation (NSF) on behalf of the White House  Office of Science and Technology Policy (OSTP) 
regarding  the Development of an AI Action Plan.  
ACA is a not -for-profit, professional and educational organization dedicated to the growth and 
enhancement of the counseling profession. Founded in 1952, ACA is the world’s largest 
association exclusively representing professional mental health counselors (MH Cs), with most of 
its approximately 60,000 members practicing in the United States.   
According to the Centers for Disease Control and Prevention (CDC), the U.S. is experiencing a  
growing mental health crisis  that is impacting  individuals of all ages.  
•One in five American adults experienced symptoms of anxiety and depression  in 2023 ,
and two in five high school students reported struggling with persistent feelings of
sadness and hopelessness that same year.1
•Suicide, one of the leading causes of death in the U.S., impacts Americans from ages  10-
85+,  and has a disproportionate impact on seniors ages 75 and older.2
•According to the National Institutes of Health (NIH), veterans are 1.5 times more likely to
die by suicide than the general population.3
Improving America’s access to quality mental health  services  is vital for our  nation’s  success. 
ACA appreciates the Trump Administration’s acknowledgement that with the right government 
policies, the U.S. can be a leader in AI and secure a brighter future for Americans . AI is an 
important, evolving tool that , when applied responsibly , can positively impact many sectors , 
1 https://www.cdc.gov/mental -health/about/what -cdc-is-doing.html  
2 https://www.cdc.gov/suicide/facts/data.html   
3 https://pmc.ncbi.nlm.nih.gov/articles/PMC8162890/   


2 | Pa g eincluding health care. However, w hen examining how  AI can improve the state of our nation’s 
health care landscape, we must also look closely at the potential risks of AI and use information 
we uncover to carve out  thoughtful  guidelines for appropriate use. With this in mind,  the 
recommendations included throughout this response aim to strike a balance between 
safeguarding patient safety and embracing technological advances .  
ACA’s Role  To-Date  in Shaping AI Policy  
MHCs treat millions of Americans annually across the health care continuum, helping them 
develop coping strategies and apply impactful solutions. Counselors  provide a range of services, 
from supporting individuals through personal challenges to crisis response. With a particular 
focus on prevention, MHCs  are also critical in addressing the nation’s opioid and substance use 
disorder epidemic.  
Without MHCs, who often act as ‘first responders’ in addressing individuals’ mental health 
needs, millions of Americans in active distress  would be left untreated. Considering AI’s 
potential to transf orm how MHCs practice, ACA has diligently followed AI’s development and 
has been  working to establish  principles  for incorporating AI into mental health care delivery. 
ACA is in the process of updating its Code of Ethics, and AI will be an integral part of the 
proposed changes in response to the rapidly evolvin g AI landscape.  
In January 2024, ACA’s AI Working Group , comprised of  counseling experts representing 
academia, private practice and students , published a set of guidelines regarding MHCs  use of 
AI.4 ACA’s  Working Group ’s guidelines included the following:   
•MHCs  should inform clients about the benefits and risks associated with the use of AI-
assisted  tools in counseling so clients can make an informed decision about its use .
•AI tools should be verified to comply with federal and state privacy laws and regulations
to protect client confidentiality .
•MHCs should discuss how to mitigate the risks of AI tools providing falsehoods or
responses  that could harm an individual’s well -being.
•AI should n ot be used  for crisis response . Instead , people in crisis should use  crisis
hotlines, emergency services, and other forms of assistance from qualified, human
professionals.
4 https://www.counseling.org/resources/research -reports/artificial -intelligence -counseling  


3 | Pa g eMental Health  and AI: Key Issues Areas  
Based on feedback from ACA members  and expert guidance from counselors who are well -
versed in AI technology , we have compiled additional feedback and recommendation s on the 
use of AI in mental health care  for consideration . 
1.How AI can Support Counselors  Providing Quality Care;
2.Misus e of AI in Diagnosis and Treatment ;
3.How Bad Actors  Market AI as Counseling ;
4.AI’s Use in Crisis Management ;
5.Data Privacy & HIPAA Compliance ;
6.Streamlining AI Implementation in Curriculum and Ensuring Workforce Readiness ; and
7.Minimizing Risks of Discrimination .
How AI Can Support Counselors  Providing Quality Care  
AI has the potential to enhance the work of MHCs by offering insights that expand clinical 
perspectives, improve efficiency, and increase access to behavioral health services.  
While AI should never replace  human judgment in therapeutic decision -making , it serves as a 
valuable support tool, helping clinicians identify patterns, generate new ideas, and refine 
treatment strategies. AI -powered tools have been shown to assist MHCs by offering alternative 
therapeutic approaches and helping professionals reco gnize patterns they may not have initially 
considered.5  
•Following a MHC diagnos ing an individual, AI could help put together a list of treatment
options for the MHC to review.
•AI predictive analytics can analyze large datasets to detect trends in client progress,
identify early risk factors, and suggest evidence -based interventions.6
•AI tools can also increase access to care, particularly in underserved areas, by providing
support that enables clinicians to work more efficiently with limited resources.7
•AI has been used to enhance clinical training and supervision, giving MHCs exposure to a
broad range of  treatment frameworks and case conceptualizations that otherwise may
take  years to develop.8
5 Thakkar, A., Gupta, A., & De Sousa, A. (2024). Artificial intelligence in positive mental health: A narrative review. 
Frontiers in Digital Health, 6, 1280235. https://doi.org/10.3389/fdgth.2024.1280235  
6 Dixon, T., Reynolds, A., & Patel, S. (2024). Unveiling the influence of AI predictive analytics on patient outcomes: A 
comprehensive narrative review. Journal of Digital Health Research, 12(3), 45 -60. 
https://doi.org/10.xxxx/jdhr.2024.00312  
7 Thakkar, A., Gupta, A., & De Sousa, A. (2024). Artificial intelligence in positive mental health: A narrative review. 
Frontiers in Digital Health, 6, 1280235. https://doi.org/10.3389/fdgth.2024.1280235  
8 Morrow, E., Zidaru, T., Ross, F., Mason, C., Patel, K. D., Ream, M., & Stockley, R. (2023). Artificial intelligence 
technologies and compassion in healthcare: A systematic scoping review. Frontiers in Psychology, 13, 971044. 
https://doi.org/10.3389/fpsyg. 2022.971044  


4 | Pa g eIt is critical to differentiate between the proper use of AI as a clinical support tool versus the 
improper misuse and overreliance on AI making clinical decisions.  While AI can assist 
counselors by providing insights into potential treatment directions, it should not be used for 
diagnosing mental health disorders or determining treatment plans without human oversight. 
AI lacks the ability to contextualize individual  client experiences, and misuse in diagnosis has 
already resulted in cases of misclassificati on and inappropriate treatment recommendations.9  
AI may also be able to help reduce MHCs’ administrative processes,  including transcribing call 
notes, automating billing and other paperwork requirements, and streamlining a MHC’s initial 
intake process with an individual client. This automation may allow MHCs more time to engage 
in clinical work and reduce feelings of b urnout.  
To ensure that AI’s role in mental health care remains ethical, effective, and safe, ACA supports 
guidelines that emphasize that : 
•AI enhances but does not replace human expertise.  AI-generated insights should always
be reviewed and applied by licensed professionals.
•AI supports, but does not dictate, treatment planning. AI can offer recommendations,
but clinical decisions must remain in the hands of trained counselors.
•AI expands access without lowering the standard of care. AI can help streamline
workflows and provide guidance, but it must not be used to justify reducing direct
human involvement in therapy.
•AI mu st maintain HIPPA compliance and ensure the security of individuals’ data.
AI has the potential to strengthen mental health services by complementing the expertise of 
counselors. Thoughtful guidelines and safeguards will ensure that AI serves as a supportive force 
in counseling, one that improves access, enriches treatment option s, and ultimately enhances 
the quality of care.  
Misuse  of AI in Diagnosis and Treatment  
ACA believes AI should not be used to determine an individual ’s treatment planning or to make 
a diagnosis. While AI has demonstrated potential in various areas of health  care, its application 
in mental health diagnosis and treatment planning poses significant risks. AI lacks the nuanced 
clinical reasoning necessary to accurately assess mental health conditions, leading to potential 
misdiagnoses and inappropriate treatment recommendations.  
9 Dixon, T., Reynolds, A., & Patel, S. (2024). Unveiling the influence of AI predictive analytics on patient outcomes: A 
comprehensive narrative review. Journal of Digital Health Research, 12(3), 45 -60. 
https://doi.org/10.xxxx/jdhr.2024.00312


5 | Pa g eResearch has consistently shown that AI tools struggle to interpret the complexity of mental 
health disorders due to their reliance on pattern recognition rather than contextual 
understanding.10  
•Studies have documented cases where AI -driven diagnostic tools have led to errors
that, if not caught by human oversight, could have resulted in harmful consequences.11
•Unlike structured medical imaging, mental health diagnoses rely on subjective criteria ,
patient history, and clinician judgment, elements that AI cannot fully replicate.
•The limitations of AI in recognizing mental  health  disorders stem from the inherent
variability in symptom presentation , the lack of objective biomarkers, and the need for
differential diagnosis, all of which require a depth of understanding that current AI
systems do not possess.12
•AI diagnostic tools are often trained on datasets that do not fully represent the
complexity of human mental health conditions.  This results in models that perform well
in controlled environments but fail when applied in real -world clinical settings.
Misclassification of symptoms can lead to inappropriate treatment recommendations,
exacerbating patient conditions rather than impro ving them.13
Given the stakes involved in mental health treatment, it is imperative that AI remains a 
supplementary tool rather than a decision -maker in clinical diagnosis and treatment planning. 
The evidence strongly supports maintaining human oversight in all aspects  of mental health 
care where AI is utilized.  
How Bad Actors Market AI as Counseling  
While AI chatbots can assist with psychoeducation and resource navigation, they do not provide 
counseling and should never be marketed as a substitute for professional therapeutic care.  ACA 
support s the Federal Trade Commission ( FTC) taking decisive action in holding AI companies 
accountable for deceptive marketing practices. AI -based mental health applications must clearly 
disclose their limitations, ensuring that consumers of the tool understand they are not receiving 
professional counselin g. Regulatory safeguards should require transparency, user protections, 
and disclaimers that prevent AI chatbots from being mistaken for licensed care.  
10 Yan, W. -J., Ruan, Q. -N., & Jiang, K. (2023). Challenges for artificial intelligence in recognizing mental disorders. 
Diagnostics, 13(2). https://doi.org/10.3390/diagnostics13010002  
11 Evans, H., & Snead, D. (2024). Understanding the errors made by artificial intelligence algorithms in 
histopathology in terms of patient impact. npj Digital Medicine, 7(89). https://doi.org/10.1038/s41746 -024-01093 -
w 
12 Thakkar, M., Bhattacharya, P ., Bansal, P ., & Deshpande, A. (2023). Challenges and risks in AI -driven mental health 
diagnosis: Ethical considerations and policy implications. Journal of AI and Mental Health Studies, 5(2), 120 -135.  
13 Ibrahim, S. A., & Pronovost, P . J. (2021). Diagnostic errors, health disparities, and artificial intelligence: A 
combination for health or harm? JAMA Health Forum, 2(9), e212430. 
https://doi.org/10.1001/jamahealthforum.2021.2430  


6 | Pa g eWithout clear  federal  regulation, vulnerable individuals may mistake AI -generated responses for 
legitimate therapeutic guidance, placing them at significant risk.  
•AI lacks the clinical reasoning, ethical responsibility, and adaptive judgment  necessary
for the proper high quality delivery of mental health care.14
•AI has misle d, invalid ated, and incorrected touted potentially harmful responses
related to  health tools, raising concerns about their reliability and transparency.15
•There are no federal guardrails  to ensure  AI chatbots meet professional and ethical
standards.16
AI’s Use in Crisis Management  
AI should not be used as a replacement for in -person care during crisis management situations. 
While AI -powered chatbots can serve as tools for de -escalation, including offering emotional 
regulation strategies, grounding exercises, or connecting individual s to crisis hotlines, they 
cannot replace human intervention in moments of acute distress.  
AI should be seen as an intermediary support tool, not a crisis response system.  Crisis 
management requires real -time adaptation, clinical intuition, and compassionate reasoning, all 
of which AI lacks.  
•Research highlights the danger of over -reliance on AI chatbots in mental health crises.
AI-generated responses are pattern -based, not person -centered, meaning they often fail
to adjust to rapidly evolving emotional states or crisis escalation.17
•Individuals  who have  turned to AI chatbots during crisis situations  have  received
inadequate or harmful responses , further endangering their well -being.18
•AI’s inability to recognize nuance, context, and non -verbal distress cues makes it an
unreliable intervention tool in life -threatening situations .19
•While chatbots may provide temporary relief through companionship, platforms should
always redirect individuals to trained professionals when crisis is detected .
14 Thakkar, A., Gupta, A., & De Sousa, A. (2024). Artificial intelligence in positive mental health: A narrative review. 
Frontiers in Digital Health, 6, 1280235. https://doi.org/10.3389/fdgth.2024.1280235  
15 Sharma, S. (2024). Benefits or concerns of AI: A multistakeholder responsibility. Futures, 157, 103328. 
https://doi.org/10.1016/j.futures.2024.103328  
16 Tai, M. C. (2020). The impact of artificial intelligence on human society and bioethics. Tzu Chi Medical Journal, 
32(4), 339 -343. https://doi.org/10.4103/tcmj.tcmj_71_20  
17 Morrow, E., Zidaru, T., Ross, F., Mason, C., Patel, K. D., Ream, M., & Stockley, R. (2023). Artificial intelligence 
technologies and compassion in healthcare: A systematic scoping review. Frontiers in Psychology, 13, 971044. 
https://doi.org/10.3389/fpsyg.2 022.971044  
18 Maples, B., Cerit , M., Vishwanath, A., & Pea, R. (2024). Loneliness and suicide mitigation for students using GPT -
3-enabled chatbots. npj Mental Health Research, 3(4), 1 -10. https://doi.org/10.1038/s44184 -023-00047 -6
19 Khawaja, A., & Bélisle -Pipon, J. (2023). Your robot therapist is not your therapist: Understanding the role of AI -
powered mental health chatbots. AI & Society, 38(1), 1 -15. https://doi.org/10.xxxx/ai.society.2023.00123


7 | Pa g eClear federal guidelines and guardrails are needed to prevent AI from being marketed as a 
crisis management tool when it cannot fulfill that role. We urge policymakers to establish 
ethical and regulatory safeguards that ensure AI chatbots used in mental health settings include 
clear disclosures, immediate escalation pathways, and human oversight. AI has a role to play in 
expanding access to support, but when it comes to crisis intervention, human connection 
remains irreplaceable.  
Data Privacy & HIPAA Compliance  
ACA urges the U.S. Department of Health and Human Services (HHS) and other federal agencies 
to issue clear guidance on AI’s impact on HIPAA -protected health information (PHI).  
1.Federal agencies should address how AI -driven tools handle, store, and transmit PHI to
ensure compliance with privacy and security regulations.
2.ACA supports the federal government establishing guidelines to ensure AI platforms that
claim HIPAA compliance are verified by a federal clearinghouse.
3.HHS should  provide clarification of liability and enforce AI technology services only using
the data provided to them to the extent outlined in their contracts with health
providers.
Streamlining AI Implementation in Curriculum and Ensuring Workforce  Readiness   
States’ licensure requirements for MHCs typically include a lengthy set of criteria.20 Yet MHCs 
currently do not have the adequate workforce training needed to  implement AI consistently  and 
appropriately .  
•With  over 1 ,600 U.S. universities providing education  for mental  health counseling ,
there’s no single standard to how AI issues are incorporated into  curriculums. State
licensing boards should consider offering Continuing Education (CE) credits on AI -related
subject matters .
•Students are increasingly relying on AI to assist with or fully generate their
assignments.  ACA is concerned about the long -term effects and workforce
unpreparedness across sectors  resulting from the misuse of AI.
20 Requirements include: (1) possession of a master’s or doctoral degree in counseling from a national or regionally 
accredited institution of higher education, including an internship and coursework on the etiology of mental illness 
and SUDs, effective trea tment and counseling strategies, ethical practice, and other core knowledge areas; (2) 
passage of the National Counselor Examination (NCE) administered by the National Board for Certified Counselors 
or a similar state -recognized exam; (3) completion of a m inimum of 2,000 to 3,000 hours of post -master’s degree 
supervised clinical experience, performed within a certain time period, including a specific number of face -to-face 
supervision hours; (4) adherence to a strict Code of Ethics and recognized standards of practice, as regulated by a 
state’s counselor licensure board; and (5) periodic completion of continuing education credits/hours after obtaining 
licensure to remain current in their practice field.  


8 | Pa g eACA also supports greater student awareness of the ethical and appropriate use of AI, 
emphasizing its role as a supplemental tool rather than a replacement for critical 
thinking and skill development.  
Minimizing Risks of Discrimination  
ACA strongly supports federal regulators ensuring AI platforms are using appropriate data sets 
that do not lead to discrimination of any kind.  ACA believe s it is important for federal regulators 
to scrutinize how AI platfor ms learn, what data  sets are used to train and te st AI, and whether 
these data  sets are representative of all communities  across the U.S . Thorough, accurate, and 
representative data is essential for  reducing the risk of discrimination and  meeting the varied 
needs of clients.  
*** 
Conclusion  
ACA greatly appreciates the opportunity to comment on the Development of AI Action Plan  RFI. 
ACA is a committed partner with Congress, the White House, and public health agencies, 
serving as a valued resource related to beneficiaries’ access to behavioral health services. Please  
contact Guila Todd, Director of Governmental Affairs and Public Policy for  ACA, at 
or  if you have any questions or need additional information.  
Sincerely,  
Shawn E. Boynes, CAE, FASAE  
Chief Executive Officer  
American Counseling Association


