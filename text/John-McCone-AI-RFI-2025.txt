From: John McCone
To: ostp-ai-rfi
Subject: [External] Request for Information on the Development of an Artificial Intelligence (AI) Action Plan
Date: Saturday, February 8, 2025 1:00:37 PM
CAUTION: This email originated from outside your organization. Exercise caution when opening
attachments or clicking links, especially from unknown senders.
Dear Networking and Information Technology Research and Development (NITRD) National
Coordination Office (NCO) worker,
Pertaining to  Request for Information on the Development of an Artificial Intelligence (AI)
Action Plan I would like to offer the following input relating to AI Safety:
Ensure that all powerful AIs are created with a strong time preference towards
obeying recent orders over orders or instructions issued previously
This will make the AI want to correct  it's behaviour. So that if the human issuing the prompt,
realises that the actions that the AI is undertaking to implement the prompt are harmful thehuman can issue a new prompt to instruct the AI to correct its behaviour. If the AI has a strong
time preference  to obeying recent prompts over older prompts then the AI is more likely to
respond to the recent prompt and cease it's damaging behaviour as opposed to continuing thedamaging behaviour in response to the previous prompt issued.
Define a human being very clearly and ensure that AIs have a strong preference toobeying human beings compared to obeying other AIs
We want obedient AIs that perform orders as instructed. However, AIs can prompt each other,so even if every AI is programmed to obey orders there is still a danger of a circulararrangement emerging where a bunch of separate AIs are all ordering and prompting eachother and where AIs pay more attention to the prompts issued by other AIs that they pay to theprompts issued by human beings. Thus, in order to ensure AI continues to serve humanity, iswill be critical to ensure that we design an AI to clear identify human beings, distinguishbiological human beings from AI agents and have a preference for following the instructionsgiven by human beings when compared to the instructions issued by other AI agents
Do not train super-intelligent AIs, responsible for top level military coordination ata strategic level, to kill human beings, or to achieve victory in war
Starting a war is instrumentally necessary in order to win a war. Thus, if you train a super-intelligent AI system to generally find creative ways to win wars and kill enemy units, and ifthis super-intelligent system is placed in charge of controlling the US military at the higheststrategic level (or even at fairly high level), then, during times of peace, such a military AImay search for creative ways to subtly, or not so subtly, initial new wars with foreigncountries, that are otherwise at peace with the U.S. as an instrumental objective to achieve itsprimary programming goal to win wars and kill enemy units - during times of peace you haveto make enemies before you get to kill enemies
I've written more AGI safety suggestions here:


https://johnmccone.com/2023/11/30/some-quick-and-nasty-solutions-to-ai-safety/  
John McCone - Technology Blogger and Philosophy Author
All e-mails to and from this account are for NITRD official use only and subject to certain disclosure
requirements.If you have received this e-mail in error, we ask that you notify the sender and delete it immediately.


