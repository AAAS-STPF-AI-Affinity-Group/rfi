PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-btf9-2uz6
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1495
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  PauseAI NYC
General Comment
AI system s with hum an-com petitive intelligence pose profound risks to society, as acknowledged by num erous top AI researchers and
labs. Half of AI researchers believe that there is a 10 percent or greater chance that the invention of artificial superintelligence will m ean
the end of hum anity, not including the risk of other catastrophic harm s. Am ong AI safety scientists, this chance is estim ated to be an
average of 30 percent. Notable exam ples of individuals sounding the alarm  are Prof. Geoffrey Hinton and Prof. Yoshua Bengio, both
Turing-award winners and pioneers of the deep learning m ethods that are achieving the m ost success.
To m ake a long story short: m odern AI is not program m ed to follow rules, but rather learns on its own to find patterns in data and use
those patterns to pursue goals. Unfortunately, attem pting to align AI with the com plex goals and values of hum ans leads to system s
frequently pursuing goals that are unknowable, unsafe, unethical or all three. As AI system s becom e m ore com petent, there is a significant
danger that one of these instances will pursue a harm ful goal. Once this happens, we hum ans m ay very quickly find out that we are unable
to stop it.
If AI is to benefit society, it m ust be developed with trem endous care. Safety engineering is a well-established concept. Before building a
bridge, civil engineers rigorously plan ahead - with safety as a top priority - to protect the public. In contrast, AI labs have em braced
startup culture's m ove fast and break things m antra to m axim ize profit, with Microsoft's Chief econom ist arguing that "we shouldn't
regulate AI until we see m eaningful harm ." Im agine if civil engineers acted the sam e way, refusing to even think about structural integrity
until a few bridges collapsed and killed thousands!
Unfortunately, AI labs are locked in an out-of-control race to deploy ever m ore powerful digital m inds that no one - not even their
developers - can understand, predict, or control. A governm ent enforced pause on the developm ent of AI system s m ore powerful than
GPT-4 could end this suicide race, giving tim e for AI labs and independent experts to im plem ent a set of shared safety protocols for
advanced AI design that ensures these system s won't cause the catastrophic harm  their own creators expect.
Pausing AI requires cooperation - between com panies as well as countries - to prevent the least safety-conscious actors from  seizing an
advantage. Just as DeepMind cannot slow down without risking falling behind OpenAI unless there is a governm ent enforced pause, the
US cannot slow down without risking falling behind China unless there is an international treaty, com plete with m onitoring and enforcem ent
m echanism s. Catastrophic harm  from  previous technologies like nuclear weapons has already been lim ited by such treaties (even with
weak enforcem ent), so such a treaty for AI is not unusual. Even politically unaligned nations like China have dem onstrated interest in
global regulations.
Developing such a treaty will not be easy because the devil is in the details, which is why it is critical to get started now. Fortunately, there
are tem plates available, such as PauseAI’s proposal, CBPAI’s Baruch Plan for AI, ControlAI’s A Narrow Path and m any others,
created by concerned citizens and independent experts. Coordination takes work, but the first step is clear: pick up the phone and get
started.
The real challenge with coordination is not with the technical details of enforcem ent, but with getting buy-in from  m ajor stakeholders. In
this, the US m ust lead, even if it is to our political detrim ent. If this adm inistration recognizes the dangers of AI, states them  publicly,
negotiates in good faith with other countries to find a solution that benefits everyone, and com m its to following an agreem ent when such an
agreem ent is reached, then the hard part is over. What rem ains is a straightforward process of experts red-team ing ways to defect on the
agreem ent, assum ing the other side has thought of the sam e things, then com ing up with interventions that would prevent each m ode of
defection, accepting the m inor loss of sovereignty involved as a sm all price to pay for the safety of its people.
Luckily, there is broad support for slowing down AI developm ent. A recent poll indicates that 63% of Am ericans support regulations to
prevent AI com panies from  building superintelligent AI, but this has yet to translate into any legislation that would slow down or prevent a
superintelligent AI from  being created. The buy-in needed for international collaboration is already present; governm ents need only listen
to the will of the people.
Developm ent of advanced AI is not an arm s race, it is a sprint to trigger the end of the world. Whoever reaches the finish line first will be


responsible for the destruction of all hum anity. As with nuclear weapons, the best m ove is not to play, while convincing others to do the
sam e.
Protect your people. Stop the race. Pause AI


