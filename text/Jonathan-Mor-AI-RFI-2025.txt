This letter responds to the Request for Information on developing the AI Action Plan 
under Executive Order 14179, "Removing Barriers to American Leadership in 
Artificial Intelligence." As the United States aims to enhance AI leadership by 
fostering innovation while ensuring national security and economic competitiveness, 
it becomes critical to thoughtfully address potential risks associated with rapidly 
advancing AI technologies. 
Recent research suggests AI capabilities are advancing faster than previously 
anticipated. A comprehensive survey of AI researchers conducted in late 2022 
indicates a 50% probability of AI systems achieving human-level performance in all 
economically valuable tasks by 2047, with a 10% chance of this occurring as early 
as 2027 [1]. These accelerating timelines underscore the urgency of developing 
robust governance frameworks that can keep pace with technological advancement. 
The following sections outline key risk areas that merit particular attention in the AI 
Action Plan development process. Each represents a domain where proactive policy 
intervention could significantly reduce potential harms while preserving innovation 
potential. 
AI Alignment and Artificial General Intelligence 
Existential Risk Assessment 
The development of increasingly powerful AI systems, particularly those approaching 
artificial general intelligence (AGI), presents potential existential risks that warrant 
serious consideration. In a landmark survey of thousands of AI researchers, between 
38% and 51% assigned at least a 10% probability to advanced AI leading to 
outcomes as severe as human extinction [1]. Even if such probabilities seem low, the 
magnitude of potential harm justifies significant preventative efforts. 
These concerns align with warnings from prominent AI safety researchers like 
Eliezer Yudkowsky, who has argued that misaligned superintelligent systems could 
pursue goals incompatible with human survival. The difficulty of ensuring alignment 
increases with system capability, creating a potentially dangerous race condition 
between capability development and safety research [LessWrong.com]. 
Expert Warnings and Consensus 
The gravity of AGI risks has prompted unprecedented warnings from leading figures 
in AI development. In May 2023, a statement signed by industry leaders including 
Sam Altman (OpenAI), Demis Hassabis (DeepMind), and prominent researchers 
equated AGI risks with global catastrophic threats like pandemics and nuclear war. 
This statement emphasized that "mitigating the risk of extinction from AI should be a 
global priority alongside other societal-scale risks such as pandemics and nuclear 
war" [Future of Life Institute]. 
These concerns aren't merely theoretical. A 2017 survey of machine learning 
researchers predicted AI would outperform humans in many activities within the next 
decade, including translating languages by 2024 and writing high-school essays by 
2026 [4]. Many of these predictions have already been realized or exceeded, 
suggesting that expert timelines for more advanced capabilities may also prove 
accurate or conservative. 


Cybersecurity Threats and Vulnerabilities 
AI-Enhanced Attack Vectors 
The integration of AI into cybersecurity creates a complex threat landscape where 
defensive and offensive capabilities evolve simultaneously. The World Economic 
Forum's 2024 Global Risks Report highlights AI-generated misinformation and cyber 
insecurity among the most critical global threats, with potential to destabilize 
economies and disrupt critical infrastructure [World Economic Forum]. 
Advanced language models can automate the discovery of zero-day vulnerabilities, 
generate convincing phishing content that evades traditional detection methods, and 
orchestrate complex attacks that adapt to defensive measures in real-time. Research 
from security firms indicates that AI-generated phishing emails achieve significantly 
higher click-through rates than conventional attacks, demonstrating how these 
technologies lower barriers to conducting sophisticated operations. 
National Security Implications 
The proliferation of AI tools increases the complexity of cyberattacks, making it 
easier for adversaries to exploit vulnerabilities in government systems and private 
networks that support critical infrastructure. A DHS report notes that "U.S. 
Government officials across relevant agencies and departments need to define 
respective roles in the event of a real-time, foreign adversary launched GenAI attack" 
[dhs.gov]. This highlights the urgent need for coordinated response protocols that 
can address the unique challenges posed by AI-enhanced cyber operations. 
The asymmetric nature of AI-enhanced cyber capabilities is particularly concerning. 
Small teams with access to advanced AI models could potentially develop cyber 
weapons with impacts disproportionate to their resources, complicating traditional 
deterrence strategies and necessitating new approaches to cybersecurity 
governance that incorporate AI-specific risk factors. 
LLMs and Terrorism Facilitation 
Weaponization Potential 
Large Language Models (LLMs) represent a significant advancement in natural 
language processing, but their capabilities also create potential vectors for misuse by 
terrorist organizations. These models can be exploited to create propaganda 
materials that precisely target vulnerable populations, automate attack planning 
processes, develop technical instructions for weapons manufacturing, or facilitate 
recruitment through persuasive and personalized messaging. 
The dual-use nature of LLMs makes regulation particularly challenging. The same 
capabilities that allow these models to assist with legitimate activities like education 
or content creation can be repurposed to generate extremist content or detailed 
instructions for harmful activities. Anthropic researcher Jack Clark has noted that as 
these models become more widely available through open-source projects or 
commercial APIs, restricting access to malicious actors becomes increasingly difficult 
[Effective Altruism Forum]. 
Global Reach and Operational Efficiency 


The exploitation of LLMs by terrorist organizations not only enhances their 
operational efficiency but also broadens their global reach, creating new challenges 
for counterterrorism efforts. AI-generated content can be tailored to specific 
audiences, cultures, and languages, potentially expanding recruitment capabilities 
beyond traditional geographic constraints and facilitating coordination across 
dispersed cells without centralized command structures. 
This technological capability shift requires corresponding evolution in 
counterterrorism strategies that incorporate AI-specific monitoring and intervention 
approaches. Traditional content monitoring systems may prove inadequate against 
the sophisticated outputs of advanced language models deployed for malicious 
purposes, necessitating development of specialized detection methods and 
international cooperation frameworks. 
Deepfakes and Public Figure Impersonation 
Disinformation Campaign Capabilities 
Advances in generative AI have dramatically improved the quality and accessibility of 
deepfake technology, enabling the creation of highly convincing fabricated video and 
audio content. Research by Brookings Institution notes that "security and intelligence 
officials will inevitably use [deepfakes] in a wide range of operations" due to their 
potential impact, highlighting the dual challenge of both defending against and 
potentially employing these technologies [brookings.edu]. 
When applied to public figures such as government officials, business leaders, or 
cultural influencers, deepfakes can be weaponized to generate false narratives that 
erode institutional trust and potentially incite violence. During election periods, 
strategically timed deepfakes could sway voter opinions or suppress turnout. In 
international relations, fabricated statements attributed to world leaders could 
escalate tensions or trigger diplomatic crises. 
Detection and Attribution Challenges 
As generative AI technologies improve, the technical challenge of distinguishing 
authentic media from sophisticated deepfakes grows increasingly difficult. This 
creates an asymmetric advantage for those deploying disinformation, as detecting 
and debunking false content typically requires more resources than creating it. The 
World Economic Forum notes that "as the world heads deeper into the age of AI, 
legal measures to control its misuse will be vital to protect trust in information and 
institutions" [weforum.org]. 
Technical solutions alone may prove insufficient to address this challenge, 
necessitating complementary approaches including media literacy education, 
provenance verification systems, and potentially regulatory frameworks that establish 
accountability for malicious use while preserving beneficial applications. Scholars 
have noted that "social sciences and humanities research on deepfakes is rare 
overall and does not relate deepfakes' impact to core democratic norms," indicating 
a need for interdisciplinary approaches to this challenge [ncbi.nlm.nih.gov]. 
Negligence Risks from LLM Overreliance 


Professional Decision Support Concerns 
The impressive capabilities of Large Language Models have led to their rapid 
adoption across numerous domains, including healthcare, legal services, education, 
and financial advising. However, dependence on LLMs for critical decision-making 
without appropriate human oversight can result in serious forms of negligence with 
significant consequences for individuals and organizations. 
These models often generate outputs that appear authoritative yet contain subtle 
inaccuracies, biases, or logical flaws that may go undetected by users who lack 
expertise in the relevant domain. Scott Alexander of Astral Codex Ten has 
documented numerous examples of "hallucinations" in LLM outputs that could lead 
to harmful decisions if implemented without verification [AstralCodexTen.com]. In 
high-stakes contexts such as medical diagnosis, legal proceedings, or financial 
management, uncritical reliance on AI-generated recommendations could lead to 
adverse outcomes including physical harm, legal liability, or financial losses. 
Automation Bias and Professional Standards 
The risk of negligence is compounded by automation bias—the human tendency to 
give undue weight to computer-generated information and recommendations. This 
cognitive bias may lead professionals to override their own judgment in favor of 
AI-generated outputs, particularly when those outputs are presented with high 
confidence or in authoritative formats. 
Addressing these concerns requires development of professional standards and best 
practices for appropriate use of AI systems as decision support tools rather than 
autonomous decision-makers. Clear delineation of responsibilities between human 
professionals and AI systems is essential for maintaining appropriate accountability 
while capturing the benefits of these technologies. Organizations like the Partnership 
on AI have developed frameworks for responsible AI deployment that could inform 
professional standards across sectors [Partnership on AI]. 
AI Addiction and Mental Health 
Engagement Optimization Concerns 
The integration of increasingly sophisticated AI, particularly LLMs, into social media 
platforms creates unprecedented capabilities for personalized content curation and 
engagement optimization. While this enhances user experience in many ways, it also 
intensifies the addictive potential of these platforms by creating highly tailored 
content streams designed to maximize attention capture and retention. 
Research has linked excessive social media use to various negative mental health 
outcomes, including anxiety, depression, sleep disruption, and reduced productivity. 
The introduction of LLM-powered interactions may exacerbate these issues by 
creating more compelling and psychologically rewarding digital experiences that 
further displace real-world social interactions and activities. Tristan Harris of the 
Center for Humane Technology has described AI-optimized engagement as 
potentially creating "supernormal stimuli" that exploit psychological vulnerabilities 
more effectively than previous recommendation systems [Center for Humane 
Technology]. 


Vulnerable Populations and Social Development 
Young people are particularly vulnerable to both the addictive qualities of 
AI-enhanced social media and the potential psychological impacts. Developing 
brains are more susceptible to reward-based learning and have less developed 
impulse control mechanisms. As LLMs make digital interactions increasingly lifelike 
and personalized, the boundary between online and offline relationships becomes 
more blurred, potentially affecting healthy social development. 
Addressing these concerns requires multidisciplinary approaches that incorporate 
insights from psychology, neuroscience, and public health while developing technical 
standards and regulatory frameworks that prioritize user wellbeing alongside 
engagement metrics. Recent work by AI alignment researchers has explored 
mechanisms for ensuring AI systems respect human autonomy rather than exploiting 
psychological vulnerabilities for engagement [Alignment Research Center]. 
Data Privacy and International Models 
Cross-Border Data Governance 
AI models developed in countries with different data privacy standards, such as 
China, present unique concerns regarding the security and appropriate use of 
sensitive information. Models trained on vast datasets may inadvertently memorize 
personal or proprietary information, which could subsequently be extracted through 
careful prompting or other techniques. 
The regulatory environments governing data usage and AI development vary 
significantly across jurisdictions. Models developed under less stringent privacy 
regimes may incorporate practices that would violate standards like the General 
Data Protection Regulation (GDPR) in Europe or various sector-specific privacy laws 
in the United States, creating compliance challenges for organizations deploying 
these technologies. 
Strategic Considerations and Values Alignment 
Beyond individual privacy concerns, the widespread adoption of AI models 
developed under different regulatory philosophies raises strategic questions about 
technological sovereignty and values alignment. AI systems inevitably reflect the 
priorities, values, and governance frameworks of their development environments. 
As noted by philosopher Nick Bostrom, the values embedded in advanced AI 
systems may have profound long-term implications for society [Future of Humanity 
Institute]. 
Dependency on models developed under authoritarian governance structures could 
potentially undermine democratic values or introduce subtle forms of influence over 
time. This concern extends beyond explicit backdoors or surveillance capabilities to 
include more nuanced forms of value encoding that may manifest in model outputs 
and recommendations. The AI Action Plan should consider both immediate data 
security concerns and longer-term strategic implications of reliance on foreign AI 
systems. 
Strategic Technology Transfer 


Hardware Export Considerations 
The development of advanced AI capabilities is increasingly dependent on 
specialized hardware, particularly high-performance computing chips optimized for 
machine learning workloads. Exporting such advanced AI hardware to strategic 
competitors like China risks bolstering their technological capabilities and potentially 
undermining U.S. leadership in AI innovation and national security interests. 
The computational requirements for training cutting-edge AI models have grown 
exponentially, with the largest models now requiring thousands of specialized GPUs 
or TPUs operating in parallel. This hardware dependency creates a potential 
chokepoint in AI development that has significant strategic implications for national 
security and economic competitiveness. Chip manufacturing leader Nvidia has 
estimated that training the largest language models now costs tens of millions of 
dollars in computing resources alone [Nvidia]. 
Balancing Innovation and Security 
While restricting hardware exports presents one approach to maintaining 
technological advantage, such measures exist in tension with principles of open 
scientific exchange and global commerce. Overly broad restrictions could potentially 
harm U.S. semiconductor companies that benefit from global markets, while also 
accelerating efforts by other nations to develop indigenous alternatives. 
Effective policy must balance these competing concerns through targeted controls 
on truly critical technologies rather than broad categories, while simultaneously 
investing in domestic semiconductor manufacturing and advanced computing 
infrastructure to maintain U.S. competitive advantage. Recent analysis by the Center 
for Security and Emerging Technology provides frameworks for identifying specific 
hardware capabilities most critical to advanced AI development that could inform 
more targeted export control policies [Center for Security and Emerging Technology]. 
Financial System Integrity 
AI-Enabled Tax Evasion 
Advanced AI tools could facilitate increasingly sophisticated tax evasion schemes by 
automating complex financial manipulations that are difficult for traditional regulatory 
systems to detect. These capabilities could enable the creation of elaborate shell 
company networks, automated transaction laundering, or the identification of novel 
regulatory loopholes through analysis of tax codes and enforcement patterns. 
The potential scale and sophistication of AI-enabled tax evasion presents serious 
threats to public finance systems. Tax revenue losses could undermine government 
capacity to provide essential services and exacerbate existing inequalities by 
effectively shifting tax burdens to those less able to leverage advanced technologies 
for avoidance strategies. 
Regulatory Response Capabilities 
Addressing these emerging challenges requires investment in AI-enhanced 
regulatory capabilities that can match the sophistication of potential evasion 
strategies. This includes development of advanced analytics systems for detecting 


suspicious patterns, international cooperation on financial transparency and 
information sharing, and updating regulatory frameworks to address novel avoidance 
strategies enabled by AI. 
The trans-disciplinary framework approach suggested by emerging AI governance 
research emphasizes continuous risk management throughout product lifecycles to 
ensure accountability for AI use cases [7]. This philosophy could be productively 
applied to financial technology applications to balance innovation with appropriate 
oversight. Regulatory agencies will need both technological tools and updated legal 
frameworks to effectively counter AI-enhanced financial crimes. 
Economic Displacement and Transition 
Labor Market Disruption 
The accelerating advancement of AI technologies creates significant potential for 
labor market disruption across multiple sectors. While previous waves of automation 
primarily affected routine physical and cognitive tasks, newer AI systems 
demonstrate capabilities that extend to non-routine cognitive work, potentially 
impacting previously insulated professional and knowledge worker categories. 
Recent surveys of AI researchers indicate that the timeline for significant automation 
may be accelerating. The chance of all human occupations becoming fully 
automatable was forecast to reach 10% by 2037 and 50% by 2116 [1]. While these 
dates may seem distant, they represent a significant acceleration compared to 
previous surveys, suggesting that the pace of potential displacement may be 
increasing. The economist and blogger known as "The Zvi" has noted that AI 
progress appears to be outpacing economic adaptation mechanisms that historically 
cushioned technological transitions [Substack]. 
Distributional Effects and Policy Responses 
Beyond aggregate employment impacts, the distributional effects of AI-driven 
automation warrant particular attention. Benefits from productivity enhancements 
tend to accrue disproportionately to capital owners and highly skilled workers who 
complement AI systems, while displacement costs fall heavily on those whose skills 
are more substitutable. 
Without appropriate policy responses, this dynamic could exacerbate existing 
inequalities and potentially lead to social instability. Addressing these challenges 
requires expanded support for workforce development and transition programs, 
exploration of new models for education and continuous learning, and potentially 
adjustments to social insurance systems to address transition challenges. 
Organizations within the Effective Altruism community have developed frameworks 
for identifying high-risk occupations and designing targeted support programs that 
could inform policy development [Open Philanthropy Project]. 
Advanced Weapons Development 
AI in Biological and Chemical Applications 
The integration of AI capabilities into biological and chemical research presents 
profound dual-use concerns with security implications. Advanced AI systems could 


potentially accelerate the design of novel pathogens or chemical agents, optimize 
delivery mechanisms, or enable more precise targeting of specific populations based 
on genetic or other characteristics. 
These applications represent a particularly dangerous form of dual-use technology, 
as the same AI systems that advance beneficial biomedical research or chemical 
engineering could be repurposed for weapons development. The diffusion of 
powerful open-source AI models and biotechnology tools creates a particularly 
challenging governance problem, as capabilities become accessible to a wider range 
of potential actors. 
Governance Approaches 
Addressing these risks requires updated approaches to arms control and 
nonproliferation that account for the accelerating pace of technological change. 
Traditional verification regimes face significant challenges in monitoring AI-enabled 
research and development activities, particularly given the inherent dual-use nature 
of the underlying technologies. 
This necessitates development of novel verification approaches suitable for the AI 
era, enhancement of intelligence capabilities focused on detecting weapons 
development programs, and promotion of norms against weaponization of AI in 
biological and chemical contexts through international agreements and technical 
safeguards. The Biological Weapons Convention and Chemical Weapons 
Convention may require updates to address the specific challenges posed by 
AI-enhanced research capabilities [United Nations Office for Disarmament Affairs]. 
AI in Warfare Systems 
Autonomous Weapons Concerns 
The development of autonomous weapon systems that can select and engage 
targets without direct human intervention raises serious legal, ethical, and strategic 
questions. Such systems could potentially operate at speeds and scales beyond 
human cognitive capabilities, introducing new dynamics into conflict scenarios and 
potentially lowering thresholds for the use of force. 
Key concerns include accountability challenges for actions taken by autonomous 
systems, ethical dilemmas regarding the delegation of lethal decision-making to 
machines, and strategic risks of unintended escalation during conflicts due to 
interaction effects between autonomous systems operating at machine speeds 
without human judgment as a moderating factor. 
Strategic Stability Implications 
Beyond the battlefield implications of individual autonomous systems, the broader 
integration of AI into military planning, intelligence analysis, and command 
decision-making could impact strategic stability between major powers. AI systems 
processing incomplete information under time pressure might identify false patterns 
suggesting imminent attack, potentially triggering escalatory responses in crisis 
situations. 
Addressing these concerns requires development of clear doctrine regarding 
appropriate human control over AI-enabled weapon systems, support for 


international dialogue on norms and potential limitations for autonomous weapons, 
and research into the strategic implications of AI integration across military domains. 
Organizations like the Campaign to Stop Killer Robots have developed detailed 
frameworks for maintaining meaningful human control that could inform policy 
development [Campaign to Stop Killer Robots]. 
Information Ecosystem Integrity 
AI-Generated Disinformation 
AI-generated disinformation campaigns present an increasingly serious threat to 
democratic institutions and social cohesion. OpenAI research has noted that 
language models could potentially "enhance the scalability, adaptability, and 
effectiveness of influence operations" [openai.com]. The ability to produce 
convincing false content at unprecedented scale undermines public trust in media 
and institutions by flooding information ecosystems with misleading or divisive 
material designed to exploit social tensions and polarize communities. 
The emerging generation of multimodal AI systems can generate not only text but 
also images, audio, and video content that appears authentic. This capability 
enables the creation of synthetic evidence to support false narratives, making 
traditional forms of fact-checking increasingly challenging. When combined with 
algorithmic content distribution systems optimized for engagement, these 
technologies create powerful vectors for the rapid spread of misleading information. 
Democratic Resilience Strategies 
The impacts of AI-enabled disinformation extend beyond individual instances of 
misinformation to potentially undermine the shared factual basis necessary for 
democratic deliberation. When citizens cannot agree on basic facts due to exposure 
to conflicting information environments, the capacity for collective problem-solving 
and compromise is severely diminished. 
Countering these threats requires technical approaches to detect and label 
AI-generated content, development of digital literacy programs that build public 
resilience to disinformation, exploration of platform governance models that reduce 
algorithmic amplification of false content, and investment in trusted information 
sources and public interest media. Social scientists have noted that "greater 
theoretical grounding and conceptual clarity are needed to help specify and evaluate 
the harm (and potential) of current and future deepfake applications" 
[ncbi.nlm.nih.gov], highlighting the need for interdisciplinary approaches to this 
complex challenge. 
Conclusion 
The development of the AI Action Plan under Executive Order 14179 represents a 
critical opportunity to establish governance frameworks that maximize the benefits of 
artificial intelligence while mitigating potentially severe risks. As outlined in this 
response, these risks span multiple domains including national security, economic 
stability, public health, and democratic governance. 


The accelerating pace of AI development underscores the urgency of establishing 
appropriate safeguards. Recent surveys of AI researchers suggest that timelines for 
transformative capabilities may be shorter than previously anticipated, with 
significant milestones potentially being reached within years rather than decades 
[1][4]. This compressed timeline necessitates proactive rather than reactive 
approaches to governance. 
We urge policymakers to address the critical concerns outlined in this response in 
the formulation of the AI Action Plan. By incorporating robust safeguards, oversight 
mechanisms, and international cooperation frameworks, the United States can 
maintain technological leadership while ensuring that advancements in AI technology 
align with national security goals, economic stability, and societal well-being. The 
stakes are too high for anything less than a comprehensive, evidence-based 
approach to AI governance. Bibliography 
AI Timelines and Expert Predictions 
1. Grace, K., Salvatier, J., Dafoe, A., Zhang, B., & Evans, O. (2017). When Will AI
Exceed Human Performance? Evidence from AI Experts. arXiv.
https://arxiv.org/abs/1705.08807
2. OpenAI Authors. (2023). Thousands of AI Authors on the Future of AI. arXiv.
https://arxiv.org/abs/2401.02843AI Existential Risks and Governance 
3. Yudkowsky, E. (2008). Artificial Intelligence as a Positive and Negative Factor in
Global Risk. Global Catastrophic Risks, Oxford University Press.
4. Future of Life Institute. (2023). Global Statement: Mitigating AI Extinction Risk
Should Be a Global Priority. https://futureoflife.org
5. Hendrycks, D., Mazeika, M., & Woodside, T. (2023). An Overview of Catastrophic
AI Risks. arXiv. https://export.arxiv.org/pdf/2306.12001v5.pdfAI and National Security 
6. Department of Homeland Security (DHS). (2025). Impacts of Adversarial Use of
Generative AI on Homeland Security. DHS Report.
https://www.dhs.gov/sites/default/files/2025-01/25_0110_st_impacts_of_adversarial_
generative_aI_on_homeland_security_0.pdf
7. Center for Security and Emerging Technology (CSET). (2023). Strategic Risks of
Advanced AI Hardware Exportation. https://cset.georgetown.edu
AI and Cybersecurity Threats 
8. World Economic Forum. (2024). Global Risks Report 2024. Geneva, Switzerland.
https://weforum.org
9. Microsoft Security Team. (2023). AI-Enhanced Phishing and Cybersecurity
Challenges. Microsoft Blog.


Large Language Models (LLMs) and Societal Risks 
10. Clark, J. (2023). LLM Risks: The Dual-Use Nature of AI in Generative
Capabilities. Effective Altruism Forum. https://forum.effectivealtruism.org
11. Partnership on AI. (2023). Responsible AI Deployment Frameworks.
https://partnershiponai.org
12. Larks. (2024). LLMs as a Planning Overhang. Effective Altruism Forum.
https://forum.effectivealtruism.org/posts/CR7TjgsZgv8f5ST6h/llms-as-a-planning-ove
rhang
Deepfakes and Disinformation 
13. Cook, G., Phillips, L., & Rosenbach, E. (2023). Deepfakes and International
Conflict. Brookings Institution.
https://www.brookings.edu/wp-content/uploads/2023/01/FP_20230105_deepfakes_in
ternational_conflict.pdf
14. OpenAI. (2023). Forecasting Potential Misuses of Language Models for
Disinformation Campaigns and How to Reduce Risk.
https://openai.com/research/forecasting-misuse
15. World Economic Forum. (2024). AI-Generated Disinformation and the Need for
Legislative Action.
https://www.weforum.org/agenda/2024/02/ai-deepfakes-legislation-trust
AI and Ethical Concerns 
16. Campaign to Stop Killer Robots. (2022). Maintaining Human Oversight in
AI-Driven Warfare. https://stopkillerrobots.org
17. United Nations Office for Disarmament Affairs (UNODA). (2023). AI Implications
for Biological and Chemical Weapons Conventions. https://unoda.org
AI Alignment and Bias 
18. Clarke, S. (2022). A Survey of the Potential Long-term Impacts of AI. Effective
Altruism Forum.
https://forum.effectivealtruism.org/posts/3ffgjMEJ4jY4rdgJy/a-survey-of-the-potential-
long-term-impacts-of-ai
19. Kundu, R. (2024). AI Risks: Exploring the Critical Challenges of Artificial
Intelligence. Lakera. https://www.lakera.ai/blog/risks-of-ai
20. Clark, J. (2023). What should the UK's £100 million Foundation Model Taskforce
do?
https://jack-clark.net/2023/07/05/what-should-the-uks-100-million-foundation-model-t
askforce-do/
AI Addiction and Wellbeing 
21. Harris, T. (2023). Supernormal Stimuli: AI-Driven Behavioral Engineering. Center
for Humane Technology. https://www.humanetechnology.com


22. Alignment Research Center. (2023). AI Alignment With Respect to Human
Autonomy and Wellbeing. https://alignmentforum.org
Data Privacy and International Concerns 
23. Bostrom, N. (2014). Superintelligence: Paths, Dangers, and Strategies. Oxford
University Press.
24. European Union. (2018). General Data Protection Regulation (GDPR). Official
Journal of the European Union. https://gdpr-info.eu
Economic Displacement and Policy Responses 
25. Autor, D. H., Levy, F., & Murnane, R. J. (2003). The Skill Content of Recent
Technological Change: An Empirical Exploration. Quarterly Journal of Economics,
118(4), 1279–1333.
26. Open Philanthropy Project. (2023). AI-Driven Labor Market Disruptions and
Support Frameworks. https://openphilanthropy.org
27. Substack (The Zvi). (2023). AI Accelerations and Economic Adaptation
Challenges. Substack Blog.
Regulatory Frameworks and Strategic Planning 
28. Biden Administration. (2024). Executive Order 14179: Removing Barriers to
American Leadership in Artificial Intelligence. https://www.whitehouse.gov
29. Partnership on AI. (2023). AI Governance Frameworks for Deployment and
Monitoring. https://partnershiponai.org


