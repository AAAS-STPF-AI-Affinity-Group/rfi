 1 
 Developing an Action Plan for Artificial Intelligence (AI) Innovation:  
 A Framework Built on Transparency and Public Trust in AI  
 Darby Orcutt, Thomas A. Birkland, Veljko   Dubljević   , Kevin Lee, George List, William  
 Rand, Benjamin Reading, and Munindar P. Singh  
 EXECUTIVE SUMMARY  
 As academic researchers engaged in Artificial Intelligence development and innovation,  
 we share the vision of maintaining U.S. leadership in AI. Our universities (NC State  
 University and NC Central University), two of the “world-class research institutions”  
 noted in President Trump’s Executive Order 14179, exemplify this commitment. Our  
 shared interest in promoting successful AI innovation led us to collaborate as the Center  
 for AI in Society and Ethics (   go.ncsu.edu/ai-society   ). As the Executive Order asserts,  
 U.S. global leadership in AI requires “the right Government policies,” including robust  
 funding for both basic and applied AI research, with a focus on interdisciplinary  
 approaches such as ours - that foreground the social and human implications of AI.  
 AI is inherently shaped by and, in turn, shapes social institutions. AI technologies will  
 empower new heights of human progress. The rise of AI will usher in a new industrial  
 revolution, characterized by massive shifts toward human-machine collaboration,  
 transforming the human work and the workforce. It will boost medical care and scientific  
 discoveries into overdrive, rapidly impacting the quantity and quality of people’ s lives. AI  
 will utterly transform the core dynamics of economies, of education, and of the very  
 ways that humans interact with and relate to one another. The positive impacts that AI  
 innovation could have on people and society, both in the near and short term, cannot be  
 overstated.  
 Yet, public apprehension and wariness of AI, evidenced in surveys of public attitudes,  
 underscores the need for transparency and human-centric AI development. As Pflanzer  
 et al. (2023) note, “AI does not simply, automatically, and seamlessly integrate into our  
 daily lives and social institutions. Rather, it directly reshapes social, cultural, and  
 economic structures and affects the lives of individual citizens in profound and often  
 tacit, unpredictable, or morally questionable ways.” This highlights the potential for AI to  
 reweave or even tear our socioeconomic fabric, impacting not just productivity and  
 safety but also autonomy and dignity. AI innovation must be appropriately carried out  
 and governed to ensure that its massive potential benefits indeed manifest. Governance  
 principles that prioritize transparency and consent—fundamental to democracy—must  
 guide AI innovation.  


 2 
 Existing frameworks for technology oversight are often reactive, slow to adapt, and lack 
 adequate public input. Unlike past technologies, AI pervades every sector and 
 increasingly operates autonomously, requiring a proactive and holistic approach to 
 governance. Public concerns about AI undermine its adoption, consumer confidence,  
 and support for its development. T o address these challenges and facilitate effective 
 and fair human-AI teaming (see: Pflanzer et al. 2022), AI governance must integrate  
 both technical and public expertise throughout the development cycle.  
 Unsuitability of Current Governance Frameworks 
 Whereas the U.S. has instituted various governance mechanisms across multiple 
 domains, including the SEC for securities, NHTSA for highway safety , the FCC for  
 communications, and Institutional Review Boards (IRBs) for research involving humans,  
 these frameworks are often ill-suited for AI governance. Current mechanisms are  
 generally reactive, slow to adapt, and lack support for ongoing public engagement. In 
 the case of AI, the stakes are much higher, with concerns ranging from bias and  
 discrimination to the catastrophic risks posed by mis-deployed AI systems that could  
 endanger human lives. These existing frameworks, built in an era of slower  
 technological change, are not equipped to deal with the rapid evolution of AI 
 technologies. 
 Moreover, current governance mechanisms often operate in silos, with separate  
 agencies handling dif ferent technology domains. This fragmented approach is 
 problematic for AI, which, by its nature, will pervade all sectors of society and the  
 economy . Unlike previous technologies, AI has the capacity to operate autonomously , 
 making decisions without human oversight and shifting responsibility in unprecedented  
 ways. Therefore, a flexible, standards-based governance framework is necessary—one  
 that provides overarching guidance across sectors while addressing the unique 
 challenges posed by AI applications. 
 AI Innovation Requires a Foundation of Transparency and Consent  
 Successful AI innovation incorporates both technical and public expertise to ensure AI  
 development is innovative, aligned with the public good, and appropriately mitigates  
 risks. To the greatest extent technologically feasible, AI decision-making must be  
 explainable, accountable, and subject to public oversight. While open source  
 approaches should be encouraged, a balance of interests in terms of protecting  
 intellectual property and maintaining competitive advantage should be recognized as 
 legitimate. These principles align with similar administrative law doctrines, consumer 
 protection laws, and financial disclosure mandates, such as the Equal Credit 
 Opportunity Act. 


 3 
 Specifically, standards of governance should: 
 ● For Transparency  :
 ○ Require AI systems to include self-monitoring capabilities that track
 performance on both domain-specific and domain-independent key
 performance indicators (KPIs).
 ○ Ensure that AI systems report their performance transparently to relevant
 stakeholders.
 ○ Establish pathways for continual improvement and correction, not just for
 individual decisions but for the overall operation of AI technologies. This is
 essential as stakeholders may discover new needs, imperfections, or
 unintended consequences after interacting with the system.
 ● For the Public’s Consent  :
 ○ Ensure that AI systems are auto-correcting so mistakes can be undone
 before they entrench undesirable practices.
 ○ Implement standards for determining the appropriate level of scrutiny for a
 particular AI technology based on its domain of application, with the
 potential for revisiting these standards as technologies evolve.
 ○ Actively engage affected communities and other stakeholders in defining
 their needs, values, and risk attitudes. Public consultation should be
 integral to the governance of AI to maintain trust and transparency .
 Given the intimate nature of many AI applications, governance frameworks must ensure 
 that the perspectives of affected communities are included in decision-making. AI 
 systems should be treated as ongoing research projects, where interdisciplinary teams, 
 including relevant human-centric experts (for example, in psychology , economics, and 
 ethics) are involved in every stage of development. 
 A Proposed AI Governance Framework  
 Built upon this foundation of transparency and consent, we propose flexible standards  
 of governance for AI (rather than sheer regulation) to ensure ef fective, human-centered  
 outcomes while fostering innovation. This framework values oversight, adaptability , and 
 compliance with the U.S. Constitution and existing regulations and statutes. 
 Accordingly , our proposed governance framework consists of five pillars:  
 1. Continual Monitoring and Improvement  : AI systems should support continual
 self-monitoring, track performance against defined benchmarks, and facilitate
 ongoing updates. These principles are informed by similar policies in FDA
 medical device regulations, algorithmic trading oversight, and real-time safety
 monitoring in self-driving vehicle laws.


 4 
 2. Bias Prevention  : The risk of systemic bias in AI applications requires
 safeguards. This governance framework mandates fairness audits, independent
 oversight, and compliance with equal opportunity and civil rights statutes. It
 aligns with the Equal Employment Opportunity Commission’s (EEOC) disparate
 impact doctrine and recent Department of Justice guidance on AI-driven hiring
 discrimination.
 3. Risk-Proportional Scrutiny  : AI governance must adapt to the risk level of
 applications. High-risk applications, such as those in healthcare and criminal
 justice, should undergo heightened scrutiny, while lower-risk applications may
 require lighter oversight. This approach mirrors the FDA’s risk-tiered classification
 of medical devices and IRB oversight models. Risk is understood to be a function
 both of the likelihood of an undesirable outcome or incident and the magnitude of
 the harm that would result from that incident. Like with the FDA and with the
 IRB’s oversight of research, the greatest attention must be paid to those AI
 applications that pose the greatest risk. Even outcomes that are unlikely require
 considerable attention if the consequences of an error are sufficiently grave.
 These risks must be carefully weighed by competent authorities in the public and
 private sector, and systems must be designed to ensure appropriate monitoring
 and oversight.
 4. Active Stakeholder Engagement  : AI governance must incorporate multiple
 perspectives, including technologists, ethicists, policymakers, and the general
 public. This approach is supported by participatory rulemaking processes, such
 as The Federal Advisory Committee Act (FACA), reinforcing the necessity of
 inclusive governance.
 5. Support for Interdisciplinary AI Research  : AI innovation requires not only
 funding, but funding of basic and integrative research encompassing all aspects
 of AI, from computer science to neurocomputational ethics to domain-specific
 applications, and more. In addition, researchers require access and clear
 guidelines in using proprietary data crucial to AI training. The Fair Use Doctrine
 was set forth in Section 107 of the U.S. Copyright Act to enable research in the
 pre-digital age; it must be made clear that this still applies in the digital world as a
 foundational Doctrine protecting research uses from obstruction by terms of use,
 licensing, or other agreements. Generative AI innovation cannot happen without
 content, and actual or feared litigation in this area is already stifling U.S.
 competitiveness in AI.
 This governance framework finds support in existing legislative directives, including the 
 National Artificial Intelligence Initiative Act of 2020 (NAIIA)  (15 U.S.C. 9401 et seq.), 
 which establishes interagency coordination and defines national AI priorities, and aligns 
 with the  Federal Trade Commission (FTC) Act   (15 U.S.C. 41 et seq.), which provides 
 statutory authority for enforcement against deceptive AI practices.  


 5 
 Conclusion 
 AI governance should not merely react to failures but should proactively shape the 
 responsible development and deployment of AI technologies. By adopting a rational and 
 ethical sociotechnical perspective (see: Chopra & Singh 2018; Dubljević et al. 2021) 
 and leveraging AI’s capabilities, we can create an effective governance model that  
 fosters innovation while safeguarding public trust and societal well-being. Standards of  
 transparency and informed consent, together with interdisciplinary engagement, will  
 ensure that AI technologies are developed with the public's best interests in mind and 
 are capable of serving the common good. 
 References: 
 Chopra, A. K. & Singh, M.P. (2018): Sociotechnical systems and ethics in the large. In  
 Proceedings of the AAAI/ ACM Conference on Artificial Intelligence, Ethics, and Society  
 (AIES), pp. 48–53, New Orleans, ACM. DOI: 10.1 145/3278721.3278740. 
 Dubljević, V ., List, G., Milojevich, J., Ajmeri, N., Bauer , W.A., Singh M.P., Bardaka, E., 
 Birkland, T .A., Edwards, C.H.W ., Mayer, R.C., Muntean, I., Powers, T.M., Rakha, H.A., 
 Ricks, V .A. & Samandar , M.S. (2021): Toward a Rational and Ethical Sociotechnical 
 System of Autonomous V ehicles: A Novel Application of Multi-Criteria Decision 
 Analysis. PLoS ONE, DOI: 10.1371/journal.pone.0256224.  
 Pflanzer , M., Dubljević, V., Bauer, W.A., Orcutt, D., List, G.F. and Singh, M.P. (2023): 
 Embedding AI in Society: Ethics, Policy , Governance and Impacts (Editorial), AI & 
 Society 38:1267-1271. DOI: 10.1007/s00146-023-01704-2.  
 Pflanzer , M., Traylor, Z., Lyons, J., Dubljević, V. & Nam, C.S. (2022): Ethics of 
 Human-AI T eaming: Principles and Perspectives. AI and Ethics, DOI: 
 10.1007/s43681-022-00214-z.  
 This document is approved for public dissemination. The document contains no  
 business-proprietary or confidential information. Document contents may be 
 reused by the government in developing the AI Action Plan and associated 
 documents without attribution. 


