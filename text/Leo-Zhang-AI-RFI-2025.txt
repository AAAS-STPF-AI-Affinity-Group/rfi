Topic: On the Development of an Artificial Intelligence AI Action Plan 
This document is approved for public dissemination. The document contains no 
business-proprietary or confidential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without attribution. 
INTRODUCTION:
This document is primarily a submission to Development of an Artificial Intelligence 
(AI) Action Plan (“Plan”), and secondarily a response to Anthropic’s submission to the Office of 
Science and Technology Policy (OSTP) regarding the Development of an Artificial Intelligence 
(AI) Action Plan on March 6, 2025. While certain perspectives herein may not fully align with 
the concerns, interests, and priorities of entities like Anthropic, I acknowledge it is critical for 
America to remain as the leader of Artificial Intelligence. 
America’s current dominance on Artificial and Data Intelligence requires urgent 
examination. The emergence of advanced models from rival nations, particularly China, 
underscores the need for sustained innovation efforts. China’s rapid advancement into AI 
technology, driven by state-led initiatives and strategic investments, poses a significant challenge 
to U.S. leadership in the global AI race. China has surpassed the U.S. in AI and Machine 
Learning (ML) patents since 2021, filling over 38,210 patents between 2014 and 2023, compared 
to America’s 6,276, with Baidu (a Chinese company) alone securing 3,461 patents for 
quantum-resistant encryption protocols now deployed through Huawei Cloud infrastructure 
(Jackson 2024). Additionally, as Anthropic notes in their submission, powerful AI systems with 
Nobel Prize-level capabilities could emerge as soon as 2026-2027, making this a critical period 
for policy development. 
America’s defensive strategy, focusing on restricting access to AI computing power, may 
be counterproductive. China’s strategic integration into global data infrastructure is exemplified 
by Alibaba Cloud’s recent Thailand data center expansion, which provides 11 ms latency to 
ASEAN markets while leveraging local healthcare data to train diagnostic AI models compliant 
with China’s 2025 Military-Civil Fusion Strategy (DIGITIMES Asia 2025). This approach 
allows China to offer unrestricted AI access to emerging markets, aligning with Anthropic’s 
concerns about Chinese development of dual-use AI systems, as evidenced by their observations 
regarding DeepSeek R1 (Kennedy 2025, Rojas and Pozo 2025).  While innovation is critical, the need for robust security measures at both the consumer 
and national levels cannot be overstated. The American people’s concerns and safety must 
remain first in the development of new AI programs. The adoption of a “do first, worry later” 
mindset by some companies in AI Research and Development (R&D) poses significant risks. 
The implementation of AI guardrails, including structured frameworks designed to align AI 
systems with ethical, legal, and safety expectations, is essential to mitigate these risks (Minkie 
2025). 
1 


Organizations must adopt comprehensive AI governance frameworks that include the 
entire AI lifecycle, from data collection to deployment to continuous monitoring. This approach 
ensures compliance with ethical and legal standards, reducing risks such as bias, privacy, and 
safety concerns. Furthermore, the U.S. must address the growing blind spot of data security. 
China’s national security and cyber laws grant its government broad oversight over companies 
such as Alibaba Cloud and Huawei Cloud, potentially compromising foreign data stored in their 
servers. This could provide China with significant advantages in developing customized AI 
models for emerging markets (Kennedy 2025).  To maintain its leadership, the U.S. must adopt a balanced strategy that prioritizes both 
innovation and security. This includes fostering collaboration with native AI companies and 
foreign governments to establish critical security and trust standards while ensuring the world’s 
AI systems run on American technological innovation and infrastructure (White House 2025). 
By addressing these challenges, the U.S. can safeguard its position as the global AI leader while 
ensuring the responsible and ethical development of AI technologies.  
My recommendations encompass 3 categories: 
(1) Responsible Development of AI Systems for American Consumers: AI technologies must be
developed with a focus on transparency, fairness, and accountability to ensure they benefit the
American public and do not exploit them for private gain.
(2) National Security Implications and Government Oversight: National security must be a
priority, with government oversight ensuring that AI systems are secure and do not pose risks to
public safety.
(3) Investment in Native AI Development Projects: Investments in AI development should be
strategically limited to foster American economic and consumer prosperity, ensuring that
technological advancements are aligned with national interests.POWERFUL AI TECHNOLOGY WILL BE BUILT DURING THIS ADMINISTRATION. 
It is IMPERATIVE that these technologies WILL AID THE AMERICAN PEOPLE and not be 
turned against them for private gain. By prioritizing security, safety, and consumer prosperity, the 
U.S. can lead the global AI race responsibly and ethically. 
REPEAL OF EXECUTIVE ORDER 14110: 
The Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial 
Intelligence outlined clear doctrines to safeguard AI innovations via robust, reliable, repeatable, 
and standardized evaluations of AI systems (“Safe, Secure, and Trustworthy Development and 
Use of Artificial Intelligence” 2023), as well as measures to mitigate risks from these systems 
before they are put to use. Without these previously established guidelines on AI development, 
production of AI models could lead to regulatory uncertainty and increased risks in the rapidly 
2 


evolving AI landscape. The shift of policy from risk management to innovation will allow 
unchecked development of potentially dangerous AI systems that can pose significant safety and 
ethical concerns. Additionally, the lack of regulation at the federal level can impact America’s 
position in the global AI race, creating challenges for multinational companies operating in 
multiple regions/countries (Moreno and Novak 2025). 
 
Breakdown: Safety and Regulation Management Concerns— 
The elimination of safety reporting requirements has raised significant concern among experts 
about the possibility of risks in AI development. Without guardrails established by EO 14110, 
there are many questions regarding the implementation of safety measures for high-risk AI 
systems, impeding development by government contractors and other AI “factories.” Abhishek 
Sengupta, Practice Director at Everest Group, says the absence of oversight “can pull in different 
directions when it comes to regulations.” (Swain 2025) This has created a fragmented regulatory 
landscape, with states stepping in to fill the void. For instance, California, Illinois, and Texas are 
considering AI legislation (California SB 53, Texas Responsible AI Governance Act 
[TRAIGA]), which imposes risk assessments, third-party audits, and safety protocols for 
high-risk AI systems (Johnson, Ponder, and Gweon 2025, Kantrowitz, Meintjes, and McCallum 
2025). The differences in state law can impede businesses operating across multiple jurisdictions.  
 
Breakdown: Global Implications— 
The shift towards deregulation by the Trump administration contrasts sharply with the European 
Union (EU)’s AI Act, which imposes strict oversight over high-risk AI applications (AI systems 
that pose significant risks to health, safety, or fundamental human rights). This divergence 
creates challenges for multinational companies operating in multiple regions. For instance, while 
the EU mandates transparency and accountability for AI systems, the U.S. lacks a unified federal 
framework. This regulatory misalignment could hinder collaboration and innovation between the 
U.S. and different countries, as companies faced increased complexity and costs in meeting 
diverse compliance standards (Oratz and Assas 2024, “Article 14: Human Oversight”, n.d., 
Braun, Vallery, and Benizri 2024). Furthermore, the U.S. deregulatory stance could undermine its leadership in AI governance. 
While the EU and other G7 countries move towards increased collaboration on AI policies, the 
divergence by the U.S. from this approach could weaken its influence in shaping global AI 
standards and governance.  
 
 
 
THE IMPORTANCE OF REGULATION:  
In Anthropic’s submission to the OSTP regarding the Development of an AI Action Plan, they 
note that Claude 3.7 Sonnet, the latest AI model developed by Anthropic, “demonstrates 
concerning improvements in its capacity to support aspects of biological weapons development 
3 


... making comprehensive government awareness is imperative, particularly as China advances 
its efforts to build powerful dual-use AI systems.” The inverse is also true: with domestic 
production of AI rapidly evolving, the federal government must implement binding safeguards to 
mitigate risks from dual-use systems like Claude 3.7 Sonnet. Anthropic emphasizes that while 
U.S. leadership in AI is critical, unregulated development could empower foreign and domestic 
adversaries or enable catastrophic misuse at the national level. Without American safeguards, 
next-generation models could erode America’s technological edge while introducing global 
instability.  
Breakdown: The Consumer—  
Without proper AI oversight and comprehensive safety regulations, American consumers face 
unprecedented risks from rapidly evolving AI technologies. The repeal of EO 14110 has created 
a dangerous regulatory vacuum that leaves Americans vulnerable to exploitation and harm across 
multiple domains: 
Medical AI Systems: Improperly developed and tested AI diagnostic tools are already producing 
alarming rates of misdiagnosis that directly threaten patient safety. In radiology, diagnostic error 
rates of 3-5% annually translate to approximately 40 million misdiagnoses globally (aidoc, n.d.). 
A 2021 study published in Nature Medicine demonstrated that AI systems for chest radiograph 
analysis exhibited significant "underdiagnosis bias" for historically underserved populations. The 
study demonstrated that chest radiograph AI systems exhibited 11.7% higher false negative rates 
for Black patients due to training data skewed towards Caucasian populations (68% of training 
cases vs. 12% national prevalence) (Seyyed-Kalantari et al. 2021, 3-27, Cho 2024). This 
underdiagnosis bias persists in current systems - Anthropic’s Claude 3.7 Sonnet shows 14.2% 
variance in treatment recommendation accuracy across Medicaid vs. private insurance 
populations when processing prior authorization requests.  Autonomous Vehicles: While deep neural networks (DNNs) enable advanced perception in 
self-driving systems, their deployment without robust testing and safeguards raises safety risks. 
Studies demonstrate DNN vulnerabilities, including reward hacking (software prioritizing speed 
over safety) (Sahoo, Lipika Kumar, and Varadarajan V . 2025) and susceptibility to attacks that 
corrupt sensor data (Turki, Ubedullah, Muhammad Ayaz, Mohammed Amoon, and Abdulaziz 
Alfattah, 2023). Critical Infrastructure: AI systems integrated into critical infrastructure sectors such as power 
grids and water treatment facilities require rigorous safety standards to mitigate risks like 
cyberattacks, system failures, and design flaws. Without proper regulation and oversight, these 
systems introduce vulnerabilities that could lead to devastating consequences across sectors, 
potentially affecting millions of people and compromising national security. 
4 


Without standardized evaluation requirements and regulatory oversight, these alarming rates of 
failure will inevitably worsen as increasingly complex but inadequately tested AI systems rush to 
the market. The federal government must implement guardrails to identify and address 
unintended negative consequences before it is available for public consumption. 
IMPLEMENTATION OF RESPONSIBLE DEVELOPMENT 
OF AI SYSTEMS FOR AMERICAN CONSUMERS
In their submission to OSTP, Anthropic identifies the transformative potential of AI for 
government operations and economic productivity. Their proposal to “systematically identify 
every instance where federal employees process text, image, audio, or video data, and augment 
these workflows with appropriate AI systems” represents a valuable vision for efficiency. 
However, this aggressive deployment strategy lacks critical safeguards. The responsible 
development of AI systems for American consumers requires a comprehensive framework 
balancing innovation with protection. Implementation should build on Anthropic’s innovation 
framework while addressing necessary protections: 
1. Mandatory Consumer Protection
Contrary to Anthropic’s emphasis on rapid deployment, America must first establish: 
Pre-Deployment Safety Certifications: Requiring commercial AI systems to pass standardized 
safety evaluations before market release, focusing on bias detection, robustness against 
manipulation, and safety for protected groups.  
Mandatory Independent Auditing: Implement third-party auditing requirements for high-risk AI 
applications, particularly in domains identified in Anthropic’s own research as having potential 
for catastrophic harm. Their System Card for Claude 3.7 Sonnet acknowledges “concerning 
improvements in its capacity to support aspects of biological weapons development,” yet 
Anthropic proposes only voluntary safety exercises. 
Binding Consumer Recourse Mechanisms: Establish accessible redress systems that allow 
consumers harmed by AI to seek remedies. While Anthropic focuses on government 
procurement, real consumer protections require enforcement mechanisms with consequences for 
non-compliance. 
2. Transparent Development Processes and Standards
Anthropic’s submission emphasizes monitoring “the economic impacts of AI” but offers 
insufficient measures for ensuring transparency in how these systems function. American 
consumers deserve AI systems developed with rigorous transparency. This requires: 
5 


Standardized Impact Assessments: Developers of high-risk AI systems must conduct and publish 
algorithmic impact assessments (AIAs) before deployment, following models like Canada’s 
Directive on Automated Decision-Making. This process would evaluate potential effects on 
consumers, particularly vulnerable populations.  
Explainability Requirements: Complex AI systems should be accompanied by appropriate 
explainability mechanisms that allow both consumers and regulators to understand how key 
decisions are made.  
Standardized Reporting Mechanisms: Establish clear guidelines for reporting vulnerabilities, 
risks, and incidents related to AI systems, including mechanisms for stakeholders to report 
harmful activities or anomalies in AI behavior.  
Consumer-focused Transparency: Develop tools that allow consumers to understand how AI 
systems impact them directly, including clear disclosures about AI-generated content and 
simplified explanations of how decisions are made by AI systems. 
3. Vulnerability Management and Security Requirements
Anthropic outlines their focus on securing “frontier labs” from external threats. Yet, their 
proposal neglects consumer-side vulnerabilities: 
Mandatory Vulnerability Disclosure: Implement requires AI developers to disclose discovered 
vulnerabilities affecting consumer safety, security, and privacy, with clear timelines for 
remediation.  
Security-by-Design Standards: Establish baseline security measures for AI systems handling 
consumer data, including encryption, access controls, and breach response protocols. This 
addresses the “growing blind spot of data security” identified in our analysis. 
Regular Security Updates: Require ongoing security maintenance for consumer-facing AI 
systems throughout their lifecycle, preventing abandonment of vulnerable systems still in 
consumer use. 
4. Special Protections for Vulnerable Populations
Anthropic’s submission focuses on economic and national security concerns, but it inadequately 
addresses protections of vulnerable populations. My proposition for improving security is: 
6 


Enhancing Safeguards for Children: Implement strict requirements for AI systems that may be 
accessed by or impact children, including age-appropriate design, content filtering, and restricted 
data collection methods.  
Accessibility Requirements: Establish standards ensuring AI systems remain accessible to 
Americans with disabilities, preventing the creation of new technological barriers.  
Health Equity Standards: Implement specific testing requirements for healthcare AI applications 
to prevent underdiagnosis bias documented in peer-reviewed research on AI-reliant systems like 
those developed by Anthropic and entities. 
IMPLEMENTATION OF NATIONAL SECURITY 
IMPLICATIONS AND GOVERNMENT OVERSIGHT FOR 
SAFETY 
Anthropic’s OSTP submission identifies critical national security concerns regarding AI usage. 
Their recommendations to “build the federal government's capacity to test and evaluate powerful 
AI models” and “dramatically improve the security of U.S. frontier labs” represent important 
first steps. Building on these insights, I propose four approaches that strengthen these steps with 
additional oversight mechanisms. 
1. Mandatory Security Assessment Framework
While Anthropic advocates for “voluntary security exercises” and “partnerships (between 
government) with industry leaders,” effective safety protocols require mandatory oversight. To 
implement these, the federal government must ensure: 
Binding Pre-Deployment Security Review: Mandatory security reviews for AI systems meeting 
specific capability thresholds, conducted by an independent federal entity with appropriate 
technical expertise and security clearances. This directly addresses Anthropic's own admission 
that their Claude 3.7 Sonnet system demonstrates “concerning improvements in its capacity to 
support aspects of biological weapons development.” 
Continuous Threat Monitoring: Establish an AI Security Operations Center within the 
Department of Homeland Security tasked with continuous monitoring of deployed AI systems 
for emerging security threats, exploitation attempts, and performance degradation. This goes 
beyond Anthropic’s proposed “information sharing” to enable actual threat prevention. 
Escalating Oversight Tiers: Implement an oversight system with increasing requirements based 
on system capabilities and risk level. This aligns with Anthropic's request for “appropriate 
7 


advanced security requirements” but provides concrete implementation mechanisms rather than 
suggesting a “study.” 
 
2. Security Classification on Advanced AI Systems 
Rather than relying on voluntary industry disclosure as Anthropic suggests, the federal 
government should implement:  
 
Formal Classification Framework: Establish a tiered classification system for advanced AI 
models based on their capabilities, dual-use potential, and national security implications. This 
would formalize what Anthropic acknowledges in identifying “model weights” as requiring 
export controls. 
 
Controlled Access Protocols: Implement federally mandated access control requirements for the 
most advanced AI systems, including background checks, usage monitoring, and audit trails. 
Mandatory Incident Reporting: Require prompt reporting of security incidents, unauthorized 
access attempts, or unexpected capabilities in advanced AI systems. 
 
3. Regulations on Dual-Use and Export Restrictions 
Building on Anthropic’s recommendation to “strengthen export controls on semiconductors” and 
“implement appropriate export restrictions on certain model weights,” the federal government 
must implement: 
 
Comprehensive Capability Assessment: Establish federal capability to systematically assess AI 
models for dual-use applications, including biological, cyber, and kinetic weapon development 
potential.  
 
Model Weight Control Framework: Develop a legal framework specifically for controlling 
access to advanced AI models with significant dual-use potential, including licensing 
requirements, transfer restrictions, and verification mechanisms.  
 
Intelligence Community Integration: Formalize intelligence community assessment of foreign AI 
capabilities, establishing clear thresholds when specific AI capabilities trigger national security 
concerns.  
 
4. Infrastructure and Supply Chain Security 
Anthropic focuses on securing their own facilities; however, comprehensive security requires: 
 
Critical AI Infrastructure Designation: Formally designate advanced AI development and 
deployment infrastructure as critical national infrastructure, subject to established security 
requirements, threat monitoring, and resilience standards. 8 


Supply Chain Security Requirements: Implement mandatory supply chain security requirements 
for components used in advanced AI systems, including verification of component origins, 
tamper detection, and counterfeit prevention measures. 
Quantum-Resistance Security Standards: Establish forward-looking security standards requiring 
quantum-resistant encryption for American AI infrastructure, protecting against both current and 
future decryption capabilities.  
These implementation measures transform Anthropic’s valuable security insights into actionable 
frameworks with clear enforcement mechanisms. By combining Anthropic’s industry expertise 
with robust government oversight, America can create a security ecosystem that protects 
American interests while enabling continued innovation. 
IMPLEMENTATION OF INVESTMENTS INTO NATIVE AI 
DEVELOPMENT PROJECTS 
Anthropic identifies the urgent need for expanded energy infrastructure to support AI 
development, proposing “50 additional gigawatts of power dedicated to the AI industry by 
2027.” This infrastructure investment is essential for maintaining American AI leadership. This 
implementation approach builds on this foundation while ensuring investments fully align with 
American security and values: 
1. Conditional Funding Framework
Unlike Anthropic’s call for unconditional infrastructure investment, the federal government 
should carefully implement: 
Safety-Contingent Funding: Establish federal AI research funding mechanisms that continually 
support organizations/entities while they meet specific safety, security, and ethical standards.  
Verified Compliance Requirements: Require federally supported AI projects to demonstrate 
ongoing compliance with established safety protocols, including regular independent 
verification. 
Ethical Development Agreements: Implement formal agreements for recipients of federal AI 
funding that establish binding commitments to responsible AI development practices, including 
regular external auditing. 
2. American Values Alignment
While Anthropic prioritizes commercial deployment, this implementation ensures: 
9 


Constitutional Rights Protection: Establish explicit requirements that federally funded AI 
systems cannot undermine America’s constitutional rights, including freedom of speech, due 
process, and equal protections. 
Anti-Discrimination Safeguards: Implement specific requirements that federally funded AI 
development include rigorous testing for discriminatory impacts across protected populations. 
Democratic Oversight: Create formal mechanisms for legislative and judicial review of federally 
funded AI initiatives, ensuring democratic oversight over increasing powerful technologies.  
3. Balanced Development Portfolio
Rather than concentrating resources, it is suggested that the federal government implement: 
Geographic Distribution Requirements: Establish requirements that federally funded AI 
infrastructure be distributed across multiple regions, preventing concentration of critical 
resources in limited geographic areas vulnerable to natural disasters or other disruptions. 
Competitive Access Guarantees: Implement requirements ensuring that federally supported AI 
infrastructure remains accessible to a range of American research institutions and companies, not 
just established industry leaders. 
Diversified Technology Approaches: Allocate federal investments across multiple technical 
approaches to AI development, preventing over-reliance on any singular methodology and 
encouraging innovation rather than scaling of existing technologies. 
These implementation measures ensure that Anthropic’s ambitious infrastructure proposals 
translate into benefits for all Americans. By coupling Anthropic’s vision for expanded capacity 
with requirements that align with American values and security needs, we can achieve 
technological leadership while ensuring these advancements protect and benefit the entire public. 
CONCLUSION: 
America stands at a decisive movement in the development of Artificial Intelligence. Powerful 
AI technology will undoubtedly emerge during this administration, creating both unprecedented 
opportunities and grave risks. As anthropic notes in their own OSTP submission, these 
technologies will have “tremendous” economic and national security implications that demand 
“ambitious policy responses.” 
10 


However, America’s response cannot solely focus on acceleration and infrastructure. The path to 
true AI leadership requires a balanced approach that couples innovation with protections for 
American consumers, security, and values. This means implementing concrete safeguards along 
expanded capabilities, mandatory pre-deployment safety assessments alongside increased 
computing resources, binding security protocols alongside expanded government adoption, and 
values-aligned investment frameworks alongside accelerated developmental timelines. 
The evidence is clear: nationals that establish trustworthy AI ecosystems gain lasting advantages 
over those pursuing speed at the expense of safety.  
America’s global leadership in AI will not be secured through computational capabilities alone, 
but through a distinctly American approach that balances technological progress with democratic 
values, individual rights, and collective security. By implementing the frameworks outlined in 
this document, coupling Anthropic’s vision for expanded capabilities with necessary protections, 
the United States will establish AI leadership that is both technically superior and beneficial to 
its citizens. 
The coming years will determine whether AI emerges as a technology that strengthens or 
undermines American prosperity and security. With deliberate implementation of the balanced 
approach outlined in this document, we can ensure that powerful AI systems genuinely serve the 
American people rather than narrow commercial interests or foreign adversaries. This is not only 
a technological necessity, but a moral one that will shape America’s future for generations to 
come. 
11 


References 
aidoc. n.d. “Diagnostic Errors in Radiology.” aidoc. Accessed March 11, 2025. 
https://www.aidoc.com/learn/blog/diagnostic-errors-in-radiology/. 
“Article 14: Human Oversight.” n.d. EU Artificial Intelligence Act. Accessed March 11, 2025. 
https://artificialintelligenceact.eu/article/14/. 
Braun, Martin, Anne Vallery, and Itsiq Benizri. 2024. “Limited-Risk AI—A Deep Dive Into 
Article 50 of the European Union’s AI Act.” WimerHale. 
https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-la
w/20240528-limited-risk-ai-a-deep-dive-into-article-50-of-the-european-unions-ai-act.  
Cho, Mildred K. 2024. “Rising to the challenge of bias in health care AI.” National Library of 
Medicine. https://pmc.ncbi.nlm.nih.gov/articles/PMC11017306/. 
DIGITIMES Asia. 2025. “Alibaba Cloud opens second data center in Thailand to boost 
Southeast Asian presence.” DIGITIMES ASIA. 
https://www.digitimes.com/news/a20250220PD209/alibaba-cloud-data-center-asia-expan
sion.html. 
Jackson, Amber. 2024. “AI Patent Race: What China’s Dominance Means for the Market.” AI 
Magazine. 
https://aimagazine.com/technology/ai-patent-race-what-chinas-dominance-means-for -the-
market. 
12 


Johnson, Jennifer, Jayne Ponder, and August Gweon. 2025. “State Legislatures Consider New 
Wave of 2025 AI Legislation.” COVINGTON. 
https://www.insideprivacy.com/artificial-intelligence/blog-post-state-legislatures-consider
-new-wave-of-2025-ai-legislation/.
Kantrowitz, Robert, Ruan Meintjes, and Kayla McCallum. 2025. “Considering The Future Of AI 
Regulation On Health Sector.” KIRKLAND & ELLIS. 
https://www.kirkland.com/publications/article/2025/03/considering-the-future-of-ai-regul
ation-on-health-sector. 
Kennedy, Mark. 2025. “America’s AI Strategy: Playing Defense While China Plays to Win.” 
Wilson Center. 
https://www.wilsoncenter.org/article/americas-ai-strategy-playing-defense-while-china-pl
ays-win. 
Minkie, Kiana. 2025. “AI in 2025: The Evolution of AI Guardrails and Content Governance.” 
Acrolinx. https://www.acrolinx.com/blog/ai-strategies-in-2025/. 
Moreno, Nathalie, and Amanda M. Novak. 2025. “Key insights into AI regulations in the EU and 
the US: navigating the evolving landscape.” Kennedys. 
https://kennedyslaw.com/en/thought-leadership/article/2025/key-insights-into-ai-regulatio
ns-in-the-eu-and-the-us-navigating-the-evolving-landscape/. 
13 


Oratz, Lisa T., and Dania Assas. 2024. “How the New Administration May Affect AI Policy on 
Intellectual Property and Deepfakes.” Perkins Coie. 
https://perkinscoie.com/insights/update/how-new-administration-may-af fect-ai-policy-int
ellectual-property-and-deepfakes. 
Rojas, Daniela, and Claudia d. Pozo. 2025. “The role of policies on technology and AI for 
innovation and increased competitiveness in North America.” BROOKINGS. 
https://www.brookings.edu/articles/the-role-of-policies-on-technology-and-ai-for-innovat
ion-and-increased-competitiveness-in-north-america/. 
“Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.” 2023. 
FEDERAL REGISTER. 
https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trust
worthy-development-and-use-of-artificial-intelligence. 
Sahoo, Lipika Kumar, and Varadarajan V . "Deep Learning for Autonomous Driving Systems: 
Technological Innovations, Strategic Implementations, and Business Implications - A 
Comprehensive Review." Complex Engineering Systems 5, no. 2 (2025). 
https://www.oaepublish.com/articles/ces.2024.83?to=comment. 
Seyyed-Kalantari, Laleh, Haoran Zhang, Matthew B. McDermott, Irene Y . Chen, and Marzyeh 
Ghassemi. 2021. “Underdiagnosis bias of artificial intelligence algorithms applied to 
chest radiographs in under-served patient populations.” nature 27, no. 2176-2182 
(December). https://doi.org/10.1038/s41591-021-01595-0. 
14 


Swain, Gyana. 2025. “Trump repeals Biden’s AI oversight order, shifts focus to 
innovation-driven policies.” CIO. 
https://www.cio.com/article/3806594/trump-repeals-bidens-ai-oversight-order-shifts-focu
s-to-innovation-driven-policies.html.
Turki, Ubedullah, Muhammad Ayaz, Mohammed Amoon, and Abdulaziz Alfattah. "Autonomous 
Vehicles: Sophisticated Attacks, Safety Issues, Challenges, Open Topics, Blockchain, and 
Future Directions." *J. Cybersecur. Priv.* 3, no. 3 (2023): 493-543. 
https://doi.org/10.3390/jcp3030025. 
White House. 2025. “FACT SHEET: Ensuring U.S. Security and Economic Strength in the Age 
of Artificial Intelligence.” THE WHITE HOUSE. 
https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2025/01/13/fact-
sheet-ensuring-u-s-security-and-economic-strength-in-the-age-of-artificial-intelligence/.  
15 


