PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-gdu7-6z4h
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1588
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Konstantyn Spasokukotskiy  
Email: 
General Comment
See attached file.
This file is a copy that has been prior sent to  and which reception has not been confirm ed.
Attachments
AI action plan rfi 03.2025


 To: 
 Faisal D'Souza  
 National Coordination Office (NCO) at  
 Information Technology Research and Development (NITRD)  
 2415 Eisenhower Avenue,  
 Alexandria, VA 22314, USA  
 From: 
 Konstantyn Spasokukotskiy   
 Subject:  
 A comment on   the Development of an Artificial Intelligence   (AI) Action Plan (“Plan”)  
 in response to Request for Information (RFI)  
 issued by NSF on 02/06/2025, document # 2025-02305 (90 FR 9088)  
 that processes President Trump’s Executive Order 14179 (   Removing Barriers to  
 American Leadership in Artificial Intelligence   ). 
 Input:  
 On the highest priority policy actions  
 that should be in the new AI Action Plan  
 A suggestion about concrete AI policy actions that address the topics:  
 assurance of AI model outputs   , risks,   regulation and   governance   , technical and  
 safety standards, national security and defense, research and development,  
 innovation and competition, international collaboration, and import controls.  
 This document is approved for public dissemination. The document contains no  
 business-proprietary or confidential information. Document contents may be reused by the  
 government in developing the AI Action Plan and associated documents without attribution.  
 24th February 2025  
 page  1  of 11 


 1. Summary
 ● Securing American AI competitive edge is the correct primary objective. Thanks to
 Trump executive orders it is being reestablished.
 ● An omnipotent AI that is lacking safeguards is a weapon benefiting the enemy.
 ○ An overly potent AI lacking safety does help the enemy to burden the owner.
 ○ An overly restricted AI does help the enemy to maintain the upper hand.
 ○ The dichotomy “potency vs. safety” has merit, but was driven by a
 misinformed approach in Biden executive orders.
 ● AI safety and AI potency are not mutually exclusive.
 ○ The root cause of the fallacy thinking was potency and safety mapped on the
 same financial axes. The financial entanglement made the aspects
 development mutually competitive.
 ○ In fact, it is technically possible to strengthen the safety and boost AI potency
 at the same time, through the same expense. The development can be
 coherent.
 ○ AI safeguarding can come at nearly zero costs.
 ○ A set of organizational measures can disentangle the AI aspects and enable
 their coherent development.
 ● The proposed organizational measures to disentangle AI potency and AI safety have
 been tried and proven in many other industries.
 ○ A time-division principle is proposed to isolate the objectives.
 ○ A detailed policy set can be copy-pasted from the construction industry that
 regulates safety via building permits.
 ○ A governmental entity is required to establish and monitor the regulation.
 ○ The government entity wants to implement multiple separate bodies - at least
 one dependent on AI short-term growth and the other focusing on long-term
 safety.
 page  2  of 11 


 2. Introduction
 2.1 An ancient pattern 
 nce upon a time there was a monster  
 in ancient Greek. It had untold powers that 
 might bring immense destruction, or 
 alternatively protect and crown princes. 
 Kyriarchos (Kυρίαρχος) was adored and  
 feared at the same time. Most people did 
 not dare to come close to the forest, 
 where Kyriarchos was living.  
 group of young princes, dreaming of  
 patriarchy, meant to domesticate the 
 monster. Since no living thing ever 
 survived a close encounter with the 
 monster , nobody had any sense of what  
 might be the best strategy for dealing with  
 the monster. A prince named Evgenikos  
 (Ευγενικ ός) proposed to bring treats 
 regularly until the monster developed an addiction. Kyriarchos would then (supposedly) 
 obey the master of the treats. Storgikos (Σ τοργικός) proposed to test the monster’s taste 
 first. He thought that focusing on a single bait will make the ef fort more af fordable and 
 effective. A prince named Synetos (Συνετός) proposed to test consumption too. He aimed 
 for a reduced rationing to keep the monster under control, by restricting its ability to satisfy 
 its desires.  
 t turned out the monster had been a nocturnal carnivore,   who favoured hunting for fast  
 moving mice. It had a ration comparable to that of a middle-sized domesticated cat. 
 Kyriarchos was extremely cautious and sensible. It never ever went out of the forest just in 
 case somebody was nearby watching. Over time the princes made themselves believe that 
 they fed a cute cat. Yet Synetos insisted on regular testing. He noted soon that either the  
 cat had been growing, or proliferating. The princes concluded, they would stop feeding if 
 the consumption exceeded a certain threshold. Their hope was that an animal comparable 
 to a bear could be tamed by three young athletes.  
 othey proceeded with their respective plans. All  went well until one day the three ,
 Evgenikos, Storgikos and Synetos,  met their new pet. They now stood face to face with a 
 three-story-building sized tiger . Kyriarchos was wise to cheat the experimenters out of their 
 safety. It took the princes hostage demanding supplies  in return for their release. The 
 feeding continued until a complete impoverishment of their princedoms; Kyriarchos 
 devoured stock, inventory and humans until nothing was left. 
 page  3  of 11 


 2.2 President Trump’s right moves 
 In response to Biden-Harris  Executive Order 14110  of October 30, 2023 (Safe, Secure, and  
 Trustworthy Development and Use of Artificial Intelligence) [1] the largest AI developers and 
 operators adopted a safety policy that red-teamed their AI models. Similar to the third 
 prince’ s approach, our contemporaries want to undertake restrictive actions to contain the 
 AI-threat below a threshold. It proved futile. The Greek princes were naive in measuring the 
 non-human capabilities by human standards.   T rump  Executive   Order 14148   of January 20, 
 2025 [2],  rightly revokes Biden-Harris AI Executive  Order  14110 [1]. 
 2.3 The issue yet to be addressed 
 Productivity requires the removal of barriers to American leadership [3]. Infrastructure  
 vulnerability would be one of the barriers. Former approaches to deal with AI-vulnerability  
 have been polarized and idealistic: the “monster” ought to be tamed, or ef fectively starved. 
 Problem resolution is misguided by radical blueprints, falling to elusive targets and futile 
 practices.  
 The issue presents a classic dilemma: an   unrestricted  leverage of AI is outright 
 dangerous, yet capping the leverage is  counterproductive  .  By nature, dilemmas 
 demand balancing opposing forces, making them more challenging to address than 
 standard problems. Relying on one-sided approaches often leads to detrimental 
 consequences. Crafting an artificial Intelligence Action Plan could have been a constructive 
 step toward tackling this issue effectively.  
 There does exist a solution for resolving the dilemma. This letter presents the key points. 
 An operationalization of the solution would be a set of simpler tasks. The US government  
 has a track record of resolving such tasks with great ef ficiency. The author abstains from  
 those instructions until deliberately and specifically requested. 
 page  4  of 11 


 3. The problem
 3.1 Key concept 
 The  essence of the problem lies in the leverage that  an integrated AI can provide  .  AI 
 can evolve and elaborate any idea, as well as launch its implementation by a series of 
 automated actions or clear instructions to a group of people. Implementation can happen at 
 a scale that has never been seen before. The ability to scale is a positive development goal 
 and the key objective in international competition. 
 A malicious user would use the leverage to further their goals, to detrimental results. A fully 
 autonomous system can  execute harming orders with  speed and magnitude that are 
 not containable by  any known  law enforcement  system.  This concerns the police, the 
 defence department, and market regulators, as well. Even a tiny misdemeanor could be 
 amplified and multiplied to awful proportions, dwarfing the most egregious felonies with 
 respect to their impact. APTs could use the amplification against American interests.  
 3.2 Two-sided interventions 
 The dilemma “an unrestricted leverage of AI is outright dangerous, yet capping the leverage  
 is counterproductive” emanates from two sides: 1) AI capability growth, 2) utility  
 governance. Trump's Executive Order 14179 [3] aims to place the former back onto the 
 industry agendas. However, that in itself would not be sufficient. The AI Action Plan must  
 address the latter as well. For any solution to work ef fectively both sides have to be 
 balanced at any given time. 
 The balance is existential, it falls out of the economic realm. The market has to mature 
 before the balance (a stale ecosystem) is maintained by the market players. It is initially a  
 task for the federal institutions to establish proper AI governance at par with the AI growing  
 capacity. Should it be neglected, a disaster of epic proportions would be addressed by the  
 question “when”. The adversaries will certainly stress the situation and use the unattended 
 AI powers to command American failure. Accelerated development would require intensive 
 government interventions on both sides of the dilemma.  
 The  concept of balance has not been taken into consideration  so far. This could be the  
 most important feature in the upcoming AI Action Plan and legislation set. A similar 
 approach once made Napoleon famous, as he introduced the Napoleonic Code with 
 balanced rights and responsibilities.  
 3.3 One-sided scenarios are failures  
 Let’s consider an example of one-sided intervention. Reiterating the previous Biden-Harris 
 AI safety approach [1], the AI players were essentially set to control the action amplification 
 factor. Red-teaming was to test how much a Kyriarchos can chew at once, i.e. how much 
 harm AI is capable of causing. If it were too much, the AI-model was supposed to be held  
 back following the Synetos approach. 
 This approach has not separated the two sides of the dilemma. As a result, every safety  
 effort was mapped onto the commercial axes, forming a   negative feedback  mechanism. It is 
 page  5  of 11 


 antithetical to the AI arms race, which is currently unfolding and gaining steam. Companies 
 are vying for commercial success, which lets them acquire enough resources to allow them 
 to stay in the race. The negative feedback loop puts a contender back in the race, while 
 others proceed. It would be naive and overly optimistic to anticipate its fair implementation. 
 AI companies are predisposed to yammer about more subsidies for increasingly 
 deceptional and inefficient safety activity.  
 Tying safety to the action amplification factor , or expressed differently, setting an 
 AI-leverage cap, is a shot in the leg, too. The amplification factor will grow and exceed any 
 threshold as a result of the dynamics of the international contest. The NSF and NITRD  
 National Coordination Of fice would also want to help the US economy to outcompete any  
 contender for AI capabilities for as long as it goes. 
 If we negate the Synetos approach, i.e. apply   positive   feedback  , it will also fail. By getting 
 rid of testing, we do not eliminate the factor that constrains growth. W e  replace a  
 deliberate decision by a random trigger   that would  painfully constrain the proverbial 
 monster and provoke it to strike unexpectedly. Allowing a random trigger to initiate the chain 
 of events as it was in the Greek story, is extremely reckless. Unfortunately , such is the 
 situation in which we find ourselves. 
 There are multiple parties, who directly benefit from vibrant AI development. All of them  
 accelerate the market dynamics. If one of the parties, let it be for example the energy  
 supply party, ceases their participation for legitimate reasons, or even at a whim, the market  
 pull will first empty out all dependents. That is, the energy shortage will trigger uneven and 
 exorbitant prices. Retail consumers wouldn’t be capable of competing with bulk purchasers,  
 like AI-computing farms. AI-farms would be overpaying, exploiting their productivity drivers  
 and higher margins. Regulation, by itself, will not be able to cope with the phenomenon. 
 Any preference for little Jill would just trigger spreading the AI capacity onto the edges, 
 similar to how mainframes were dissolved by the on-premise PC-like servers. In the end, it 
 would be even harder to balance distributed AI deployment and to control its impact. 
 While spreading was a desirable feature in the PC-era, contributing to the software 
 availability and infrastructure (Internet) resiliency, now it is neither beneficial nor desirable.  
 A consolidated AI development and deployment would be cheaper to sponsor , easier to 
 oversee, simpler to govern, and quicker to saturate the market, thus quicker producing a 
 stale self-regulated ecosystem and a responsible AI culture. Software quality was optional  
 during the PC-era. It evolved over time. The AI-era demands certain quality standards  
 (benevolence) right upfront. 
 page  6  of 11 


 4.  Solution 
 4.1 Requirements 
 A sponsored AI development would benefit from a  balanced  approach.  Such an approach 
 encompasses a)  AI capacity  growth that is  matched  by  2) an appropriate  governance 
 standard  . These objectives should be isolated and  driven separately, offering no 
 opportunity for cross-mapping, merging, and detrimentally competing. 
 A  federal agency   must shape balanced AI development,  as well as monitor the AI 
 operations to  ensure that practice matches the intention  (documentation). 
 The key principles here are: 
 ●  A balance between AI capability and AI control has to be an 
 ○  auditable and 
 ○  audited feature. 
 ●  The balance method is not fixed. The balance utility must be checked if the AI 
 control: 
 ○  is capable of containing the full leverage scale, or alternatively the spillovers 
 are properly insured, 
 ○  has a reasonable safety margin for the anticipated leverage, 
 ○  strength is evaluated in relation to the maximal anticipated AI-power , i.e. the  
 controls are dimensioned not by the mathematical expectations but by the top 
 outliers;  
 ●  AI control does not weaken and corrupt over time. 
 ○  Evaluations do not draw measures from the human bases, but from the 
 (theoretical) technical top values, 
 ○  the engineering teams work in a framework that is insulated against 
 malfeasance, 
 ○  there is an approved mitigation action plan to address the contingency of an 
 inadvertently triggered event. 
 It has to be assumed that despite all the efforts, there would be no complete oversight. At  
 the AI-capabilities exceeding that of humans it is theoretically not feasible. Therefore the 
 governance standards should employ a self-adjustment mechanism. Self-adjustment would 
 augment human oversight. Assuming that AI capacity can substantially grow under the 
 radar, the mechanism is to demand at any facility disregard its actual AI-capacity and staff. 
 4.2. Policy actions 
 From the standpoint of industrial investment, a two-legged development is much too  
 expensive. Therefore any attempt to mandate it, in the absence of rigorous enforcement,  
 would provoke fraud. The second leg reserved for sustainability, robustness and balance is 
 an outright executive’s heresy , who spend life creating and exploiting imbalances. An 
 imbalance is in the blood of the market competitors. The balancing and enforcement job is 
 specifically reserved for a governance entity . 
 page  7  of 11 


 The sheer scale of the AI phenomenon will draw resources noticeable on the W orld GDP 
 scale. Even the most formidable countries would experience challenges facing the 
 economic pressures. The governance entity in charge of AI regulation should be not less 
 than federal, if not pan-regional in its scope. 
 A  method to isolate the dilemma objectives  clearly  may be  time-division  . The reaction 
 time and pace in the control loops could be drastically different. Then a cross-mapping  
 becomes impractical, reducing the chances that the system degrades into a one-sided 
 problem.  
 For example, utility governance could be driven by the long-term incentives exclusively , and  
 the AI capability growth by short-term. The market institutions could take the short-term 
 regulations over . A special purpose non-profit entity could regulate the long-term. All 
 short-termism should be prohibited by its mandate. A coordination across the entities for 
 synergetic ef fect could be mandated to additional parties, thus prohibiting migration of the 
 focus. Such a governance model is proposed as follows. 
 The federal government establishes   two entities   . The  entities  approve and affirm AI 
 licenses  . One entity checks the documentation (long-term  objectives). Another entity 
 orders regular auditing of the AI operations (short-term objectives). A  third entity  might get 
 a  law enforcement  mandate to swiftly dismantle rogue  AI operations. 
 The AI licensing process could resemble a construction permit process. The construction 
 industry also places long-term safety in the forefront. One could tap those experiences and 
 legislative solutions that already exist today.  
 4.2.1 Approval 
 The prince's failure was to assess the monster by human originated units of measurement.  
 It led to a systematic underestimation. This was a direct consequence of their  
 page  8  of 11 


 practical/commercial approach. Easily obtained facts take precedence over hard to get 
 hypotheses. The same bias is to be anticipated in any for-profit organisation. 
 A theoretical approach would have drawn on the secondary data sources, like paw size, 
 ground displacement, concealed monitoring equipment, etc. It could provide an alternative 
 measurement unit base and a plausible estimation of the monster size and strength. Thus 
 the prince’s desire to test and tame the monster was fundamentally correct, but poorly  
 implemented due to a biased perception. 
 An entity , which is not being pressured by a commercial agenda, would not cut corners on  
 AI-safety investigation methods. This approach has been implemented many times over . 
 NSF  could be a useful intermediary that approves AI   operation licenses. The approval has 
 to be based on the theoretical controllability. The decision underpinning will be provided by 
 the companies. NSF would need to prove the evidence and logic. More importantly , a party 
 like NSF could also draw conclusions based on the   state of the art   knowledge from multiple  
 independent sources, which otherwise would not be shared between competing parties. 
 4.2.2 Affirmation 
 An auditor periodically affirms that the approved and the operated systems are identical. 
 Financial auditors  could expand their role to include   monitoring AI operations.  
 The federal government may then gate incentive funds distribution on the basis of the 
 compliance results. 
 4.2.3 Enforcement 
 Despite a rigorous licensing process, there could be regular demand for an unprepared, 
 unannounced inspection, as well as for immediate operational interventions. A law 
 enforcement unit ought to react swiftly to the crime, which is amplified by AI. 
 A fair share of petty crime would be effectively held in check by the unit. To expand the  
 checked area, the unit would be differentiated by AI preparedness and a mandate to 
 preemptive strike without a court order and public exposure on certain occasions. The law 
 enforcement unit ought to establish special weapons and tactics that help to operate in a  
 cyber environment.  
 Governments at all levels may establish a feedback mechanism that allows unobtrusive 
 help by the industry players. The players would contribute to structuring, as well as sponsor 
 the centralized  dedicated law enforcement unit  , which  has  higher reaction speed  and 
 outreach. The market player’s incentive should be to offload the tasks onto government  
 agencies, and to elicit economies of scale. Multiple players would contribute to the same 
 shared agency . Alternatively each player can take responsibility for the damage incurred by 
 its leveraged attacks, i.e. if they were performed in its facility . Over time, as the AI market  
 matures, the enforcement unit could be privatized and run in the interest of the players to  
 maintain the status quo.  
 page  9  of 11 


 4.3 Solution examples 
 It is expected that the AI operators will deny and actively oppose some requirements. The 
 self-adjustment and malfeasance protection frameworks being likely examples. Y et it is of 
 primary importance that the requirements remain fulfilled, otherwise a commercial 
 suboptimization will disable any balanced and capable governance.  
 In order to sweep of f the argument that those requirements are not realistic, here are  
 references to the methodologies that do exactly what is required. The examples are not 
 meant to limit readers' thinking. There could be other approaches that fit the demand.  
 4.3.1 Self-adjusting AI control  
 AI system leverage is the primary reason to deploy AI. The leverage develops along a 
 growing AI system scale today. The AI scale grows at breathtaking speed. The AI decision 
 making scale would exceed that of the transnational corporations in a matter of a few years. 
 No individual, or group, would be able to compete with AI in the near future. AI systems  
 themselves and their operators separately will be predisposed to cheat without being  
 noticed from that point onwards. An overscaled oversight would not carry us much further 
 either.  
 All AI systems with a potential to grow and get more sophisticated over time must adopt 
 some mechanisms for self-adjustment and self-alignment. The mechanism would embed a  
 self-induced   “moral” and “ethics” into the AI models.  The mechanism is similar to what we 
 vote for and encounter in legislation drafters. It will likely become better with AI capacity 
 growth. Hence, as people loosen their grip, AI gains through self-improvement. An option to 
 implement such a mechanism is described in [4]. 
 4.3.2 Malfeasance framework 
 No technical system is immune to the threat of treachery or incompetence from service 
 personnel. Covert changes in system setup is a long-known problem in banking. The 
 banking frameworks are a step in the right direction, but are still far from being water-tight. 
 A significantly expensive bank crime would trigger a significantly expensive chase. The trick 
 is to raise the bar for a minimal possible crime up to the point that a successful chase is 
 inevitable. 
 Such an approach may not work for AI. Whoever gains full control over AI leverage will be 
 in the position to outcompete any effort by detectives and prosecutors to catch up.  
 An AI framework has to shift focus on not letting malign individuals infiltrate and settle in 
 any AI control center . An explanation of social dynamics and instructions for such a  
 framework are described in [5]. 
 page  10  of 11 


 References 
 [1] Baiden, J., Executive order 14110, “Safe, Secure, and Trustworthy Development and
 Use of Artificial Intelligence”,  FedReg   , Oct. 30th,  2023.
 [2] Trump, D., Executive order 14148, “Initial Rescissions of Harmful Executive Orders and
 Actions”,  FedReg   , Jan. 20th, 2025.
 [3] Trump, D., Executive order 14179, “Removing Barriers to American Leadership in
 Artificial Intelligence”,  FedReg   , Jan. 23rd, 2025.
 [4] Spasokukotskiy, K., “Synthetic consciousness architecture”,  Front. Robot. AI   , 11.2024.
 [5] Hardy, J.B., “The hidden game revealed: The game & the players”, iUniverse, 2010.
 page  11  of 11 


