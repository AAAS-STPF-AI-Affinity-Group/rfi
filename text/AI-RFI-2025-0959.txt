PUBLIC SUBMISSIONAs of: March 21, 2025
Received: February 25, 2025
Status: 
Tracking No. m 7l-1t9g-hcx3
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-0959
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Anthony LaVista Esq 
General Comment
I would urge the adm inistration; in crafting it's action plans, to not overlook the im portance of requiring the inclusion of ethical and m oral
controls in any new AI that is created. Artificial intelligence can and will be a great help to m ankind. We just need to ensure that we create
safeguards to protect us from  the drawbacks of turning over tasks to a non-biological consciousness.


5. Case Studies: Failures of Laissez-Faire AI Governance
a. Facial Recognition Misuse
- In 2024, an unregulated facial recognition system  m isidentified a state legislator as a shoplifting suspect, leading to wrongful detainm ent.
The vendor faced no penalties due to absent federal accountability laws.
- Lesson: EO 14110’s transparency m andates could have prevented this harm  by requiring accuracy reporting and third-party validation.
b. Autonom ous Vehicle Safeguards
- A 2025 Tesla “Full Self-Driving” update caused 17 collisions due to edge-case failures. The National Highway Traffic Safety
Adm inistration (NHTSA) lacked authority to enforce pre-deploym ent safety testing.
- Lesson: Regulatory vacuum s incentivize profit-driven deploym ent over public safety.
c. Healthcare Diagnostics
- IBM Watson Health’s AI system  recom m ended unsafe treatm ent protocols for cancer patients in 2023, later attributed to biased training
data. No federal m echanism  existed to recall or investigate the flawed m odel.
- Lesson: EO 14110’s incident reporting requirem ents would have enabled rapid corrective action.
The stakes extend beyond econom ic m etrics: they define whether AI will deepen societal divides or elevate collective well-being. 
REFERENCES: 
1. Executive Order 14110, 88 FR 75191 (2023).
2. EU AI Act (2024).
3. Stanford HAI, 2024 AI Developer Survey.
4. ACLU, Algorithm ic Bias in Hiring (2024).
5. MITRE Atlas, Adversarial Threat Landscape Report (2023).
6. Brookings Institution, AI Autom ation and Econom ic Equity (2025).


