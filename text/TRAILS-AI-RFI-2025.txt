 | Page 
 1 Response to the National Science 
Foundation’s Request for Information on 
the Development of an Artificial 
Intelligence (AI) Action Plan 
Submitted by: The Institute for Trustworthy AI in Law and Society (TRAILS) 
Overview  
America dominates global markets for AI. As Vice President Vance noted, “The U.S. possesses 
all components across the full AI stack, including advanced semiconductor design, frontier algorithms, and, of course, transformational applications.”
1  In addition, the US  has abundant  
funds for capital investment, a robust public and private research sector , extensive AI expertise,  
large pools of various types of data, and tools to protect AI stakeholders from harmful practices 
or misuse.  Despite these advantages, AI will not be adopted if people do not believe that they 
can trust its behavior. To maintain and realize the full potential of America’s AI dominance it is 
essential that we invest in mechanisms – across AI design, development, deployment, and 
governance – to ensure that the technology is trustworthy and that people know when to trust it. 
By fostering trust in AI, we can harness its power to drive human flourishing, ensuring that AI 
serves to enhance well-being, opportunity, and progress for all. 
Sustaining U.S. leadership in AI demands more than continued investment or technological 
advantage alone—it requires trust. While other nations actively pursue strategies to enhance their 
AI capabilities and competitiveness, the U.S. must also distinguish i ts technology through 
reliability and trustworthiness. Recognizing this, the Institute for NSF-NIST Trustworthy AI Institute for Law & Society (TRAILS) is the first organization to integrate AI participation, 
technology and governance during the design, development, deployment and oversight of AI 
systems. We investigate what trust in AI looks like, how to create technical AI solutions that 
1 https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-the-artificial-
intelligence-action-summit-paris-france 


 | Page 
 2 build trust, and which policy models are effective in sustaining trust.2 With that perspective in 
mind, TRAILS maintains that the AI Action Plan must center on ensuring that US made AI 
is trustworthy. In the pages that follow, TRAILS outlines why trust is important to the success 
of US AI. We also recommend several policies that the USG should take to sustain its 
competitiveness in AI.  
All of these recommendations are situated from the position that the U.S. cannot take its 
technological leadership in AI for granted. Policymakers in many other countries are determined 
to nurture their own domestic suppliers of AI because they believe AI is both a “general-purpose 
technology” essential to their nations’ economic growth and a “dual -use technology” essential to 
national security.3 Furthermore, AI scaling laws, the methods and expectations that labs have 
used to increase the capabilities of their models for the last five years, are now showing signs of 
diminishing returns.4 AI developers are devising new strategies and business models to create 
AI. For example, the Chinese company Deep Seek claims to have produced a new AI model relying on less compute, energy, and capital costs than many US or European foundation models . 
Finally, other governments, including China, the UAE, and Saudi Arabia have clear plans and 
funds to nurture and finance AI and their civil and military sectors often collaborate to fund, develop and test AI.
5 
Trust as a Foundation for AI Leadership 
AI can be opaque, complex, and unpredictable.6 To ensure that AI realizes its full potential, 
creators and deployers of AI must first find ways to engender trust or make AI worthy of our 
2 https://www.trails.umd.edu/ 
3 S. Aaronson, AI Nationalism and its Effects, CIGI Paper No. 306, September 2024, 
https://www.cigionline.org/publications/the-age-of-ai-nationalism-and-its-effects/ 
4 https://techcrunch.com/2024/11/20/ai-scaling-laws -are-showing-diminishing-returns-forcing-ai-
labs-to-change-course/ 
5  S. Aaronson, AI Nationalism,; https://www.lawfaremedia.org/article/sovereign-ai-in-a-hybrid-
world--national-strategies-and-policy-responses;  and https://www.brookings.edu/articles/the-
global-ai-race-will -us-innovation-lead-or-lag/ 
6 D. Acemoglu, Harms of AI, NBER Working Paper 29247, 2021, 
https://www.nber.org/papers/w2924 7 


 
 | Page 
 
3 trust.7 That alone is not enough – they must also rigorously evaluate and accurately report when 
AI systems are or are not trustworthy.8 As the Executive Order on Promoting the Use of 
Trustworthy Artificial Intelligence in the Federal Government (December 3, 2020) states: 
“The ongoing adoption and acceptance of AI will depend significantly on public trust. 
Agencies must therefore design, develop, acquire, and use AI in a manner that fosters 
public trust and confidence while protecting privacy, civil rights, civil liberties, and American values, consistent with applicable law.”
9 
Unfortunately, there is mounting evidence that trust in US AI is declining in the US and 
internationally. A 2024 Gallup poll found that some 50% of Americans see more harm than good 
from societal use of AI. Moreover, Americans don’t trust that firms will use AI responsibly. But 
some 57% believe that greater transparency into how firms use AI could reassure them that AI is 
worthy of their trust.10 A 2024 Pew Poll found similar results: 52% of Americans are more 
concerned than excited about AI in daily life. In contrast, only two years earlier in 2022, Pew found that only 38% were more concerned. 
11 In another 2024 survey of global attitudes towards 
AI, IPSOS found that, although more people are using AI, they remain concerned about ensuring AI's responsible development.
12 Distrust in US AI is also rising among policymakers. Some 
governments now believe it could be risky to depend on other countries for access to AI or its 
 
7  S. Afroogh et al. Trust in AI: progress, challenges, and future directions. Humanit Soc Sci 
Commun 11, 1568 (2024). https://doi.org/10.1057/s41599-024-04044-8/ 
 
8 B. Stanton and T. Jensen, Trust and Artificial Intelligence, (Draft)  March 2, 2021, 
https://www.nist.gov/publications/trust-and-artificial-intelligence-draft 
 
9Executive Office of the President,  Promoting the Use of Trustworthy Artificial Intelligence in 
the Federal Government, President Donald J. Trump, Executive Order 13960 of December 3, 2020,  https://www.federalregister.gov/documents/2020/12/08/2020-27065/promoting-the-use-
of-trustworthy-artificial-intelligence-in-the-federal-government  
 
10 https://news.gallup.com/poll/648953/americans-express-real-concerns-artificial-
intelligence.aspx#:~:text=In%20terms%20of%20trust%2C%20in%202024%2C%20about,2024)
%20trust%20businesses%20a%20lot%20or%20some  
 
11 https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-
views-of-artificial-intelligence/ 
 
12  https://www.ipsos.com/en-us/google-ipsos-multi-country-ai-survey-2025. The survey was 
conducted between September 17th- October 8th, 2024, on behalf of Google. For this survey, a sample of 21,043 adults age 18+ were interviewed online in 21 countries. 


 | Page 
 4 components.13 For example, in 2024, Dutch officials expressed concerns that they are creating a 
national security risk by relying on US firms for AI infrastructure.14 Australian officials 
expressed similar concerns.15  
The sections below delineate TRAILS’ recommendations on how the US Government can 
encourage the development of AI that is trustworthy, thereby increasing the chances of 
successful adoption of this technology. We also argue that a well-informed, whole-of-society 
public debate is necessary for capturing the beneficial potential of the technology, while limiting 
the risks associated with it.  
Hence, in the Action plan, the US should: 
Invest in Research to Promote Trustworthy AI  
AI is advancing at an unprecedented pace, but trust remains a critical barrier to its widespread 
adoption. Private-sector AI development often focuses on speed to market, which can leave gaps that degrade trust: in transparency, oversight, and consistent evaluation standards, among others. 
Without robust mechanisms to assess AI’s trustworthiness, the U.S. risks falling behind in 
ensuring that AI systems are safe, reliable, and widely accepted . This, in turn, gives an 
opportunity for foreign alternatives to become more widely adopted. Public investment in AI research can bridge these gaps by supporting rigorous, evidence-based frameworks that ensure 
AI is transparent, accountable, and aligned with human needs. Collaborative efforts—such as the 
partnership between TRAILS and the National Institute for Standards and Technology (NIST) —
are already paving the way for stronger evaluation methods and shared benchmarks. Expanding 
these efforts will not only accelerate AI innovation but also strengthen public trust and global 
13 M Alduhishy, Sovereign AI: What it is, and 6 strategic pillars for achieving it , World 
Economic Forum, April 25, 2024, https://www.weforum.org/stories/2024/04/sovereign-ai-what-
is-ways-states -building/ 
14 Government of the Netherlands. 2024 . The government -wide vision on Generative Al of the 
Netherlands.  The Hague, The Netherlands: Ministry of the Interior and Kingdom Relations. 
January 17. 2024, p. 14,  www.government.nl/documents/parliamentary-
documents/2024/01/17/government-wide-vision-on-generative-ai-of-the-netherlands. 
15 G. Bell, et al, . Rapid Response Information Report: Generative AI: Language models and 
multimodal foundation models. Australia’s Chief Scientist. March 24, 
2023,www.chiefscientist.gov.au/GenerativeAI 


 | Page 
 5 confidence in U.S.-developed AI technologies. To achieve this, the government should prioritize 
research in areas that complement and enhance private-sector efforts rather than duplicating 
them, including: 
Participatory AI  
AI works best when it serves a broad range of users,16 yet foundational AI research often 
proceeds without input from those affected, adversely or otherwise, by its deployment. This lack 
of engagement can limit AI’s usefulness and adoption. To strengthen the global leadership of 
U.S.-developed AI, the government should invest in participatory research methods that involve
businesses, workers, technical experts, consumers, and civil society at every stage of AI design
and deployment. Such investments complement, rather than duplicate, private -sector efforts by
ensuring AI are not only commercially viable but also widely applicable and trusted.17
Strengthening participatory research will reinforce U.S. leadership in AI development whilepromoting innovation and sustainable growth in the private sector.
Technical methods and metrics to  ensure trustworthy AI systems 
Trust in AI must be grounded in rigorous, reliable evaluation. Private companies focus on AI 
innovation and performance, but their evaluation methods are often proprietary and inconsistent, 
making it difficult to assess reliability across the industry. Public investment in standardized 
evaluation, verification, and benchmarking—through initiatives like NIST’s Assessing Risks and 
Impacts of AI (ARIA) program and the AI Safety Institute Consortium—provides independent, 
trusted frameworks that strengthen industry credibility, ensure fair competition, and enhance 
global confidence in U.S.-developed AI. Policymakers should support research that creates us er-
facing technologies to clarify AI behavior, transparency, privacy, and reliability. These investments also promote marketplace stability, ensuring that companies compete on services provided.  
16 F. Delgado et al. The participatory turn in ai design: Theoretical foundations and the current 
state of practice. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, October 2023,  (pp. 1-23). 
17 K. Shilton. Values levers: Building ethics into design. Science, Technology, & Human 
Values, 38(3 2013, pp.), 374-397, https://www.jstor.org/stable/23474474 


 | Page 
 6 Empirical evaluation of AI’s judgments and decisions 
Trust in AI starts with understanding it – how these systems make judgments, why they 
sometimes produce unexpected results, and how users interpret their outputs.18 Currently, many 
AI users lack a clear understanding of how these technologies function, when they may behave 
unpredictably, and how to assess the reliability of their responses.19 This uncertainty can limit 
adoption and confidence in AI-driven solutions. To address this, scientific research should focus on developing methods and tools that clarify AI decision-making, predict when errors are likely 
to occur, and improve evaluation processes. Industry and regulatory bodies also need consistent, 
reliable approaches to assess AI systems against established standards while ensuring that 
oversight does not hinder progress. By systematically analyzing how AI aligns with real -world 
expectations and addressing instances where it replicates flawed human reasoning, the U.S. can strengthen reliability, confidence in AI systems, and long-term global competitiveness.
20 
Investing in structured evaluation methods will help ensure that AI not only advances technologically but does so in a way that is measurable, verifiable, and dependable across a 
range of applications. 
Research- driven AI governance  
Strong AI governance starts with research, not guesswork. Standardized governance frameworks, 
developed through empirical research, can enhance safety, fairness, and interoperability while avoiding unnecessary regulatory burdens that might hinder innovatio n. Private AI companies 
often create internal standards without regulator or expert consultation, leading to fragmented approaches to AI safety, bias mitigation, and transparency. Independent research institutions help 
bridge this gap by developing widely accepted governance frameworks, with organizations like 
NIST, ISO, and IEEE playing a key role in drafting AI guidelines that emphasize transparency 
18D.  Broniatowski Psychological foundations of explainability and interpretability in artificial 
intelligence (Vol. 4, 2021,  p. 00). US Department of Commerce, National Institute of Standards and Technology, https://www.nist.gov/publications/psychological-foundations-explainability-
and-interpretability-artificial-intelligence 
19 S. Edelson et al., How Decision Making Develops: Adolescents, Irrational Adults, and Should 
AI be Trusted with  the Car Keys? Policy Insights from the Behavioral and Brain Sciences , 11(1), 
2024, pp. 11-18, https://psycnet.apa.org/record/2024-67503-002. 
20 E. Tabassi, Artificial Intelligence Risk Management Framework (AI RMF 1.0), January 26, 
2023, https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-
rmf-10 


 
 | Page 
 
7 and accountability. For example, the NIST AI Risk Management Framework (AI RMF) 
exemplifies how research-backed models can guide responsible AI deployment, enabling 
businesses to compete on meaningful advancements like accuracy, explainability, and security. 
To date, TRAILS has completed almost two years of research on trustworthy AI. Our 
recommendations to the AI Action plan build on our findings, specifically:  
Provide I ncentives to AI Firms to E ncourage them to Reveal 
Information about Data Provenance 
The data sets for foundation models are large, diverse and multinational, and are thus difficult to 
govern. But the world must do more to govern these models for two reasons: first, because many of these systems are black boxes, whose developers provide little information about how they 
work; and second, because more and more people rely on these models for information. Policy 
makers should aim to ensure that the data sets that underpin these models are accurate, complete 
and representative. Recent  research indicates that increased dataset transparency can enhance 
interoperability, improve data quality, and encourage responsible data stewardship. Efforts like 
Datasheets for Datasets
21 and the NIST AI RMF 202322 highlight how provenance tracking can 
strengthen AI accountability by enabling authenticity verification and in so doing, mitigating downstream risks. Requiring AI developers to undergo audits by trained outside experts on 
foundation models and data—ensuring provenance, testing methods, and data usage—would 
allow auditors to verify that firms provide complete and accurate information. 
Support International  Research C ollaborations as a Means of 
Building Trust 
According to Statista, the US will remain the largest market for AI. But AI growth markets are 
overseas in middle income and developing countries such as Indonesia and India where the 
 
21 T. Gebru et al. Datasheets for Datasets CACM, December 2021, 
https://arxiv.org/abs/1803.09010  
 
22 E. Tabassi, Artificial Intelligence, and the 2024 Generative AI Risk Management Framework, 
https://www.nist.gov/itl/ai-risk-management-framework 


 | Page 
 8 population is growing.23  For this reason, the US should work with our allies on shared 
approaches to research, data governance, and AI Safety. NSF has developed programs with other 
governments including the Quad Nations (India, Japan, Australia), Israel, and the UK . Such 
shared funding not only yields new research, but can build shared norms, relationships and trust 
between AI stakeholders in these nations.24 
Use the Evidence-B ased Policy Making Act as a Model for 
Developing  AI Policies 
In 2018, President Trump signed the Foundations for Evidence-Based Policymaking Act which 
encouraged collaboration and coordination to advance data and evidence -building functions in 
the USG. The act required that federal agencies appoint evaluation, statis tical, and chief data 
officers and develop an evidence-building plan every four years as well as an annual evaluation 
plan. The USG should build on this legislation in designing the AI Action plan as well as annual evaluation plans. 
 
Encourage Greater Understanding of AI and Greater Participation 
in AI Governance 
The US has long used the Federal Register to solicit public comment on governance, as it is 
doing with this call. But research has shown that the US does not make a broad effort to market such calls from public comment. Consequently, officials often receive only several hundred 
comments, primarily from the technical community or business es. Policymakers are thus missing 
an opportunity to build trust in AI by not using the process to involve a broader cross-section of their constituents. Moreover, discussions with a broader cross-section of their citizens may 
enable officials to anticipate future problems related to AI and even to gain new insights into 
how citizens use AI. Policymakers could incentivize such participation by holding regional 
23 Statista, Artificial Intelligence – Worldwide, https://www.statista.com/outlook/tmo/artificial-
intelligence/worldwide 
24 https://www.nsf.gov/oise/international-collaborations#the-quad-australia-india-japan-and-the-
united-states -6f7 


 
 | Page 
 
9 townhalls and online webinars where they can talk with their constituents.25 More generally, 
policymakers should foster partnerships among academia, industry, and professional 
organizations to create research-supported governance frameworks that clearly define standards for safety, fairness, and effective integration without hindering innovation. 
 
Conclusion: Trust as the Cornerstone of AI 
Leadership  
The future of AI leadership in the United States hinges on building and sustaining trust—among 
researchers, developers, policymakers, and the public. AI systems that are transparent, 
accountable, safe, and aligned with societal norms will not only gain public confidence but will also enhance U.S. innovation, competitiveness, and security in an increasingly AI-driven world. 
We appreciate the opportunity to provide input and look forward to continued collaboration on 
shaping the AI Action Plan. 
Respectfully submitted, 
The Institute for Trustworthy AI in Law and Society (TRAILS) 
  
 This document is approved for public dissemination. The document contains no business-
proprietary or confidential information. Document contents may be reused by the government in developing the AI Action Plan and associated documents without attribution. 
 
25 S. Aaronson,  A Dysfunctional Dialogue About AI-NTIA and the Public on The Risks and 
Benefits of Open Foundation Models December 20, 2024,  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5176397  and S. Aaronson and A. Zable, 
Missing Persons: The Case of National AI Strategies, CIGI Paper No 283,  March 2023, https://www.cigionline.org/publications/missing-persons-the-case-of-national-ai-strategies/ 


