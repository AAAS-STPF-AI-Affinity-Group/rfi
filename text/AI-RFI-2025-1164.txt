PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 12, 2025
Status: 
Tracking No. m 86-g5xq-txeu
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1164
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Adam Scholl  
General Comment
I work on AI alignm ent research, which is unfortunately still a basically-wholly unsolved problem . By which I m ean not just that it is a hard
problem , but one which we still have roughly no idea how to even try to solve.
I started working on this in 2016, when it was unusual to be concerned about the risk of hum an extinction from  AI. Today it is m ore
unusual to find top researchers who are unconcerned about this than the reverse, and m ost prom inent AI researchers have signed public
letters stating that they consider it plausible that unaligned AI m ay soon destroy all life on Earth.
From  m y perspective, it seem s insane that the governm ent does not currently regulate frontier AI m odels as weapons of m ass destruction,
nor even try to prevent their construction and proliferation. Not because currently-available system s are dangerous—they’re still pretty
harm less—but because we have strong reason to suspect they will becom e dangerous som eday, and no particular reason to think that day
won’t be soon.
To whoever m ay be reading this: I im plore you to do what you can to help protect Am ericans (and indeed everyone on Earth) from  this
terrifying threat.


