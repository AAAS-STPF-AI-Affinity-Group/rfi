RFI Response To: Federal Register Notice 2025-02305 “Request for Information on the 
Development of an Artiﬁcial Intelligence (AI) Action Plan” issued 26 February 2025 
Date: March 15, 2025From: Decision-Zone USA, Inc.Contact: Dr. Sarah L. Eaton
This document is approved for public dissemination. The document contains no business-proprietary or conﬁdential information. Document contents may be reused by the government in developing the AI Action Plan and associated documents without attribution.
We appreciate the Ofﬁce of Science and Technology Policy (OSTP) and the National Coordination Ofﬁce (NCO) for seeking input on the AI Action Plan. Our response addresses the national security imperatives of AI policy, focusing on the need for robust, explainable, and resilient AI-driven autonomy in defense and critical infrastructure applications. As AI becomes increasingly embedded in military and strategic systems, ensuring reliable and adversary-resistant decision-making is paramount to maintaining technological superiority and operational security.
Decision-Zone USA is a defense technology startup specializing in mission-critical autonomous systems and real-time decision-making architectures. Our platform, DADA X, pioneers an event-driven computational paradigm that enables deterministic, real-time autonomy without relying on neural networks or statistical inference. By operationalizing causal event patterns, our technology ensures predictability, adaptability, and resilience in high-stakes environments where traditional AI approaches falter.
Decision-Zone USA commends the Defense Innovation Unit (DIU) for its commitment to accelerating the adoption of cutting-edge technology in support of national defense. Under the leadership of Director Doug Beck, DIU has demonstrated remarkable success in bridging the gap between the Department of Defense (DoD) and the commercial sector, awarding 550 prototype contracts totaling $2.5 billion—with 80% of awards going to non-traditional suppliers, two-thirds to small businesses, and one-third to companies engaging with DoD for the ﬁrst time. We 
strongly support DIU’s ongoing efforts to increase budget ﬂexibility and enhance transparency with Congress, ensuring that the risks and rewards of innovation are fully understood. 
As a company focused on mission-critical autonomous systems, we fully endorse policies that enable faster, more adaptive acquisition strategies to strengthen national security. DIU’s approach aligns with our belief that advancing new computing paradigms requires procurement processes that reward innovation, adaptability, and operational effectiveness in contested environments.
1


Current AI approaches, dominated by deep learning and statistical methods, introduce inherent 
risks in mission-critical settings, including lack of causal reasoning, vulnerability to adversarial manipulation, and dependence on large-scale data retraining. Our response advocates for alternative computing paradigms such as event-driven causal autonomy that address these shortcomings and provide real-time, deterministic control for autonomous operations in contested environments.
We need every autonomous system in the DoD to execute under real-world combat stress.  The recent advancements in AI have led to widespread optimism about its potential, but also a tendency to overestimate its current reliability, especially in defense applications. While AI as an informational and recommendation tool has demonstrated value in data analysis, logistics, and operational planning, its direct application to mission-critical autonomy requires a more rigorous approach than what is currently assumed in many commercial AI narratives.
DoD leadership should already have concerns that AI-based mission autonomy isn’t living up to expectations. They have seen failures in open-world testing conditions that erode trust.  And they must surely suspect that RL-trained drones, multi-agent orchestration, and AI-powered cyber defenses aren’t advanced or reliable enough in contested environments. But no one is saying it out loud. We welcome the opportunity to shed light on what they have already seen happening, because leadership needs new language to challenge current assumptions. The window is open right now to deﬁne the future of autonomy before AI’s weaknesses become catastrophic failures.
In this document, we propose speciﬁc policy actions to advance AI capabilities in the defense sector, including funding for novel AI methodologies, resilience frameworks for autonomous systems, and procurement incentives for AI that enhances mission assurance. Our recommendations aim to ensure that the United States maintains technological dominance in mission autonomy while safeguarding national security interests.
2


Ensuring Intelligent Systems Meet the Standards for Mission-Critical 
Autonomy
The DoD must ensure that warﬁghters and national security operations are not placed at risk due 
to overly optimistic assumptions about AI capabilities. Mission leaders can’t trust an AI model that makes unpredictable decisions. 
AI-driven autonomy has gained traction in recent years, but its fundamental limitations remain largely unaddressed. Today’s AI relies on statistical inference, optimizing for correlations rather than true understanding. Even more importantly, it cannot capture human intent. This leads to a host of challenges, including unpredictable decision-making, lack of transparency, and an inability to adapt to dynamic real-world conditions. As industries increasingly integrate AI into mission-critical operations, the risks of misalignment, inef ﬁciency, and loss of control become 
more evident. Without a paradigm shift, AI based autonomy will remain brittle, reactive, and misaligned with human-deﬁned goals. Existing autonomy solutions lack true intent-driven adaptability. The race for autonomy is not about ef ﬁciency, it’s about control. Governments and 
corporations deploying AI-based autonomy are betting on a fragile, easily compromised system.
AI systems built on statistical inference and deep learning face foundational constraints that must be addressed before they can be trusted for real-world autonomous decision-making and action in adversarial environments:
• Lack of Causal Understanding – AI models recognize patterns rather than understanding causality, which makes them unreliable for real-time, high-consequence decisions.
• Adversarial Vulnerabilities – AI systems trained on vast datasets are susceptible to spooﬁng, data poisoning, and deception, creating risks in contested domains.
• Operational Fragility – Many AI models require continuous retraining, curated data pipelines, and extensive compute resources, which are not feasible at the edge in high-tempo, denied environments.
Decision-makers are asking: "Will this win battles, integrate easily, and scale fast?” But at current levels AI-based autonomy will fail in novel and contested environments.  Battle ﬁelds 
don’t follow training data.  Compute and bandwidth heavy, AI-based autonomy is impractical for deployed systems and requires constant retraining and human oversight.  Our view is that current AI-based Autonomy is a National Security risk.
Autonomous warfare is going to create an even more dynamic threat environment.  Existing autonomy solutions lack true intent-driven adaptability. Autonomous defense systems and platforms must be designed to comprehend commander's intent and demonstrate the ﬂexibility to 
adapt as both intent and mission requirements continually evolve.
To maintain technological superiority in autonomy, the Department of Defense must ensure that autonomy solutions are reliable, secure, and adaptable before being integrated into mission-critical systems. The next era of military-grade autonomy will not be built on probability and 
3


training data. As Craig Martell, the former chief digital and AI of ﬁcer at DoD once stated, we 
need ﬁve nines of correctness, not hallucinations. The need for deterministic, auditable, and 
intent-driven autonomy is urgent.  Rather than assuming commercial AI will inevitably “get there,” policymakers should prioritize alternative computational paradigms that provide deterministic causal autonomy with real-time adaptability. 
Causality is the key to unlocking the full potential of autonomous systems. Without the ability to understand and manipulate cause-and-effect relationships, systems, devices and applications will remain conﬁned to the realm of correlation and association, unable to truly grasp the dynamics of the world they operate in or execute our intents. Without causality, there is no full autonomy, only more advanced automation. 
In our view, true autonomous systems must embed human intent at their core and operate in real time by interpreting and acting on causal events as they occur. This requires a decentralized framework where autonomous agents coordinate dynamically rather than being tethered to a central, monolithic model. Such capability is central to the Mosaic warfare vision as set forth by DARPA. Event-driven autonomous systems reduce computational burden by eliminating the need to handle massive datasets in real time. We need event-driven causal agents that operate directly on sensor events. Event-driven causal AI provides a foundation upon which real-world, adaptive, and aligned autonomy can be built.  While many are betting on “bigger AI” as the next frontier, we assert that the future lies in building systems that can not only predict but actively shape outcomes by harnessing real-time, event-driven causality. 
Our call is to rethink intelligence from ﬁrst principles—causality over correlation, intent over 
prediction, decentralization over monoliths. This challenges the core assumptions of today's AI and paves the way for an alternative methodology for autonomy. Our skepticism about scaling as the sole path forward aligns with debates in the AI community about diminishing returns and the need for new approaches.  
Recommended Policy Actions for the OSTP AI Action Plan
1. Establish Standards for Deterministic Computing in Critical Systems
Policy Action: Create certiﬁcation standards for autonomous systems in critical infrastructure that require deterministic decision-making rather than just statistical con ﬁdence levels.
The shift from deterministic to probabilistic AI systems creates fundamental reliability challenges in critical infrastructure. While modern AI excels at pattern recognition and prediction, its statistical nature introduces inherent uncertainty that can be problematic in high-stakes environments.
Critical infrastructure systems—like power grids, transportation networks, medical devices, and 
defense systems—require absolute predictability and accountability. When failures occur in these domains, the consequences can be catastrophic, including loss of life.
4


Deterministic computing provides crucial advantages in auditability, reliability guarantees, safety 
veriﬁcation and accountability.  This policy would create a two-tier approach: probabilistic AI for areas where statistical conﬁdence is sufﬁcient, and deterministic computing for high assurance systems where absolute reliability is necessary. This balanced framework would enable innovation while protecting critical infrastructure from unpredictable AI behavior. The standard could include requirements for transparency in decision logic for systems deployed in crucial sectors.
2. Develop Causal Transparency Requirements 
To unlock the full potential of autonomous systems in defense and regulated industries, we must 
prioritize causal understanding over mere correlation, fostering greater transparency and trust.  
Policy Action: Require that autonomous systems in regulated industries provide causal explanations for decisions, not just statistical correlations.  
Our platform’s ability to track causal chains through its event-driven model shows that a new 
level of decision transparency is achievable. This approach addresses the common challenge in traditional AI systems, which often provide probabilistic outputs without clear cause-and-effect reasoning. By mandating causal transparency, policymakers can ensure that AI systems in domains like ﬁnance, healthcare, and law enforcement meet accountability standards while reducing the risk of black-box decision-making errors. This creates more robust, trustworthy systems that align better with how human experts make and justify decisions. This policy would signiﬁcantly improve AI governance while encouraging technological innovation in more transparent, understandable AI architectures.
The United States must maintain decisive technological superiority in autonomous systems. Causal transparency would confer a strategic advantage. In defense contexts, causal transparency is essential for several reasons:
1. Command accountability: Military command structures require clear chains of accountability. Ofﬁcers authorizing autonomous system deployment must understand not just what decisions might be made, but why they would be made.
2. Rules of engagement compliance: Autonomous systems must demonstrably follow established rules of engagement and international humanitarian law. Understanding causal reasoning is crucial to verify this compliance.
3. Strategic predictability: Military planners need to understand how autonomous systems will behave across various scenarios to integrate them effectively into broader operations.
4. Strategic stability: In interactions with potential adversaries, unpredictable AI behavior could lead to escalation. Causal transparency helps prevent misinterpretation of autonomous system actions.
The Department of Defense already has established principles for responsible AI use that emphasize accountability and appropriate human judgment. Causal transparency requirements would strengthen these existing frameworks rather than creating entirely new regulatory burdens.
5


3. Support Alternative Computing Paradigms
Policy Action: Earmark R&D funding specifically for non-traditional approaches to autonomy 
that don't rely on neural networks or statistical methods.  
While neural networks have driven much of AI's recent progress, they introduce vulnerabilities 
such as adversarial attacks and overﬁtting. Investing in alternative computing paradigms, such as event-driven, causal-based approaches will diversify the nation's AI toolkit. This diversi ﬁcation 
enhances national technological resilience and reduces over-reliance on methods prone to manipulation or failure in uncertain conditions.
4. Create a Resilience Framework for Autonomous Systems
Policy Action: Establish requirements for autonomous systems to function properly in 
adversarial environments and novel situations without retraining.  
The capability to adapt in real time without requiring retraining represents a major security advantage. This resilience mitigates the risk of AI system failures when faced with unexpected conditions, ensuring continuity in critical systems. A resilience framework would set industry benchmarks for adaptive decision-making systems that align with national security priorities.  
5. Prioritize Legacy System Integration and Platform-Agnostic Autonomy
Policy Action: Create procurement preferences for autonomous technologies that can integrate with existing infrastructure rather than requiring complete replacement.  
The  ability to interface with legacy systems through event-driven mechanisms enables cost-effective modernization strategies. By avoiding costly infrastructure overhauls, this approach accelerates AI adoption in industries that rely on established systems, including defense, utilities, and transportation. 
Unlike solutions that require new hardware purchases, a software defined approach should reward companies whose product is designed to integrate into legacy DoD platforms, enhancing existing assets without costly acquisitions and extending the lifecycle of existing assets.  
Current strategy among defense suppliers is to bundle autonomy with proprietary hardware, but DoD must be able to retain control over its platforms while leveraging cutting-edge autonomy capabilities. Seek software that can upgrade and enhance what warfighters already use rather than being forced into new purchases.  
6. Develop Real-Time Response Requirements
Policy action: Set industry-specific latency standards for critical autonomous decisions. 
6


Many AI-driven systems rely on deep learning models that cannot guarantee bounded response 
times due to variable computational loads, dependency on large datasets, and network constraints. This unpredictability is unacceptable in high-stakes operations such as missile defense, electronic warfare, and autonomous combat systems.  
 We should be more concerned about data dependency and latency risks. Many AI architectures depend on centralized data processing and cloud-based analytics, creating potential bottlenecks and vulnerabilities in denied, degraded, intermittent, and limited (DDIL) environments. Establish latency standards that ensure real-time autonomy at the tactical edge, rather than reliance on cloud-based, high-latency architectures.  We also recommend procurement preferences for real-time autonomous systems:
• Incentivize DoD and federal agencies to prioritize low-latency, event-driven architectures that support decision-making at machine-speed.
• Promote real-time computing paradigms that can operate on local processing units rather than requiring continuous cloud connectivity.
This will ensures military readiness, enabling autonomous defense systems to respond in real-time to evolving threats without delays introduced by traditional AI inference pipelines. It also prevents overreliance on cloud-based AI models that introduce latency and potential failure points. Ultimately this will position the U.S. to lead in real-time autonomous systems, ensuring that critical AI-driven decisions occur at machine-speed with guaranteed precision.
7. Support Decentralized Autonomy Research 
Policy action: Fund research into autonomous systems that can operate effectively without 
central authority, yet adhere to specified human intentions.  
Decentralized autonomy allows autonomous agents, whether drones, vehicles, or unmanned 
naval systems, to operate independently yet cohesively, ensuring mission success without requiring constant oversight from a central authority. However, current AI research has largely overlooked event-driven, causal autonomy architectures that can enable real-time, intent-driven coordination among autonomous agents. A decentralized model ensures redundancy, allowing autonomous units to continue functioning even when parts of the system are compromised. Autonomous systems must be capable of acting independently while remaining fully aligned with commander or operator intent. Research should focus on methodologies that enable autonomous systems to make decisions based on explicit intent rather than inferred statistical probabilities.
By adopting these policies, the United States can create an AI innovation environment that 
prioritizes reliability, accountability, and security. We strongly recommend that OSTP integrate these recommendations into its AI Action Plan to ensure continued leadership in advanced AI technologies.
7


8


