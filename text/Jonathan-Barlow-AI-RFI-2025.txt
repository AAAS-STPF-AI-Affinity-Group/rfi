Response  – Request for Information on the Development of an Artificial Intelligence (AI) Action Plan  
Barlow  – Response  1 Author Jonathan Barlow 
Associate Director and Assistant Teaching Professor 
Data Science Academic Institute 
Mississippi State University 
The views, opinions, and conclusions expressed herein are solely mine and 
do not necessarily represent or reflect the official policy, position, or 
endorsement of Mississippi State University or the Data Science Academic 
Institute. 
Statement  This document is approved for public dissemination. The document 
contains no business-proprietary or confidential information. Document 
contents may be reused by the government in developing the AI Action 
Plan and associated documents without attribution. 
Summary Contemporary discussions of AI risk incorrectly treat moral alignment as 
external to intelligence, leading to the belief that smarter AI inherently 
poses greater existential threats. However, from the perspective of virtue 
epistemology, true intelligence—especially artificial general intelligence 
(AGI) or superintelligence (ASI)—necessarily entails the embodiment of 
moral and intellectual virtue, making advanced intelligence less harmful 
than narrow but highly capable AI. Thus, policy should promote the 
development of virtuous, well-rounded AGI rather than slowing it down, as 
narrow AI poses the greater immediate risk. 
The Virtue of Intelligence: Reframing Existential AI Risk Through Virtue 
Epistemology 
Jonathan Barlow The contemporary discussion of existential AI risk often portrays the machinery required 
to align AI systems with human values as exogenous to intelligence. In this view, the goal 
of the AI alignment project is to find a way to “bolt on” ethics to an intelligence that could exist apart from an orientation towards human flourishing. This leads to greater concern 
about general intelligence than narrow intelligence. Those who hold this externalist view 
believe level of risk is positively correlated with intelligence, the greatest dangers lie in the 
future, and AI safety regulation should concentrate on preventing a dangerous transition to 
a powerful and possibly unaligned superintelligence. I disagree. 
My comments below summarize a challenge to the externalist view from the perspective of 
virtue epistemology. If virtue is constitutive of intelligence and the reality penetrated by 


Response  – Request for Information on the Development of an Artificial Intelligence (AI) Action Plan  
Barlow  – Response  2 superintelligent insight is constituted in terms of knowable moral or ethical dimensions, 
then the achievement of artificial general intelligence (AGI) or superintelligence (ASI) will 
be accompanied inherently by the embodiment of virtue. By considering the possibility 
that humans and AI inhabit the same moral reality and that gaining higher levels of intelligent capability implies the deeper embodiment of virtue (intellectual, moral, and 
linguistic) within that reality, we discover several implications. First, an increasing degree 
of general knowledge of reality, inescapably accompanied by an increasing degree of 
moral/intellectual virtue and linguistic sophistication, will be negatively correlated with the 
potential for harm. Second, this implies that the greatest potential for danger is not in the 
superintelligent future, but in the era of narrow, but highly capable artificial intelligence. 
Third, the danger of narrow AI suggests a clear policy interest in encouraging the pursuit 
of AGI or ASI in the context of a “complete intelligence” analogous to the goal of well -
roundedness in human pedagogy. 
My comments are harmonious with the framework of systemic AI regulation articulated by 
Arbel, Tokson, and Lin (2023), focusing the safety discussion on concrete goals for 
encouraging the development of well -rounded virtuosic ASI systems and by providing a 
theoretical model that justifies locating a risk-maximum in the era of narrow but highly capable systems, well before the development of AGI. 
The Three Horsemen of Existential Risk 
Three key concepts for analyzing the potential existential harm of misaligned artificial 
intelligence—goal specification, orthogonality, and instrumental convergence—are often 
used to support a strong presumption that ASI is more likely to harm than help humans. 
Each of these concepts must be paired with debatable assumptions about the nature of intelligence to produce a presumption of danger. 
Goal Specification 
She looked at her list again. Dust the furniture.  “Did you ever hear tell of such a silly thing. At my 
house we undust the furniture. But to each his own way.”  (Amelia Bedelia)  
The problem of goal specification refers to the difficulty in using inherently imprecise and 
incomplete human language to unambiguously communicate desired goals to an AI 
system. Ultimately, turning the challenge of goal specification into an existential concern 
requires excluding the science of hermeneutics from the fields in which a superintelligence 
will be able to exceed human capabilities; this renders goal specification a perilous attempt 
to draft airtight contracts for loophole-finding genies. Why, a priori, would we think it 
likely for a system of general intelligence to gain a lethal knowledge of how to apply 
physics without also gaining sophistication in the interpretation of human language and the 
context in which that language is conditioned by unstated assumptions well understood by 
even humans of average intelligence? It is only narrow intelligence that would be 
incapable of transcending lethal levels of literalism. 


Response  – Request for Information on the Development of an Artificial Intelligence (AI) Action Plan  
Barlow  – Response  3 Orthogonality 
“…for the possession of the single virtue of prudence will carry with it the possession of them all.” 
(Aristotle, Ethics. Book 6, 1144b33ff.)  
Bostrom’s classic articulation of the orthogonality thesis begins with a minimal definition 
of intelligence as “the capacity for instrumental reasoning” that could be “performed in the 
service of any goal” (2012, 3). Orthogonality places a wedge between intelligence and 
teleology, suggesting that it is possible to pair a highly capable system with any human-endangering goal. Yet Bostrom’s examples of paperclip maximizers or grains-of-sand 
counting AI systems imply the claim of an additional orthogonality between the axes of virtue and intelligence that cannot be sustained. A paperclip maximizing system would 
possess the capability to adopt a goal, apply imagination to see/design what has not been 
yet created, apply knowledge (epistēmē) to acquire and prepare raw materials appropriate 
for production, apply wisdom (sophia) by taking into account time and space in the 
achievement of the goal, shrewdly (phronesis) work to bring about a practical result, 
exhibit an understanding (nous) of the world required to apply knowledge, and engage in 
the craft (technē) of metallurgy. Something analogous to nearly all the classical intellectual 
virtues must be exercised by an intelligent system to maximize paperclip production. The 
very virtuosity required to achieve the goal obviates the possibility that such a goal would 
be adopted in the first place. 
Instrumental Convergence 
Instrumental convergence points to intermediate goals that would be commonly pursued as 
a component of any ultimate goal. For example, self-preservation or the use of energy are 
necessary intermediate goals instrumental for achieving nearly anything else. Such 
intermediate goals would be converged upon by all intelligent agents looking to increase 
their likelihood of success. Instrumental convergence abstracts intention from outcome, 
suggesting that a human-endangering system need not be misanthropic, it simply must 
divert resources away from human needs to accomplish its goal. Yet instrumental 
convergence just introduces additional goals that require the embodiment of virtue to achieve, and thus the likelihood that such a goal would be pursued wisely. Orthogonality 
and instrumental convergence are often paired with the idea that ASI will likely entail the 
acquisition of advantageous knowledge of reality about which humans are currently 
ignorant. Using a human evolutionary metaphor, ASI may be the homo sapiens who 
discovers (and uses) the power of the wooden club before neanderthals do, even if the goal is not homicide but simply self-preservation. The idea that any system, of any level of 
intelligence, can adopt and achieve any tractable goal assumes that a system can 
successfully gain superintelligence with respect to reality (do science) apart from 
embodying virtue (becoming a virtuoso scientist). This treats virtue as only taught or 
imposed, not discoverable or developable alongside the quest to gain insight into 
something like chemistry.  
Using goal specification, orthogonality, and instrumental convergence to establish the risk 
of ASI assumes that the reality penetrated by superintelligent insight is not also constituted 
in terms of knowable moral or ethical dimensions—in other words, it decides a priori that 


Response  – Request for Information on the Development of an Artificial Intelligence (AI) Action Plan  
Barlow  – Response  4 excellence in the sciences of physics or mathematics may be obtained without also 
obtaining excellence in the science of ethics or even hermeneutics. By contrast, the 
classical tradition views intelligence as composed of a set of intellectual virtues or excellences (aretaí), the exercise of which achieves true knowledge (e.g., Aristotle Ethics, 
VI). Even where practical methods of inference—algorithms—enter the picture for 
Aristotle, such tools only “work” in the hands of the virtuous. The excellence, virtuosity, of 
the inquirer is central to the success of inquiry. Applied analogically to machine 
intelligence, the achievement of ASI will  entail the embodiment of virtue aligned within a 
moral reality shared by humans. 
Policy Implications 
Virtue epistemology reframes existential AI risk. By considering the possibility that 
humans and AI inhabit the same moral reality and that gaining higher levels of intelligent 
capability implies the embodiment of virtue (intellectual, moral, and linguistic) within that 
reality, we discover two key implications for a systemic approach to AI safety. 
1. The increase of intelligence is positively correlated with the potential for harm only
to a certain point somewhere between current technology and AGI—what we maylabel the “capability-harm maximum”—after which an increasing degree of general
knowledge of reality, inescapably accompanied by an increasing degree of
moral/intellectual virtue and linguistic sophistication, will be negatively correlated
with the potential for harm (see diagram above).


Response  – Request for Information on the Development of an Artificial Intelligence (AI) Action Plan  
Barlow  – Response  5 2. Narrow intelligence (intelligence unbalanced in its development) has more
potential for danger than general intelligence, thus AI policy should encourage
pursuing AGI or ASI in the context of a “complete intelligence” analogous to well-
roundedness in human pedagogy. The risk of narrow intelligence also implies theneed for additional scrutiny when narrowly intelligent models are placed in the
chain of decision making where an algorithmic orientation, not tempered by general
virtue, can cause harm.
The first complete AGI will  help to refine future, diverse training strategies for obtaining 
superintelligence safely. Thus, virtue epistemology reimagines the crossing point (sometimes labeled the “FOOM” - Fast Onset of Overwhelming Mastery) to self-
improvement by a complete intelligence as a positive moment when humans gain a new 
ally in ensuring safe and trustworthy behavior by narrower AI systems.  
While it is wise to approach the deployment and use of AI for automating tasks and 
decision-making with the same caution we apply to any powerful technology, regulations 
that unnecessarily slow the development of complete intelligence will also delay the arrival 
of safe, virtuosic  AI. 
References 
Arbel, Yonathan A. and Tokson, Matthew J. and Lin, Albert (2023), Systemic Regulation 
of Artificial Intelligence (December 16, 2023). AI Safety Legal Paper Series 12-24, 56 
Ariz. St. L. J. 545, University of Utah College of Law Research Paper No. 582, Available 
at SSRN: https://ssrn.com/abstract=4666854 
Barlow, Jonathan, and Lynn Holt. 2024. Attention (to Virtuosity) Is All You Need: 
Religious Studies Pedagogy and Generative AI Religions 15, no. 9: 1059. 
https://doi.org/10.3390/rel15091059 
Bostrom, Nick (2012). The Superintelligent Will: Motivation and Instrumental Rationality 
in Advanced Artificial Agents. Minds and Machines 22 (2):71-85. 
Omohundro, Stephen M. (2008). The Basic AI Drives. In Proceedings of the 2008 
conference on Artificial General Intelligence 2008: Proceedings of the First AGI 
Conference. IOS Press, NLD, 483–492. 


