The way to LEAD (Lasting and Effective AI Dominance) 
To whom it may concern: 
The United States is poised to become the undisputed global leader in AI, but only if it 
pivots away from the past AI strategy. Like the industrial revolution, the AI revolution can 
redraw the balance of power in the international order. Europe's regulatory environment 
is stifling their ability to compete while China’s authoritarian system is accelerating their 
path to strong intelligence. Recent Chinese releases of competitive large language 
models underscore the urgent timeline required to outpace our geopolitical adversaries 
in achieving lasting global dominance in AI.  We believe actions underpinning U.S. global dominance in AI manifest as: 
1.Global Adoption: American technology leads AI adoption at home and abroad
To Do: Build trust in American-led technology by leading in AI standards, 
and tools to validate AI products against these standards. 
2.Strongest Intelligence: American AI outperforms competitors in all industries.
To Do: Maximize the availability of high quality data to AI systems. 
Mitigate hallucination through data citation and data provenance. 
3.Abundant Data: the American data economy is the world’s leading marketplace.
To Do: Create incentives for investment in and sharing of data and 
information. 
4.National Security: U.S. AI policy mitigates risks and maximises innovation edge
To Do: Achieve resilience to an AI-facilitated attack by limiting system 
centralization and implementing other elements of the AI fire code.  
5.Command of Talent: The U.S. dominates access to compute power and talent
To Do: Investment in AI innovation centers will draw AI talent to our 
shores. 
Our initiatives: 
●AI Innovation Centers: The Æthos Foundation has created the largest AI Start
up hub in Boston and is beginning to expand its model around the U.S.
●Smart Regulation: Æthos tracks products’ alignment with data and AI best
practices to help developers build trust to access proprietary sensitive data.
●Data Loops: Data.Flowers creates intelligence amplification technology for
tracking media and public health.


Who we are: 
●Sam Rowe: Æthos Founder & CEO, Sam recently consulted for the
Massachusetts governor on AI innovation policy and studied Science Technology
and International Affairs at Georgetown School of Foreign Service.
●Aleks Jakulin: Data.Flowers Founder & CEO, Æthos advisor, Aleks received the
EurAI award for the best PhD dissertation in artificial intelligence in Europe, and
coauthored standards like PNG and JPEG used by billions.
A) Investment in Data is needed for the Next Wave of AI
Ilya Sutskever, the cofounder and former chief scientist of OpenAI, stated that what's 
been the driver for all the progress we see today, including OpenAI's GPT-3 -- have 
been extraordinarily large neural networks trained on a huge dataset: the internet. While 
the computing power has been growing, data itself has not been growing. He added that 
data is the fossil fuel of AI. 
https://www.appblit.com/scribe?v=1yvBqasHLZs&t=475&g=5DOebERzwDMcSHkSGlRs
fqdfunG2  
Right now, the incentive to invest in data is limited. Most of the value of data is captured 
by the services that harvest it. The price for data is falling. Buyers of data compete with 
data collectors and curators. There has been virtually no new successful entrants into 
the data ecosystem. There have been several examples of past failure to protect the 
data ecosystems: 


1. Yahoo curated and categorized websites. Google's search leveraged the curation by
Yahoo. Once the market for search switched to Google, Yahoo's service was less
desirable, and the data was no longer updated. As a result, Google search results have
been deteriorating. Google has been compensating by utilizing the search behavior of
its users to improve its own search results, but this data was not usable by researchers
or other competitors, leading to overall deterioration not just of search but of the internet
as a whole.


2. Media companies have been creating content that then fueled the rise of social
media. Social media platforms initially routed traffic back to media companies, but with
time this relationship has been eroding, with decades of falling media sector revenue.
The quality of media has deteriorated, and new "influencer" media operations have
resulted in more political polarization and less accurate information in general. With the
adoption of AI-powered tools, this negative trend is likely going to accelerate.
3. The AI systems used for software development have been trained on communities
like Stack Overflow. With the rise of AI-powered tools, the participation in Stack
Overflow has been declining, leading to a decrease in the quality of the data used to
train AI systems. Similar patterns of deterioration have been identified for other valuable
publishers and communities, such as WebMD for medical information. As with Google in
the past, the interaction data is held within individual programming tools, leading to a
winner-take-all model.


A further challenge is hallucination, which is the ability of AI systems to generate 
content that is not based on any real-world data. There is a clear solution: AI systems 
should generate output that cites underlying data. Data citation solves two problems: it 
provides an accounting basis for the data licensing business model. Furthermore, it 
addresses the AI hallucination problem, by ensuring that AI systems are transparent 
about their sources of information - and giving a foundation for AI explainability. 
Policy recommendation: Establish a data commons office that ensures that investing 
in data yields solid returns. Ensure that the contributions to the world data economy are 
balanced instead of some countries free-riding on the benefits of data commons. 
Outcome: This will lead to the second wave of innovation in artificial intelligence 
resulting from further public, private, entrepreneurial, and international investment into 
data commons.  
B) The AI Risks are Best Handled with Fire Codes
AI technology can be considered as an offensive weapon. Our information technology is 
not yet fully robust against AI-powered attacks. It’s a matter of national security to proof 
our nation against them. 
There have been two views of AI technology. One is that AI is a complex centralized 
technology like nuclear, where it’s possible to control dissemination. The other is that AI 
is a decentralized technology which will rapidly spread around the world, like fire - and 
where rather than attempting to control the technology one should control the resilience 
towards technology. For example, we control fire through fire codes and fire 
departments, because it’s too easy to create fire.  


 
AI has been seen as a centralizing technology – where the data and the knowledge are 
assembled, to use Sam Altman, OpenAI founder’s words, in a “skyscraper full of 
computers”. Such a centralized system is highly vulnerable to a takeover in an 
adversarial environment, and the narrative over the past decade has been that such 
systems should be closely affiliated with governments, in many ways extensions of 
political leadership with strong ideological affinities.  
 
With the recent developments around the world including, for example, DeepSeek, it’s 
becoming clear that AI is not a centralizing technology. It’s instead better viewed as 
something like fire: there are people all around the world who can use large language 
models, for either good or bad. It can’t be put back into the bottle and controlled. 
Instead, we should think about how the world adapted to other similar innovations, like 
fire, metal, or plastics.  
 
Perhaps the best way to view AI risks is through fire codes. While explosive devices and 
highly combustible materials can to some extent be controlled, like high performance 
GPUs – enough damage can be created by uncontrollable off-the-shelf computing 
devices. This is not new: over the past couple of decades, we’ve seen increasing levels 
of spam, cybersecurity risks, impersonation, and social engineering attacks. 
 
There are two fundamental threats exacerbated by AI technology: ● Take-over : a compromised individual or small team amplified by AI would be 
able to move rapidly through systems, and centralized systems with no or few 
humans in the loop could be taken over almost immediately. For example, a large 


data center that’s internally unencrypted or encrypted with a small number of 
master keys could be monitored by an external party without detection - and 
when the time is right, taken over at once. An operating system could be 
remotely updated, breaking a considerable percentage of all computers running 
that operating system. ●Impersonation: AI technology advances from the last few years are now
breaking authentication and proofs-of-humanity. A compromised operating
system that deploys agents to personal and company computers could in turn
compromise anything that requires mere keystrokes or digital signatures.
Individuals and companies do care about cybersecurity, but not at the level of being 
resilient to this type of an attack. It’s the government’s job to ensure ecosystem-level 
resilience by regulating the minimum security of the devices in the market. To some 
extent, this has been done by limiting the market share of certain manufacturers, but we 
recommend a more structured approach. 
Even at a smaller level, a free market requires companies and individuals to retain 
some privacy. In the absence of privacy and competitive advantage by an innovator is 
quickly lost to the economies of scale of large competitors. As a result, if we want to 
retain the value of entrepreneurial dynamism, it is important to protect the privacy of 
small enterprises from larger platform operators. 
Such an approach would have five prongs: 
Decentralization: the same way that the US Constitution separated the governance 
between multiple branches of the government, we need to ensure that our technological 
dependencies aren’t excessively centralized. Vertical integrations between hardware, 
operating systems, applications software, data, and commerce are conflicting with an 
efficient market economy open to entrepreneurship. We need decoupling and we need 
to assume that any organization can be taken over by a hostile adversary. Modularization : the complexity and entanglement of software is unacceptably 
modularized. The same computer runs both banking software as well as downloaded 
computer games. The same software environment powers both encryption as well as 
software development. Most software has levels of complexity that make it almost 
impossible to verify the absence of security holes. We need much simpler devices and 
clearer isolation principles that allow rapid innovation of software but isolate high 
security operations. The ability of any device to spy on the customer, to update its 
software, or to facilitate other forms of external control is unacceptable, and we need 


standards that introduce friction to such situations, especially for devices that are no 
longer rapidly evolving. 
Hard Decentralized Authentication: Users are going to be activating and authorizing 
AI agents to act on their behalf. This will be both a boon to productivity, but also a 
massive opening for attacks. The solution lies in clearly separating what can be 
delegated to a computer – and that which requires an in-person signoff. Hard 
authentication through a new type of device is how the rate of spread of an attack is 
limited. Game-Theoretic Mechanism Design: Instead of relying on centralized systems to 
detect spam or AI-generated disinformation, we should instead seek to develop market 
incentives that ensure that bad actors can’t be better off than good actors. Instead of 
relying on centralized services that detect spam, we should instead encourage the 
development of microescrow systems that require senders of email to hold funds in 
reserve in case the customers decide that messages are spam. Instead of trying to 
detect if information is AI-generated, we should encourage information provenance 
where reporters and photographers are cited by the content, along with the whole chain 
of modifications to the original time-stamped information.  Market Efficiency instead of Market Control: It’s not necessary to exert market 
control to control the market. Instead of trying to regulate the products and prices, a 
marketplace can simply provide accurate scales that lets buyers and sellers measure 
the weight of the sold products without disagreements. Instead of trying to control 
information sources, a government can simply provide data about the information 
sources, letting the market participants act on it. Instead of trying to censor 
disinformation bots, a government can provide data about information sources. Instead 
of trying to police information, a government could simply allow for information retraction 
notices to be propagated. Policy recommendations: 
1. Create a red teaming group systematically identifying attacks on companies,
individuals and state actors. When a potential exploit is found, the underlying system
needs to be broken down and decoupled, fixing the exploit is not enough.
2. Drive for development of modular architectures that will contain an exploit within a
particular module without contagion.
3. Develop next generation human-speed authentication systems for the US military and
government needs.


4. Evaluate incentive systems and propose adjustments when they’re vulnerable to
AI-enabled adversarial actors.
5. Establish data loops that evaluate the exposure of the US citizens to adversarial
influence information operations with a budget that’s proportional to adversaries’
investment in information operations (~$1-10B annual).
Outcome: U.S. becomes resilient to AI-powered attacks within 2 years. 
This document is approved for public dissemination. The document 
contains no business-proprietary or confidential information. 
Document contents may be reused by the government in developing 
the AI Action Plan and associated documents without attribution. 


