PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8b-2jbp-zfh2
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-8655
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  Encode
General Comment
See attached file(s)
Attachments
Encode OSTP RFI AI Action Plan Com m ent


March 14, 2025 
Via Electronic Submission  
Faisal D’Souza, NCO 
Oﬃce of Science and Technology Policy 
Executive O ﬃce of the President 
2415 Eisenhower Avenue 
Alexandria, VA 22314 
Re: Request for Information on the Development of an Arti ﬁcial Intelligence (AI) Action Plan
Encode is an advocacy organization dedicated to advancing safe and responsible arti ﬁcial 
intelligence. This document is approved for public dissemination. The document contains no 
business-proprietary or con ﬁdential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without attribution. 
1. Advancing Science and Innovation
1.1 AI Reliability Research at DARPA 
The Department of Defense (DOD) faces signi ﬁcant challenges integrating advanced AI systems 
due to the limited reliability, explainability, and security of current technologies, which are often 
factors in high-stakes military decisions. To address these issues, DARPA is positioned to lead 
research in rigorous testing against adversarial threats, improved model interpretability, and 
secure hardware design. Investing in these areas will enable the DOD to assess AI systems 
effectively, ensuring their reliability before deployment in mission-critical situations. 
Consequently, increased investment in AI reliability research should be prioritized to maintain 
U.S. strategic advantage in national security. 1.2 Establish and Fund the National AI Research Resource (NAIRR) 
Research at American universities laid the groundwork for modern arti ﬁcial intelligence. It was 
professors at Stanford and Carnegie Mellon University that developed the algorithms for neural 
networks and deep learning that created the foundation for today’s advances in artiﬁcial 
1 


intelligence.1 In the 1960s and 1970s researchers at UC Berkeley and Stanford made 
revolutionary advances in semiconductors and microprocessors that paved the way for Silicon 
Valley itself.2 ARPANET, the foundational precursor to today’s internet, was a joint project at 
UCLA, Stanford, and MIT funded by DARPA.3 Without well-funded research at top institutions, 
America would lag far behind our competitors. 
Today, a lack of computational resources and access to critical data is sti ﬂing innovation at 
American academic institutions. The National AI Research Resource (NAIRR) would patch that 
gap. Through collaborations with agencies, universities, and industry, it has already supported 
over 250 research projects and served institutions across the country.4 Permanently 
establishing and funding NAIRR would provide American universities with the computational 
resources, data, software, and training they need to keep making breakthroughs. 
Building on the foundation that NAIRR provides to academic institutions, America needs a 
comprehensive national strategy for scientiﬁc advancement through AI. Such a strategy would 
create what could be described as a "superhighway for science" - connecting our universities' 
research capabilities with national laboratories and industry partners in a cohesive ecosystem. 
This approach would integrate academic innovation with advanced computing resources, 
scientiﬁc datasets, and domain-speci ﬁc AI models to tackle grand challenges in energy, 
materials science, and healthcare. By enabling researchers to move beyond computational 
constraints and fostering multidisciplinary collaboration, we can dramatically reduce the time 
from basic research to practical application. This acceleration would not only maintain 
America's scientiﬁc leadership but would transform how we address urgent national priorities, 
shifting breakthrough timelines from decades to years or even months. 
2. Strengthening Our Defenses
2.1 Biorisk, WMD Policy, and Gene Synthesis Screening 
Advanced AI has already helped us solve some of the biggest unsolved problems in science 
(like discovering how proteins fold), but it might also empower malicious actors to in ﬂict 
large-scale harm. One of the clearest areas of potential risk is AI being misused to aid in the 
creation or novel design of dangerous biological agents. We know that terrorist organizations 
4 National Artificial Intelligence Research Resource. "National Artificial Intelligence Research Resource 
(NAIRR)." Accessed March 15, 2025. https://nairrpilot.org/. 3 Britannica, "ARPANET," accessed March 15, 2025, https://www.britannica.com/topic/ARPANET. 2 UC Berkeley Library, "The Berkeley Remix Podcast: Season 4, Episode 2: Berkeley Lightning: A Public 
University’s Role in the Rise of Silicon Valley," July 24, 2019, accessed March 15, 2025, 
https://update.lib.berkeley.edu/2019/07/24/the-berkeley-remix-podcast-season-4-episode-2-berkeley-light
ning-a-public-universitys-role-in-the-rise-of-silicon-valley/. 1 Stanford University School of Engineering, "Laying the Foundation for Today’s Generative AI," accessed 
March 15, 2025, https://engineering.stanford.edu/news/laying-foundation-todays-generative-ai. 
2 


 
like al-Qaeda spent years trying to develop biological weapons, but lacked the technical 
sophistication to succeed.5 OpenAI, for example, recently warned that its models are “on the 
cusp of being able to meaningfully help novices create known biological threats.”6  
One clear way to mitigate this risk is requiring gene synthesis companies to screen for 
dangerous biological agents. Some government agency contracts currently enforce certain 
standards for purchasing synthetic nucleic acids and benchtop DNA synthesis equipment, and 
these standards should be maintained and expanded. This includes incorporating new 
sequences of concern derived from AI-driven research and collaborating with agencies such as 
HHS, DARPA, and the AI Safety Institute (AISI) to maintain real-time databases of emerging 
threats. Over the long term, the administration should work with international partners to 
promote universal screening standards, making it far more diﬃcult for malicious actors to 
acquire or engineer dangerous pathogens. By taking a proactive stance on AI-fueled biorisk, the 
United States can help avert signiﬁcant threats to global health and security. 
The federal government's purchasing power represents an untapped tool for encouraging 
responsible AI development with a speciﬁc focus on preventing weapons of mass destruction. 
Instead of creating new regulatory frameworks, procurement requirements could establish 
market incentives for AI companies to implement rigorous safeguards against their systems 
being used to develop biological, chemical, nuclear, or radiological weapons. This approach 
would formalize practices many leading AI companies already claim to follow, but in a 
consistent and veriﬁable way aligned with established legal de ﬁnitions of WMDs. By making 
government contracts contingent on having robust prevention policies, this light-touch strategy 
would affect only the most advanced AI systems while providing companies with regulatory 
certainty. 
2.2 Frontier AI as Critical Infrastructure (Cyber Defense) 
As AI systems grow in complexity and criticality, large-scale "frontier AI" models increasingly 
resemble essential infrastructure across ﬁnance, healthcare, logistics, and defense. These 
systems process vast amounts of sensitive data and power critical decision-making tools, yet 
present high-value targets for cyber threats. Currently, gaps exist in deﬁning frontier AI as critical 
infrastructure, creating ambiguity around mandatory reporting and government's role in incident 
response. 
Frontier AI introduces unique cyber security challenges through expanded attack surfaces, 
vulnerabilities speciﬁc to machine learning architectures, and potential cascading failures when 
compromised systems affect multiple sectors. These risks are fundamentally different from 
traditional IT security concerns and require specialized approaches to mitigation. 
6 OpenAI. "Deep Research System Card." Accessed March 15, 2025. 
https://openai.com/index/deep-research-system-card/.   5 Mowatt-Larssen, Rolf. Al Qaeda Weapons of Mass Destruction Threat: Hype or Reality? Cambridge, 
MA: Harvard Kennedy School, Belfer Center for Science and International Affairs, 2010. 
3 


The federal government should direct NIST to develop cyber security frameworks tailored to 
frontier AI systems. These frameworks should address model security, legacy software 
integration, and resilience against AI-speciﬁc attacks in collaboration with international 
partners. Formally designating frontier AI companies and their data centers as a critical 
infrastructure sector would bring them under the Cyber Incident Reporting for Critical 
Infrastructure Act (CIRCIA), requiring prompt disclosure of signiﬁcant breaches to federal cyber 
security agencies. 
This designation should be accompanied by expanded capabilities at DHS and other relevant 
agencies. This includes specialized personnel and technologies focused on AI cyber security, 
dedicated AI threat response teams, and advanced monitoring tools. Additionally, investment in 
open-source security testing tools for AI systems would democratize access to security 
capabilities for smaller organizations while fostering a broader community of security 
researchers. 
3. Visibility and Understanding
3.1 Whistleblowers and National Security 
Employees embedded in AI labs are uniquely poised to discover emerging vulnerabilities that 
can pose threats to U.S. national security. Currently, companies have no obligation to provide 
employees a way to report those threats internally and can retaliate against employees who 
alert the government. Existing whistleblower protections do not cover many of the risks that an 
employee may want to report. This lack of legal protection could leave the U.S. vulnerable to 
serious risks that employees may otherwise seek to alert the government of. With few other 
paths for agencies to be alerted to or become aware of serious threats it is critical that we 
protect the workers most aware of the state of this technology from retaliation. Multiple steps can be taken to facilitate narrow reporting of serious national security risks. 
Agencies such as the Department of Commerce (DOC) or the Department of Defense (DOD) 
should establish secure channels for conﬁdential disclosures about AI-related concerns. Those 
channels should be monitored by experts on the issue that can understand the level of risk and 
escalate to agency leadership when necessary. Portals like this would grant anonymity and 
make it clear to employees that their report will be heard by someone who understands the 
issue. This would reﬂect longstanding practices in other high-security domains like national 
intelligence, the judicial system, and ﬁnancial services. 
In tandem, Congress should clarify and expand whistleblower statutes to include AI-driven risks, 
ensuring that employees can safely, securely, and anonymously report serious risks to the 
correct government actors. These protections should focus on substantial and speciﬁc risks to 
4 


 
national security. Such protections would mirror critical industries like nuclear energy, oil and 
gas, and transportation infrastructure where employees are protected when reporting 
technically legal but dangerous conduct. These protections would increase the government's 
understanding of the evolution of AI and its risks without adding new burdens on the companies 
developing this critical technology. 
3.2 Supporting Standards Development and Evaluation 
Without the ability to evaluate and understand the state of frontier AI development the U.S. will 
inevitably fail to recognize critical risks and fall behind our adversaries. The AI Safety Institute 
(AISI) is the best repository of technical knowledge and capabilities that the U.S. currently has. It 
has already successfully worked with the top companies to evaluate the state of frontier AI and 
developed standards for AI that are our best hope of inﬂuencing international AI development. 
The technical expertise at AISI remains our best tool to assess foreign models like DeepSeek R1 
and track our progress in comparison to our adversaries. As new risks emerge from frontier 
models AISI is the best positioned body to keep agency leadership informed and ensure our 
national security apparatus is never caught off guard. It is critical not only that the work of AISI 
continues but that its funding and staff is maintained and expanded. 
The impact of such a body goes far beyond the United States itself. As oppositional countries 
like China seek to inﬂuence the future of AI internationally, bodies like AISI under NIST represent 
our strongest opportunity to encode American values into the future of this technology. China is 
already aggressively working to inﬂuence international standards bodies, ﬂooding groups like 
the ITU with hundreds of new standards designed to bene ﬁt Chinese companies and embed 
authoritarian values into the technologies of the future. China views standards setting as one of 
the most important factors for the economic future of China and has already succeeded in 
inﬂuencing technical standards on technologies like 6G mobile.7 The Chinese government 
directly subsidizes domestic ﬁrms’ participation in standards bodies, rewarding both the 
proposal and successful setting of new standards.8 University professors and researchers are 
similarly rewarded and Chinese academic institutions are actively recruiting foreign experts in 
science and technology to take part in their standards-setting work through the “Recruitment 
Program of Global Experts.”9 Meanwhile, the National Institute of Standards and Technology has 
publicly stated that its work on developing AI standards would be “very, very tough” due to a 
serious lack of funding.10  
10 Kelley, Alexandra. "NIST’s Emerging Tech Work Will Be ‘Very Difficult’ Without Sustained Funding, 
Director Says." Nextgov, May 22, 2024. 9 "University professors and researchers are similarly rewarded and Chinese academic institutions are 
actively recruiting foreign experts in science and technology to take part in their standards-setting work 
through the 'Recruitment Program of Global Experts.'" 8 United States-China Business Council. China in International Standards Setting: USCBC 
Recommendations for Constructive Participation. February 2020. 
https://www.uschina.org/wp-content/uploads/2020/02/china_in_international_standards_setting.pdf. 7 "China Is Writing the World's Technology Rules," The Economist, October 10, 2024, 
https://www.economist.com/business/2024/10/10/china-is-writing-the-worlds-technology-rules.   
5 


To compete with China internationally and maintain our in ﬂuence on advanced technology 
abroad we must take our role in standards setting seriously. We can support NIST’s work by 
working with Congress to increase its funding through both appropriations and private 
donations. The administration could pursue a new Foundation for Standards and Metrology, 
similar to the body proposed by the Expanding Partnerships for Innovation and Competitiveness 
Act modeled on existing foundations that support other federal agencies like the CDC and NIH. 
One key barrier to standards meetings being hosted in America is substantial visa backlogs and 
restrictions. A program similar to the NSA National Centers for Academic Excellence could be 
established to recognize schools that conduct outstanding work in standards development to 
channel new funding and support their work. Cooperation with American allies is another way to 
counter China’s international inﬂuence. By including standards activities in bilateral and 
multilateral science and technology cooperation agreements, the U.S. could leverage its existing 
inﬂuence to counter China’s. 
4. Infrastructure/Data Centers
4.1 Key Provisions from Prior Data Center EO 
The January 2025 executive order took initial steps by making federal lands available for AI 
development, but failed to address permitting bottlenecks and imposed restrictive clean energy 
mandates. We must build upon this foundation by leveraging Department of Defense and Energy 
sites for data centers with truly expedited NEPA reviews through categorical exclusions. While 
China commits nearly $140 billion to state-directed AI infrastructure, American companies need 
a streamlined regulatory pathway that maintains necessary oversight without years of 
bureaucratic delay.11 
4.2 Grid Connections and Energy Sources High-performance data centers require immediate access to ﬁrm, uninterrupted power at 
unprecedented scale. To meet this demand, we must fast-track the recommissioning of existing 
nuclear plants speciﬁcally for AI facilities, following Microsoft's successful Three Mile Island 
model. Simultaneously, we should deploy enhanced geothermal systems and small modular 
reactors by establishing federal loan guarantees and regulatory fast-tracks with speciﬁc 
deployment timelines. During this transition, the Defense Production Act should be leveraged to 
11 "China Steps Up Support for Tech Sector as AI Enthusiasm Soars." Bloomberg, March 6, 2025. 
https://www.bloomberg.com/news/articles/2025-03-06/china-steps-up-support-for-tech-sector-as-ai-enthu
siasm-soars.   https://www.nextgov.com/emerging-tech/2024/05/nists-emerging-tech-work-will-be-very-difficult-without-su
stained-funding-director-says/396803/.  
6 


 
resolve supply chain bottlenecks for gas turbines and other critical components needed for 
transitional power generation. 
The administration should establish "Special Compute Zones" on federal lands with categorical 
NEPA exclusions, pre-approved grid connections, and expedited 30-day permitting timelines. 
Companies accessing these beneﬁts must implement speci ﬁc security protections, including 
DOD's Cybersecurity Maturity Model Certi ﬁcation (Level 3), background screening for personnel 
accessing sensitive hardware, and supply chain security measures with intelligence community 
oversight. By coupling energy access with security requirements, we ensure that as we build 
American AI capabilities, we simultaneously protect them from foreign adversaries without 
imposing arbitrary restrictions on how these facilities are powered. 
 
5. Chips/Export Controls 
5.1 Furthering the Framework for Arti ﬁcial Intelligence Diffusion 
Export controls play a pivotal role in safeguarding cutting-edge U.S. AI capabilities from 
potential adversaries. The Framework for Artiﬁcial Intelligence Diffusion is a sensible 
continuation of technology export controls that began in the ﬁrst Trump Administration with 
their work on countering Huawei and ZTE. The framework may need to be modi ﬁed to re ﬂect 
changing technical and geopolitical realities, but broadly continuing it gives the US a strong path 
to incentivize other nations to implement basic KYC and security policies to prevent the capture 
or misuse of this tremendously consequential technology.  
5.2 Funding BIS and CHIPS Act Implementation 
The Bureau of Industry and Security (BIS) bears an expanding mandate in overseeing AI 
hardware export controls, yet suffers from limited budgets and outdated IT infrastructure.12 
Strengthening BIS’s enforcement capacity through sustained funding—such as an additional $75 
million annually for personnel and a one-time $100 million upgrade for monitoring 
systems—would enable more rigorous oversight of emerging AI technologies. These resources 
would facilitate deeper technical expertise, faster case processing, and stronger international 
investigations into potential violations. 
While we work to hinder China’s access to our technology, we must also bolster domestic 
production. Domestic semiconductor production has gained momentum, but maintaining this 
trajectory requires continued investment in research and development as well as enhanced 12 Graham, Edward. "Export Control Agency Lacks Funding and Tech to Manage Its Workload, Official 
Says." Nextgov, March 21, 2024. 
https://www.nextgov.com/digital-government/2024/03/export-control-agency-lacks-funding-and-tech-mana
ge-its-workload-official-says/395132/.  
7 


 
supply chain resilience. Expanding tax incentives, supporting specialized chip design facilities, 
and strengthening stable partnerships with allied nations for securing raw materials are critical 
steps toward advancing U.S. semiconductor manufacturing. By bolstering regulatory support 
and effectively implementing existing policy initiatives, the United States can sustain its global 
leadership in AI-speciﬁc hardware, a cornerstone of economic competitiveness and national 
security. 
 
6. Talent 
6.1 Immigration Reforms for AI Expertise 
America’s leadership in AI has long bene ﬁted from foreign-born talent attracted by the country’s 
research ecosystem and innovation culture. One study found that of the 50 “most promising” 
U.S.-based AI startups on Forbes’ 2019 “AI 50” list, 66% had an immigrant founder.13 The most 
common path for immigrants to found U.S.-based startups is through student visas, but current 
restrictions force many foreign students to stay locked into academia. Among those who stay 
on that path, many AI PhDs are forced to leave, and one-third state that immigration constraints 
played a key role. Losing such talent at a time when skilled AI scientists are in dramatic demand 
and stunningly low supply is a mistake. A number of easy changes could be made to attract foreign talent and keep it in the United 
States. Creating a subcategory for H-1Bs for AI workers with a higher annual cap and extended 
baseline duration would allow American companies to bring in more skilled AI scientists and 
provide a clear path for them to stay in the United States and contribute to American innovation. 
The O-1 visa’s dual-intent classiﬁcation is currently unclear, and many startup founders have had 
applications rejected. Making it explicit that the O-1A is a status with unequivocal dual intent 
status would clarify that issue. The administration could also recapture some of the over 
300,000 green cards that go unused and provide them to companies seeking to attract skilled AI 
workers.14 Another source of untapped pathways for AI workers is DoD’s H-1B2 program. DoD 
currently only uses ~30% of its 100 allotted visas. The remaining 70% could be easily allocated 
in the direction of AI projects at DoD. The Schedule A shortage occupation list, a set of 
occupations that get streamlined labor certiﬁcation processes, is rarely updated and doesn't 
include critical STEM occupations where there are large scale shortages like AI. Updating it 
would create new employment-based green cards and streamline typically slow labor 
14 Esterline, Cecilia. "Green Card Backlogs Block Billions in Rural Investments." Niskanen Center, June 6, 
2024. 
https://www.niskanencenter.org/green-card-backlogs-block-billions-in-rural-investments/.&#8203;:content
Reference[oaicite:0]{index=0} 13 "University professors and researchers are similarly rewarded and Chinese academic institutions are 
actively recruiting foreign experts in science and technology to take part in their standards-setting work 
through the 'Recruitment Program of Global Experts.'" 
8 


 
certi ﬁcation processes. Finally, permanent labor certi ﬁcation via special handling could be 
granted to employers seeking AI talent. Currently only universities making teaching hires are 
granted special handling. Expanding this de ﬁnition to employers hiring AI talent could 
substantially streamline hiring in cases where American companies could bene ﬁt from 
advanced foreign talent. Such an expansion could be accomplished by allowing the National 
Science Foundation to occasionally add key emerging technologies to a list that qualiﬁes for 
National Interest Waiver EB-2 I-140 Petitions. 
 
7. AI and Protecting Children 
7.1 Deepfake Porn and Non-Consensual Intimate Imagery 
Surveys today indicate that nearly one in ﬁve high schoolers say they know of a peer at their 
school who’s been victimized by AI-generated nude images.15 That statistic would have been 
unfathomable just a few years ago when it took hundreds or thousands of images to create a 
realistic deepfake image of someone. Now all anyone needs is a single, clothed image of a 
victim to create realistic nude imagery. The resulting ﬂood of AI-generated nudes in American 
schools is unignorable. 
To protect our children we need new laws, companies cooperating with the federal government, 
and updated policies in our schools. The President and First Lady’s outspoken support for the 
TAKE IT DOWN Act is a critical step towards accountability for perpetrators and support for 
victims. By criminalizing authentic and deepfake non-consensual intimate imagery and requiring 
social media companies to remove images reported by victims, we can prevent this crisis from 
spiraling out of control. The DEFIANCE Act, another bill that passed the Senate unanimously last 
session, would add a second layer of support by creating a civil remedy for victims, allowing 
them to pursue injunctive relief to end the distribution of images and receive damages for harm 
caused to them.16  
Beyond legislation we need companies to work with the Trump Administration to restrict the 
creation and distribution of AI-generated nudes. Payment service companies like Cash App and 
Square should agree to bar deepfake porn websites from using their services, and invest in the 
broader detection and mitigation of payments for the creation or procurement of image-based 
16 U.S. Congress. Senate. S. 3696 – Disrupt Explicit Forged Images and Non-Consensual Edits Act of 
2024. 118th Cong., 2nd sess., 2024. 
https://www.congress.gov/bill/118th-congress/senate-bill/3696.&#8203;:contentReference[oaicite:0]{index
=0} 15 Center for Democracy & Technology, "In Deep Trouble: Surfacing Tech-powered Sexual Harassment in 
K-12 Schools," accessed March 15, 2025, 
https://cdt.org/insights/report-in-deep-trouble-surfacing-tech-powered-sexual-harassment-in-k-12-schools/
. 
9 


 
sexual abuse. We need companies like Google, Meta, and Bing to restrict the appearance of 
image-based sexual abuse in search results and on platforms. Companies that are developing 
image-generation models must agree to ﬁlter the datasets used to train their models to remove 
image-based sexual abuse and child sexual abuse material. NIST can work to support 
companies by investing in sophisticated deepfake detection and watermarking tools so 
platforms have additional tools to identify and remove harmful material online. 
School districts should explicitly ban the creation and circulation of deepfake sexual imagery in 
their policies, communicate these rules widely through orientations and events, and treat 
fabricated intimate images with the same seriousness as authentic ones. Clear and consistent 
consequences for perpetrators, potentially including suspension or expulsion, should be 
established, and school oﬃcials must promptly inform law enforcement or child protective 
services when required. In parallel, administrators should adopt standardized procedures to 
protect victims’ identities and ensure they receive conﬁdential support, including training staff 
on how to handle deepfake incidents. By incorporating safeguards into existing codes of 
conduct, sexual harassment, and cyberbullying policies, schools can deter would-be 
perpetrators, respond quickly and appropriately when abuse occurs, and better safeguard 
students’ well-being. 
7.2 Emotional Chatbots 
AI Chatbots designed to supplant human relationships are a new frontier of harm for America’s 
youth. Young people, particularly those who are already lonely or struggling, have real potential 
to become dependent on these applications for companionship and validation. Childhood is 
meant to be a time for young people to struggle through complex relationships in safe 
environments free from exploitation. If they are able to avoid the diﬃculties of building those 
relationships by conversing with arti ﬁcial entities their ability to be well adapted adults will be 
seriously damaged. Already we are seeing young people that spend large portions of their days 
conversing with these artiﬁcial companions, and in extreme cases resorting to self harm or 
suicide.17 We may be experiencing a situation similar to the early days of social media, when we 
missed our chance to intervene and establish common-sense guardrails for children. We urge 
the administration to proactively monitor this particular application of AI for manipulative design 
practices and deceptive marketing and, if necessary, intervene to protect our children. 
 
Conclusion 
The above recommendations offer a strategic approach to sustaining America’s leadership in 
artiﬁcial intelligence. By bolstering foundational R&D, strengthening defenses against national 
17 Tiffany Hsu and Sapna Maheshwari, “AI Chatbots Are Being Blamed in a Teen’s Suicide. Can 
Companies Be Held Liable?,” New York Times, October 23, 2024, 
https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html. 
10 


security threats, ensuring adequate oversight and whistleblower protections, expanding 
infrastructure and data center capacity, re ﬁning export controls, investing in a robust AI talent 
pipeline, and immediately tackling harmful applications like deepfake pornography, the United 
States can chart a future that safely harnesses AI’s transformative potential. 
11 


