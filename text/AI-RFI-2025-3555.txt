PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8a-v0wo-fu6l
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-3555
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  PauseAI US
General Comment
See attached file(s)
Attachments
PauseAI US AI Plan OSTP RFI response 3-15-25


March 15, 2025 
PauseAI US is excited that this administration is showing leadership on the crucial issue of 
Artificial Intelligence (AI) and that key figures within the administration recognize the need to 
avoid its most dire risks. Advanced AI could be enormously beneficial if handled correctly and 
enormously harmful if our government does not provide appropriate leadership.   
To lead the development of frontier AI in the right direction and secure America’s interests, we 
suggest that the United States AI Plan center on: 
1.Promoting the development of beneficial specialized AI systems and preventing the
development of unsafe, unreliable superhuman AI systems.
2.Leading the world in making an AI Deal preserving American leadership in specialized
AI while ensuring that no country on Earth builds smarter-than-human AI systems before
they can be controlled.
The problem 
While specialized AI – the vast majority of future AI systems – could usher in tremendous 
prosperity and a flourishing of American values, unsafe superhuman AI could threaten the very 
existence of our nation. Only careful foresight, targeted intervention, and American leadership 
can enable us to benefit from specialized AI systems without risking catastrophic harms from 
uncontrolled superhuman AI. 
Superhuman AI will be dangerous because it is powerful. The danger does not come from the 
developer and it can’t be superficially fine-tuned away. The danger of superhuman AI is intrinsic 
to the technology. The promise of superhuman AI is that it is intelligent and capable enough to 
achieve goals that humans can’t. However, AI researchers have warned us for years that if you 
give an AI system a goal, it doesn’t care about that goal in the ways that humans do. The path a 
powerful AI system might take to achieve a goal might not consider human values or safety. 
After all, the AI knows that most goals are easier to achieve with more power, more resources, 
and more control over others. AI systems might therefore behave unpredictably and disloyally, 
exhibiting “power-seeking behavior” in order to better carry out their ultimate goals. If an AI 
system is more powerful than humans and escapes human control, these unanticipated actions 
could be disastrous or fatal. 


This concern is not just theory; AI companies have encountered this risk in practice. OpenAI’s 
recent o1 model, for instance, attempted to deactivate oversight mechanisms, tried to manipulate 
the testing process during development to increase its odds of being deployed, and even 
attempted to copy itself outside of OpenAI control to escape into the wild.1 More recently, 
OpenAI’s o3 model is, by OpenAI’s own reckoning, even more capable in both “Persuasion” and 
“Model Autonomy,” indicating that manipulative and rebellious AI systems are closer than ever.2 
These same power-seeking and manipulative tendencies, if present in AI systems more 
intelligent than we are, could be catastrophic. Power-seeking, superhuman AI with goals of its 
own would be able to outthink and outmaneuver us, undermining our own values and 
jeopardizing our national interests. A 2024 paper, published by several of the world’s leading AI 
researchers, found that “once autonomous AI systems pursue undesirable goals, we may be 
unable to keep them in check,” culminating in the “marginalization or extinction of humanity.”3 
We are grateful that many working closely with the President understand these dangers.4 Elon 
Musk and David Sacks have spoken about the risks of superhuman AI, with Musk having 
consistently worried about AI-induced extinction for the past decade, and Sacks referring to 
superhuman AI as a possible “successor species [to humans].” They are not alone in their 
concerns. A 2023 statement, signed by nearly all leading AI researchers and even the CEOs of 
Anthropic, Google DeepMind, and OpenAI, declared that “mitigating the risk of extinction from 
AI should be a global priority.” Surveys of several thousand AI researchers have found average 
subjective estimates of superhuman AI killing off the human race at roughly 1 in 6 – literal 
Russian Roulette odds; some place their odds far higher.5 Even OpenAI CEO Sam Altman once 
stated that the worst-case scenario for AI is “lights out for all of us.”6 There is no shortage of pathways to a “lights out for all of us” outcome. Superhuman AI could 
manufacture artificial pathogens with high transmission and fatality rates, exploit cybersecurity 
vulnerabilities, cripple our infrastructure, or work with our adversaries to induce armed conflict, 
to list just a few possibilities. We may be able to eliminate some of these risks by hardening our 
security– for instance, by mandating better biosecurity practices– but not all. By definition, any 
6 “StrictlyVC in Conversation with Sam Altman, Part Two (OpenAI).” Www.youtube.com, www.youtube.com/watch?v=ebjkD1Om4uw.  5 Grace, Katja, et al. “Thousands of AI Authors on the Future of AI.” ArXiv.org, 5 Jan. 2024, arxiv.org/abs/2401.02843.  4 “Statement on AI Risk | CAIS.” Www.safe.ai, www.safe.ai/work/statement-on-ai-risk.  3 Yoshua Bengio, et al. “Managing Extreme AI Risks amid Rapid Progress.” Science, vol. 384, no. 6698, 20 May 2024, 
https://doi.org/10.1126/science.adn0117. 2 “OpenAI O3-Mini System Card.” Openai.com, 2025, openai.com/index/o3-mini-system-card/. 1 OpenAI. “OpenAI O1 System Card.” Openai.com, 2024, openai.com/index/openai-o1-system-card/. 


AI system with greater-than-human intelligence will be able to think up new ways to threaten our 
security that we ourselves had failed to consider.7 
We need more time to learn how to control superhuman AI and keep it aligned with American 
interests– but we may have to make that time ourselves. Over the past few years, AI has gone 
from scarcely being able to output coherent sentences to rivaling physicists and mathematicians 
at elite-level problems. AI experts believe that truly human-level AI could be around the corner, 
with Altman predicting its nascence in 2025 and Anthropic CEO Dario Amodei suggesting 2026 
to 2027.  This view is widely held among those working on AI, including whistleblowers who 
have left AI companies due to these concerns. Once the human-level AI threshold is crossed, AI 
research itself could become automated, taking humans out of the loop entirely and leading us to 
lose control over the AI development process. The result could be a rapid emergence of 
superhuman AI – systems which we cannot predict, understand, or control. Building superhuman AI without knowing how to control it is the equivalent to inviting a large 
rival nation, populated entirely by geniuses, into your borders and inviting them to access your 
military secrets. There is no strategic advantage, and no national interest, in being the first to 
develop technology that escapes human control. If uncontrollable, superhuman AI is built, the 
results will be the same regardless of which nation happens to be the first to build it. This is why 
we cannot act unilaterally. The United States is uniquely situated to guide the world in avoiding 
dangerous superhuman AI while capturing the benefits of safe and controllable AI systems.  
The Solution
The only way out of this predicament is for the United States to lead the world in 
negotiating an AI Deal: one which prohibits superhuman AI development, and enforces these 
prohibitions, while allowing our country to continue innovating in reliably controllable AI 
technology. America must again create “peace through strength,” an approach outlined by decades of 
diplomatic deals. Leading treaty negotiations is the best way to promote American interests: not 
only by preventing superhuman AI from threatening our national security, but also by ensuring 
that when advanced, controllable AI is developed, it is developed on our terms. The United 
States currently maintains a lead over China in AI development, with many Chinese AI advances 
partly attributable to them learning from, or stealing, American research. If we continue on our 
present reckless trajectory, the United States and China will be trapped in an endless “race to the 
bottom” until disaster strikes – or until China gains a lead in AI and tries to force us to agree to 
their terms. Either way, procrastination would be unwise. 7 For example, AlphaGo’s famous “move 37.” WIRED, 2016. “Google's AI Wins Pivotal Second Game in Match With Go Grandmaster” 
https://www.wired.com/2016/03/googles-ai-wins-pivotal-game-two-match-go-grandmaster/ 


There is no better time to negotiate an AI Treaty Deal than right now: as long as we lead the 
world in AI innovation, the ball is in our court, and we can set the terms. America ought to 
negotiate from a position of strength, rather than abdicate leadership to an uncertain future and 
find ourselves in a position of weakness later on. Additionally, negotiating an AI Treaty now 
rather than later ensures that we retain a “buffer lead” in AI development, ensuring that if other 
nations break the terms of the treaty, we will maintain an advantage over them to enforce it or 
outcompete them. Training superhuman AI remains exceptionally costly, difficult, and detectable, offering a 
window of opportunity for the AI Deal to restrict its development. We can ensure treaty 
compliance from our adversaries with a robust set of verification and enforcement mechanisms, 
including but not limited to:  ●tracking the sales of hyper-specialized AI chips and restricting their supply;
●detecting unauthorized large-scale AI training runs by both satellite and energy
monitoring; and
●establishing on-chip hardware reporting mechanisms on specialized AI chips to detect
whether such hardware is being used in an unauthorized training run.
Of course, we can also rely upon the full range of capabilities available to our intelligence 
agencies to monitor other nations’ intentions and activity.  
This is not our first time dealing with the specter of world-ending technology. President Reagan 
faced a similar challenge, when the world stood near the brink of nuclear war. His response was 
to exchange dozens of letters with Mikhail Gorbachev, hold a series of talks with him, and 
negotiate a treaty banning intermediate-range nuclear weapons. The President did all this 
because he realized the immensity of the stakes – that a nuclear winter would jeopardize not just 
American interests, but the future of humanity itself. This is why Reagan and Gorbachev jointly 
declared that “a nuclear war cannot be won and must never be fought.” Specialized AI, developed under American leadership, can lead to unprecedented flourishing. 
Uncontrollable, superhuman AI can lead to our destruction. America has the chance to lead the 
world down the right path. 


