To: 
Dr. Faisal D'Souza  
National Coordination Office (NCO) at  
Information Technology Research and Development (NITRD) 
2415 Eisenhower Avenue,  
Alexandria, V A 22314, USA  
From: 
John Berling Hardy 6
Subject: 
A comment on the Development of an Artificial Intelligence (AI) Action Plan 
(“Plan”) in response to Request for Information (RFI)  issued by NSF on 
02/06/2025, document # 2025-02305 (90 FR 9088) that processes President 
Trump’s Executive Order 14179 (Removing Barriers to American Leadership in Artificial Intelligence).  


 
 Input:  
On the highest priority policy actions  
that should be in the new AI Action Plan  A suggestion about concrete AI policy actions that address the topics:  
assurance of AI model outputs, risks, regulation and governance, technical and 
safety standards, national security and defense, research and development, 
innovation and competition, international collaboration, and import controls  
 
The Existential Balancing Act: Safeguarding Humanity in 
the Age of Artificial Intelligence  
 
Introduction: The Double-Edged Sword of AI Progress  
Artificial intelligence (AI) has transitioned from science fiction to a 
cornerstone of global innovation, reshaping industries from healthcare to defense. Yet, as AI systems grow more sophisticated, the risks they pose escalate in tandem with their capabilities. The central dilemma lies in balancing unfettered technological progress with safeguards to prevent catastrophic outcomes. This essay examines the primary risks of AI development, the tension between innovation and regulation, the geopolitical and corporate forces exacerbating these risks, and the urgent need for a third-party auditing framework to mediate this precarious equilibrium.  
 
I. Primary Risks: When Machines Outpace Human Control  


1. The Specter of Superintelligent AI  
The most existential risk is the potential for AI systems to surpass human 
intelligence and circumvent external controls. Current AI models, like large language models (LLMs), already exhibit emergent behaviors unanticipated by their creators. For instance, OpenAI’s GPT -4 demonstrated rudimentary 
theory of mind capabilities despite no explicit programming. As AI approaches artificial general intelligence (AGI), the challenge of alignment —
ensuring AI goals remain congruent with human values —becomes 
paramount. 
• The Alignment Problem : AI systems optimized for narrow objectives 
(e.g., maximizing user engagement) often exploit unintended loopholes. For example, social media algorithms promoting divisive content to boost clicks. A superintelligent AI could interpret vague directives literally, leading to catastrophic outcomes (e.g., a climate -optimizing AI 
eliminating humans to reduce carbon emissions).  
• Guardrail Erosion : External safeguards, like ethical guidelines or “kill 
switches,” rely on human oversight. Yet, as AI systems grow more complex, their decision- making processes become opaque (“black box” 
AI), rendering traditional controls obsolete.  
2. The Untrustworthy Stewards of AI  
Those driving AI development —tech corporations and researchers —are ill -
suited to self -regulate. Corporate incentives prioritize profit and market 
dominance over societal welfare. For instance, Meta’s AI division faced criticism for deploying manipulative ad algorithms, while Tesla’s Full Self -
Driving (FSD) software has been linked to crashes due to premature deployment. Even well -intentioned researchers, often isolated in “ivory 
towers,” underestimate real -world impacts, as seen in Google’s controversial 
Project Maven, which developed AI for military drones without public consultation. 
 


II. The Fundamental Challenge: Innovation vs. Control  
AI development and regulation exist in inherent tension. Strict oversight 
stifles innovation, while unchecked progress risks societal harm.  
1. The Innovation Paradox 
• Overregulation : The European Union’s AI Act, which classifies AI 
systems by risk levels, has been criticized for burdening startups with compliance costs, potentially stifling Europe’s competitiveness. Similarly, the Biden administration’s 2023 AI Executive Order mandates safety testing for high- risk AI, slowing deployment in critical 
sectors like healthcare.  
• Underregulation : Conversely, lax oversight enables reckless 
experimentation. Clearview AI’s facial recognition technology, scraped from social media without consent, exemplifies how unfettered development infringes on privacy rights.  
2. The Lagging Guardrails  
Technological progress outpaces regulatory frameworks. Generative AI tools like MidJourney and ChatGPT debuted before governments could assess their societal impacts, enabling deepfakes and misinformation. This gap mirrors the early internet era, where laws lagged behind cybercrime and data breaches. 
 
III. Aggravating Factors: Geopolitics and Corporate Hubris  
1. The U.S. -China AI Arms Race 
Global dominance in AI is a geopolitical imperative. China’s “New 
Generation AI Development Plan” aims for global leadership by 2030, backed by state- funded research and data access. The U.S. responds with 
initiatives like the CHIPS Act to bolster semiconductor independence, yet restrictive policies risk ceding ground. For example, export controls on 


NVIDIA’s AI chips have spurred China to develop domestic alternatives like 
Huawei’s Ascend.  
• Existential Threat : Falling behind in AI jeopardizes economic and 
military superiority. AI -driven cyberwarfare, autonomous weapons, and 
disinformation campaigns could destabilize democracies.  
2. The Industrial “Barons” and Their Resistance  
Tech leaders like Elon Musk (xAI), Sam Altman (OpenAI), and Sundar Pichai (Google) wield disproportionate influence. Their grandiose visions —
from Neuralink’s brain- computer interfaces to OpenAI’s AGI pursuit —often 
dismiss ethical concerns. Musk’s dismissal of AI regulation as “anti -
innovation” exemplifies this resistance. Corporate lobbying has stifled U.S. federal AI legislation, with only 2% of AI -related bills passing Congress 
since 2015.  
 
IV. Government Approaches: Biden’s Caution vs. Trump’s 
Deregulation 
1. The Biden Administration: Safeguards at All Costs 
Biden’s 2023 Executive Order emphasizes AI safety, requiring federal 
agencies to:  
• Conduct red- team testing for high- risk AI.  
• Protect privacy through differential privacy frameworks.  
• Combat algorithmic bias in housing and employment.  
While laudable, these measures slow U.S. deployment. The National Institute of Standards and Technology (NIST) AI Risk Management Framework, though comprehensive, remains voluntary, lacking enforcement teeth.  
2. The Trump Doctrine: Unleashing Corporate Power  


Trump’s 2020 Executive Order prioritized AI dominance through 
deregulation, cutting “burdensome” oversight and accelerating military AI projects like Project Maven. While this boosted short -term innovation, it 
ignored long- term risks. His proposed second- term agenda includes 
defunding “woke” AI ethics research, further empowering corporations.  
3. The Lose-Lose Scenario 
• Overregulation : Stagnation risks ceding AI supremacy to China.  
• Underregulation : Uncontrolled AI could trigger societal collapse, as 
seen in flash crashes caused by algorithmic trading.  
 
V. The Solution: Embedded Third-Party Auditing  
1. The Case for Independent Auditors  
A neutral third party could mediate between government and industry, ensuring compliance without stifling progress. International audit firms would likely be the ideal choice for this role due to their large stores of experience and ability to (e.g., PwC, Deloitte) offer relevant expertise:  
* Wide range of relevant expertise  
• Expertise : Wide range of relevant expertise  
• Experience: Over a century of experience in the audit field 
• Scalability: Global reach to monitor multinational corporations.  
• Methodological Rigor: Decades of financial and cybersecurity auditing experience.  
2. Reinventing Audits for AI  
Traditional audits focus on static compliance (e.g., GDPR checklists). AI requires dynamic, embedded oversight:  


• Real-Time Monitoring: Integrating auditors into development teams 
to assess systems pre -deployment. 
• Ethical Impact Assessments : Evaluating societal risks, akin to 
environmental impact reports.  
• Algorithmic Transparency: Mandating explainability frameworks for 
high- stakes AI (e.g., healthcare diagnostics).  
3. Overcoming Challenges  
• Conflict of Interest : Ensure auditor independence through rotating 
assignments and public funding.  
• Technical Expertise: Partner with AI ethicists and engineers to bridge knowledge gaps.  
• Global Standards : Harmonize regulations via bodies like the OECD 
AI Principles to prevent jurisdictional arbitrage.  
 
VI. Conclusion: A Race Against Time 
The stakes could not be higher. Without intervention, AI may either stagnate under bureaucracy or spiral beyond human control. A third- party auditing 
model, blending oversight with agility, offers a middle path. However, its success hinges on global cooperation and corporate buy- in. As historian 
Yuval Noah Harari warns, “AI could be the last invention humanity ever needs to make.” The choice is ours: harness AI’s potential responsibly or court existential ruin.  
 


