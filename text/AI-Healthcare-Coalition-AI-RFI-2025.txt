March 15, 2024 
White House Office of Science & Technology Policy  
National Coordination Office  
Networking and Information Technology Research and Development Program 
2415 Eisenhower Avenue 
Alexandria, VA 22314 
Attn: Faisal D’Souza, NCO
Submitted via email at 
Re: AI Action Plan 
On behalf of the AI Healthcare Coalition, thank you for the opportunity to contribute to the 
development of the United States Artificial Intelligence (AI) Action Plan to advance 
America’s AI leadership. We are pleased to submit our response to the White House 
Office of Science & Technology Policy (OSTP) and the National Coordination Office 
(NCO), Networking and Information Technology Research and Development (NITRD) 
Program.  We are committing to assisting the Trump Administration in its execution of 
President Trump’s Executive Order (EO) 14179, “Removing Barriers to American 
Leadership in Artificial Intelligence,” and look forward to working with you on issues 
concerning the intersection of health and technology policy.  
Founded in 2019, the AI Healthcare Coalition convenes healthcare AI innovators and 
stakeholders to advocate for patient access to safe, ethically-developed healthcare AI. 
Our members are AI innovators who have, or are in the process of developing, AI services 
(or devices) that require U.S. Food & Drug Administration (FDA) oversight and market 
authorization. Our founding membership includes the first-ever autonomous AI innovator 
to obtain FDA authorization, as well as innovators who have obtained the first-ever 
Medicare reimbursement for their respective AI services.  
As you develop our national AI Action Plan, we offer these recommendations for your 
consideration, and hope to be a continuing resource as you consider the complexities of 
AI deployment in healthcare settings.  We offer our thoughts below and look forward to 
continuing the conversation. 


2 I. Executive Actions to Unleash AI & Health Innovation
The Trump Administration can take clear steps now to ensure that FDA-authorized, 
rigorously validated AI health systems reach patients and this American industry sector 
flourishes. We offer below specific policy recommendations and background. 
U.S. Food & Drug Administration (FDA) 
Avoid Duplicative Regulatory Burden for FDA-authorized AI Models. Rapid
innovation in medical technology, including AI, has the potential to Make
Americans Healthy Again by addressing chronic disease and increasing
efficiencies. Rigorous safety validation by the US Food & Drug Administration
(FDA) is an essential component to ensure trust in such innovation. The FDA has
done significant work to evaluate and approve certain AI/ ML-enabled medical
devices. The FDA has also released numerous guidance documents concerning
AI/ML oversight, including the AI/ML-Based Software as a Medical Device (SaMD)
Action Plan ,1 the Proposed Regulatory Framework for Modifications to AI/ML-
Based SaMD ,2 and the Clinical Decision Support Final Guidance ,3 and the
Marketing Submission Recommendations for a Predetermined Change Control
Plan for AI/ML-Enabled Device Software Functions .4
Given the volume of already-existing regulation specific to health AI systems, it is
paramount that the U.S. does not impose duplicative, industry-agnostic laws and
regulations that are overly burdensome for American AI healthcare innovation,
particularly since many AI healthcare products are already subject to extensive
regulatory review. It is imperative to foster a legal and regulatory schema for
healthcare AI that puts American companies in a position where they can compete,
develop, use, and commercialize AI.
We caution against other regions’ industry agnostic, top-down approach to AI
regulation, as we have seen some AI innovator companies cease operations in
1 U.S. Food & Drug Administration (FDA), “Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a 
Medical Device (SaMD) Action Plan,” (January 2021) available at 
www.fda.gov/media/145022/download?attachment. 
2 FDA, “Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-
Based Softward as a Medical Device (SaMD): Discussion Paper and Request for Feedback,” (April 2019) available at 
https://www.fda.gov/media/122535/download?attachment. 
3 FDA, “Clinical Decision Support Software: Guidance for Industry and Food and Drug Administration Staff,” 
(September 2022) available at https://www.fda.gov/regulatory-information/search-fda-guidance-
documents/clinical-decision-support-software. 
4 FDA, “Marketing Submission Recommendations for a Predetermined Change Control Plan for Artificial 
Intelligence-Enabled Device Software Functions,” (December 2024) available at https://www.fda.gov/regulatory-
information/search-fda-guidance-documents/marketing-submission-recommendations-predetermined-change-
control-plan-artificial-intelligence. 


3 regions where such approaches have been implemented. AI innovation and 
deployment is a global endeavor, and our U.S. companies should be set up for 
successful development and provision of safe, trustworthy AI.  Like we have seen 
in recent U.S. proposals to address supply chain disruptions, once a market 
becomes untenable for manufacturing and production, it is very difficult to bring 
essential capabilities back to U.S. soil. Ensuring that we have the capabilities here 
at home to provide the best care—through AI, tech capabilities, and common-
sense approaches to AI safety and ethics—is essential to American 
competitiveness in global AI innovation.  
Facilitate Public-Private Standards Development. We increasingly hear that the
regulatory processes available to the FDA can be opaque for innovator companies
seeking review, and stakeholders have observed that the process can at times be
ad hoc, unpredictable, and slow—thwarting innovation and adoption of
revolutionary technologies.  To address these issues and hasten review for
innovative products, the FDA has created and implemented the ‘Collaborative
Community” methodology. This framework is meant to create broad, international
stakeholder consensus from patients, physicians, scientists, ethicists, payors,
technology creators and investors.
More than twenty-two of such Collaborative Communities have been created to
date. In one example, the Collaborative Community on Ophthalmic Innovation
(CCOI), was founded in 2021 in coordination with the FDA with over 100 widely
respected AI, clinical and methodological experts from academia and industry.
CCOI now has over 1000 contributing members, and the workgroup created the
first ever consensus statement, “Foundational Principles of AI”, which outlined AI
trust evaluation criteria based in ethics and patient benefit. This work was
developed in a public, transparent process over multiple years, culminating in
publication in 2022 with multiple FDA and academic co-authors.
CCOI’s “Foundational Principles of AI” represents a practical effort by the FDA to
nimbly develop standards that can be utilized to engender more dynamic review
of emerging technologies like AI. We urge you to ensure that this and similar work
is leveraged by the FDA to help AI innovators obtain thorough, rigorous review of
their technologies grounded in specialty-specific, consensus-driven standards.
On the clinical standards side specifically, among the standards for consideration
in clinical validation are ISO-14155 clinical investigation and ISO-14971 risk
management. Both standards may be applied broadly, however, industry could
benefit from more specific standards that take the uniqueness of AI/ML into
consideration. For example, the Association for the Advancement of Medical
Instrumentation® (AAMI) developed a guidance document on the application of


4 ISO-14971 to AI/ML, which discusses AI/ML specific risks including data drift. 
Similarly, sponsors of clinical studies could benefit from AI/ML clinical standards 
that could take AI-specific topics into account, such as the deidentification and 
management of data used in clinical studies. 
Successful facilitation of public-private standards will create greater transparency 
and consistency in regulatory review and commercialization of cutting-edge AI. 
Because existing requirements can be opaque and outdated as applied in the 
context of health AI, transparent and stakeholder-informed standardization will 
accelerate the development of AI health technologies.   
Centers for Medicare & Medicaid Services (CMS) 
Implement President Trump’s Medicare Coverage for Innovative Technologies
(MCIT) Program. During President Trump’s first term, the MCIT program was
improved to provide a transitional Medicare coverage policy for certain medical
technologies that receive “breakthrough technology” status under the FDA.
President Biden pulled back this policy, and instead finalized a similar policy,
entitled the “Transitional Coverage for Emerging Technologies,” or TCET,
program. While this program makes some positive steps forward by requiring CMS
and the FDA to work contemporaneously and iteratively with innovator applicant
companies, the agencies have said that very few—approximately four—
applications would be eligible for the TCET process annually. This is far too few.
We urge the Administration to facilitate timely coverage for AI health technologies
by finalizing a MCIT-like policy. In addition to increased patient access, such a
policy would create confidence for investment in these important technological
advancements, aiding commercialization.
Create Permanent Medicare Payment Pathways for AI Technologies. While some
AI healthcare services are currently reimbursed under the Medicare program, there
is no clear pathway for clinicians and innovators to obtain reimbursement for AI
services. As more AI services are rigorously validated and FDA-authorized, a clear
reimbursement pathway—or multiple pathways—is/are needed to ensure that
Medicare beneficiaries and clinicians can access these services. Notably, AI
services are diverse and varied, spanning multiple medical specialties and
incorporating modalities that vary in their use of algorithmic design and machine
learning.
For example, in the case of AI services provided in the outpatient setting, we often
see such services assigned to a clinical ambulatory payment classifications
(APCs) that do not adequately reimburse for provider costs because there is no
specific pathway for AI services. While CMS does have a New Technology APC


5 process to allow reimbursement for new technologies in the outpatient setting, that 
process was not created to facilitate the payment of AI technologies, and often 
presents too short or low a payment to allow access. For these reasons, we 
support a 5-year outpatient payment rate for AI technologies. Permanent payment 
pathways will create certainty for AI technology developers and providers, which 
would drive investment and accelerate innovation. 
Ensure Separate Payment for AI Services and Underlying Imaging Services. CMS
recently finalized a policy in its Outpatient Prospective Payment System (OPPS)
rule to allow separate payment for the AI service and the underlying imaging
service, but we see ongoing challenges in specialty areas where imaging services
and payment are bundled (for example, AI that is viewed as computer-aided
detection (CAD)). We urge the administration to provide further guidance in this
area to ensure that its separate payment policy, as finalized by CMS in the CY
2023 OPPS final rule, is effectuated.
Exclude AI Services from the OPPS Cap on Imaging Services. Sec. 5102(b) of the
Deficit Reduction Act (DRA) requires that “for imaging services […] furnished on
or after January 1, 2007, [CMS] will cap the [technical component] TC of the PFS
payment amount for the year (prior to geographic adjustment) by the [annual]
OPPS payment amount (prior to geographic adjustment).” Congress enacted
Section 5102(b), also known as the OPPS cap, to address rapid growth in
spending on imaging services, particularly for advanced imaging modalities—such
as computed tomography (CT), magnetic resonance imaging (MRI), and nuclear
medicine—as compared to growth in spending among less advanced imaging
modalities, such as x-ray or ultrasound.
As is clear in the statutory text, the DRA envisioned services such as x-rays,
ultrasound, PET, MRI, and CT as such “imaging services,” not autonomous AI or
other Software as a Service (SaaS) technologies. Further, when CMS
implemented the OPPS cap pursuant to Sec. 5102(b), CMS expressly excluded
some services because those services encompassed more than the use of
imaging technology alone. CMS should recognize an additional exclusion for
services for TC-only services for which there is no associated PC, as CMS has
previously recognized that codes without a TC / PC split should not be listed on
the OPPS Cap List.
Create a Pathway for Permanent, Separate Payment for AI services whose NTAP
Rate has Expired. Similar to CMS’ New Tech APC program, inpatient AI services
may apply for and receive an New Technology Add-on Payment (NTAP) payment
for 2-3 years if they meet certain criteria.  Once NTAP expires, CMS has reasoned
that the cost of the technology should be covered by the DRG payment. The


6 industry does not have data as to whether or not this is happening, and we believe 
that a lack of ongoing separate payment leads to a lack of provider adoption / 
patient access.  This is particularly true given recent CMS proposals that may 
reduce the time that NTAP status actually applies to a new technology in practice. 
The ambiguity about reimbursement post-NTAP disincentivizes investment in 
these tools which can benefit the Medicare program.  If appropriately reimbursed, 
such tools are likely to be adopted by healthcare providers, improve patient care, 
and create efficiencies. 
U.S. Department of Health & Human Services Assistant Secretary of Technology 
Policy (HHS/ASTP) 
Exclude FDA-authorized AI systems from HHS/ASTP (legacy Office of the National
Coordinator (ONC) from burdensome “predictive decision support intervention
(DSI)” regulations for certified electronic health record (EHR) vendors.  These
rules, which were promulgated under President Biden’s term, require onerous
reporting by EHR vendors—and in turn by those AI systems that run on such
vendors platforms. In a May 2024 fact sheet posted by the agency, HHS/ASTP
stated that these “requirements are intended to provide users and the public
greater information on whether a Predictive DSI is fair, appropriate, valid, effective,
and safe (what we refer to as the FAVES quality framework).” Further, HHS/OSTP
opined:
“The anticipated outcome of such transparency will increase public trust and 
confidence in Predictive DSIs, allowing users including healthcare systems, 
clinicians, and patients, to expand the use of these technologies in safer, 
more appropriate, and more equitable ways. The information this 
requirement makes available will support users in navigating the market for 
predictive and generative AI in healthcare. It will also help address 
numerous challenges, including risks of bias and harm, that such tools can 
present. This set of information will become a consistently available, 
industry-wide baseline upon which others can build, standardize, and 
enhance.”5 
These grounds are inconsistent with President Trump’s EO 14179, “Removing 
Barriers to American Leadership in Artificial Intelligence,” as they present 
additional regulatory burden for AI developers. Further, they raise the specter of 
increased compliance liability for both providers and developers, thwarting 
5 U.S. Department of Health & Human Services, Office of National Coordinator.  “ONC Health IT Certification 
Program Resource Guide: Decision Support Interventions Certification Criterion (45 CFR 170.315(b)(11))” (May 
2024) available at https://www.healthit.gov/sites/default/files/page/2024-05/DSI-Criterion-Resource-
Guide_508.pdf 


7 adoption.  At a minimum, for FDA-authorized AI/ML medical devices, ONC should 
permit reference to FDA submission and approval documents in lieu of satisfaction 
of the predictive DSI source attribute reporting. 
U.S. Department of Health & Human Services Office of Civil Rights (HHS/OCR) 
Exclude FDA-authorized AI systems from HHS/OCR’s Sec. 1557 Regulations.  On
May 6, 2024, HHS / OCR and CMS finalized updates to regulations implementing
Section 1557 of the Affordable Care Act (ACA) to increase anti-discrimination
protections for covered health care entities.6 Among other provisions, this rule
established new obligations for providers who utilize artificial intelligence (AI) and
other patient care decision support tools. At the time of issuance, HHS noted that
this regulation was in furtherance of President Biden’s now-rescinded EO on “Safe,
Secure, and Trustworthy Development and Use of Artificial Intelligence.”
Specifically, the Sec. 1557 final rule applies nondiscrimination principles to the use
of new and broadly defined “patient care decision support tools” in clinical care,
and requires those subject to the rule to identify and mitigate discrimination when
they use patient decision support tools, including automated and non-automated
tools, mechanisms, methods, and technology to provide patient care. As of now,
covered entities are required to comply with the new patient care decision support
tool requirements, including an ongoing monitoring and risk mitigation
responsibilities.
Although stakeholders expressed concerns regarding regulatory duplication at the
time that the rule was proposed, OCR expressly declined to exclude FDA-
authorized technologies from the scope of the rule. Importantly, while ONC’s
requirements for predictive DSIs apply to health information technology
developers, Section 1557’s requirements apply to covered entity users of patient
care decision support tools. These requirements create a redundant regulatory
burden for AI innovator companies and healthcare providers, especially for those
tools that are already subject to FDA review and authorization.
II. Legislative Actions to Empower AI & Health
Federal legislative changes are also needed to ensure that American health AI 
innovation is not impeded. Below we offer three examples of tangible federal 
6 “Nondiscrimination in Health Programs and Activities,” 89 Fed. Reg. 37522 (May 6, 2024) 


8 legislative changes that could be made to enable AI-assisted care to reach American 
patients: 
Amend Section 5102(b) of the Deficit Reduction Act (DRA). Congress enacted
Section 5102(b), also known as the “OPPS cap” in 2007. The law was created to
address concerns regarding rapid growth in federal spending on imaging services,
particularly for advanced imaging modalities—such as computed tomography
(CT), magnetic resonance imaging (MRI), and nuclear medicine—as compared to
growth in spending among less advanced imaging modalities, such as x-ray or
ultrasound. AI services were not contemplated at the time of enactment as within
the scope of the OPPS cap. Recently, CMS has begun to add AI services which
analyze and provide diagnostic interpretations of images to the OPPS cap list.
These services are not imaging services as contemplated by Congress when
Section 5102(b) was enacted. AI healthcare services should not be on the OPPS
cap list and Section 5102(b) should be amended to exclude such services.  While
this cap was originally created to control spending, it now poses a barrier to the
adoption of technologies that can improve diagnostic accuracy and reduce
unnecessary care.
Review the Mammography Quality Standards Act (MQSA). The MQSA was
enacted in 1994 and requires that all mammography facilities (except facilities of
the Department of Veterans Affairs (VA)) be accredited by an approved
accreditation body and certified by the FDA (or an approved State certification
agency). The MQSA requires that “mammograms be interpreted by a physician
who is certified as qualified to interpret radiological procedures, including
mammography.” There is a concern that this language may preclude the use of AI
analysis to interpret breast imaging for the identification of cancer, which is a front
line use case for AI. When Congress passed the MQSA in 1994, the possibility of
FDA-authorized, AI-enabled mammogram interpretation was not contemplated,
and the existing statutory language hinders the development of AI technology that
would advance Congress’ goal in passing the MQSA of high quality care for
patients. To ensure patient access to advanced breast cancer detection
technology, changes to the MQSA that will allow for the adoption of AI to enhance
screening and early detection should be reviewed and considered.
Amend the Social Security Act to Allow Glaucoma Screening. Section 1861(uu) of
the Social Security Act (SSA) requires that glaucoma screening be furnished by
“by or under the direct supervision of an optometrist or ophthalmologist who is
legally authorized to furnish such services under State law (or the State regulatory
mechanism provided by State law) of the State in which the services are furnished,
as would otherwise be covered if furnished by a physician or as an incident to a
physician’s professional service…”. Like in many other parts of the SSA, this


9 requirement precludes the use of fully autonomous AI services to perform an 
essential Medicare benefit – in this case, glaucoma screening. the SSA should be 
amended to allow for the use of rigorously validated, FDA-authorized autonomous 
AI services for glaucoma screening and the provision of other healthcare services. 
III. Conclusion
Your efforts to develop an AI Action Plan coincide with a time of great excitement for 
health AI.  This administration is in a unique position to usher in a few era of innovation, 
efficiency, and improved patient health outcomes through the rapid development of health 
AI technologies.   
We look forward to working with White House OSTP and the Networking and Information 
Technology Research and Development (NITRD) National Coordination Office (NCO) to 
ensure that patients and providers have access to FDA-authorized, rigorously validated 
healthcare AI.  If you have any questions about our comments, please contact me at 
Kind regards, 
Cybil Roehrenbeck 
Executive Director 
AI Healthcare Coalition 
This document is approved for public dissemination. The document contains no business-
proprietary or confidential information. Document contents may be reused by the 
government in developing the AI Action Plan and associated documents without 
attribution. 


