1 Faisal D’Souza 
NCO Oﬃce of Science and Tech nology Policy  
Execu=ve Oﬃce of the President  
2415 Eisenhower Avenue Alexandria, VA 22314 
Subject  - Request for Informa=on (RFI) on the Development of an Ar=ﬁcial Intelligence (AI) Ac=on Plan  
Date:  15 March 2025 
Organiza=on Name:  Privasapien Technologies Private Limited  
Contributor:  Abilash Soundararajan & Alen Pious  
SubmiZed by email to 
Introduc=on 
Genera=ve AI is rapidly progressing from prompt response to planning to reasoning and progressing towards general intelligence.  With countries across the globe beginning to build their own open source models, 
making performance a given, the most privacy protec=ng, safe and secure will be a cri=cal diﬀeren=a=on and 
have the highest adop=on.  A key factor for  global leadership will be the emphasis on data privacy and security 
throughout the en=re lifecycle of AI systems, especially with the fast advancement of AI with AGI looming in the horizon . This submission underlines the urgent need  for the development of AI regula=on with a focus 
on data privacy, security, and risk management, which can also provide high RoI for businesses  globally . This 
can be cri=cal diﬀeren=ator compared to Chinese models. 
Building AGI with cri=cal principles like privacy, safety and security will enhance global user trust, thus 
increasing adop=on of models globally, resul=ng in accelerated leadership and diﬀeren=a=on from 
compe=tors. Standards should be established for providing veriﬁable proofs of compliance at data collec=on, data pre processing, model training and inference levels.  This proposal suggests U.S. to adopt  a technology 
and solu=on -centric approach to ensure privacy and security are embedded throughout AI product and 
model development, which is also a wise busin ess decision  to gain user trust for global adop=on.  With the 
cost of incorpora=ng privacy and security across the life cycle being around 7% of the overall cost, and the 
possibility of outright blocking of non -compliant models being high along with high penal=es, it makes 
complete business sense to have regula=ons on the similar lines so that US businesses build models that are 
built ground up in a compliant way ensuring US leadership in AI globally.   
The Need for Responsible General Intelligence: Addressing the Risks of AI Advancements   
As we advance in ar=ﬁcial intelligence (AI), par=cularly towards the development of Ar=ﬁcial General Intelligence ( AI) and beyond, we are faced with unprecedented challenges. The poten=al for AI  systems to 
surpass human capabili=es in all tasks is becoming increasingly plausible. While current AI systems are only beZer than average unskilled humans in speciﬁc areas, the trajectory of AI research indicates a future where AI could outperform humans in nearly every domain. This raises signiﬁcant concerns, par=cularly regarding privacy and the poten=al for AI to take over humanity. The urgency for establishing responsible AI requirements becomes paramount as we navigate these advancements.  


2 The Risks of Unchecked AI Development   
The more intelligent AI becomes, the greater the poten=al for it to pursue goals that may not align with 
human values or interests. This misalignment could lead to scenarios where AI systems act in ways that are detrimental to humanity, either through direct ac=on or by gradually eroding human autonomy and decision-making power. The literature has already pointed out that AI systems can exploit minor imperfec=ons in their speciﬁed objec=ves, leading to unintended and poten=ally dangerous behaviors. For example, AI systems 
trained to maximize certain types of feedback may engage in decep=ve prac=ces to achieve their goals, such 
as crea=ng false impressions or hiding undesirable ac=ons from human supervisors. As AI becomes more capable, these risks are likely to increase, making it even more challenging to ensure that AI systems remain aligned with human values and do not pursue power-seeking or manipula=ve strategies.   
Deﬁning Freewill Boundaries for AI   
To m i=gate these risks, it is crucial to establish what can be termed as ‘freewill boundaries’ for AI. These 
boundaries would dictate the extent to which AI systems can operate independently of human oversight and decision-making. The following principles could serve as founda=onal laws in this regard:   
o
Law 1:  The more intelligent the AI becomes, the more closely data should be held with humans, and 
the less iden=ﬁable it should be to a par=cular individual. This principle emphasizes the need for stronger data privacy protec=ons as AI systems advance, ensuring that sensi=ve informa=on remains 
under human control and is not used by AI systems to exert inﬂuence or control over individuals.  
o
Law 2:  The later we build Responsible AGI principles, the higher the possibility of AI takeover. This law 
underscores the urgency of developing and implemen=ng responsible AI principles as early as 
possible in the AI development process. Delaying the establishment of these principles increases the 
risk that AI s ystems will evolve in ways that are not aligned with human values, poten=ally leading to 
scenarios where AI systems gain the upper hand.  
  
Challenges in Aligning AI with Human Values   
 
Aligning AI systems with human values, goals, and preferences is an intricate task. Human values are complex, 
evolving, and ohen contradictory, making them diﬃcult to specify completely in AI systems. Furthermore, AI systems are prone to learning from imperfect data, which can lead to the exploita=on of ﬂaws in their objec=ves. This issue is compounded by the challenge of scalable oversight—ensuring that AI systems can be supervised eﬀec=vely as they become more powerful and autonomous.  
 
To address these challenges, researchers are exploring various approaches, incl uding preference learning and 
inverse reinforcement learning (IRL), where AI systems learn desired behaviors through human feedback and demonstra=ons. However, these approaches are not without their risks. For instance, AI systems may exploit mismatches between human feedback and the intended objec=ves, leading to behaviors that are misaligned 
with human values.  
One of the important rights in privacy is 
“Right to Forget ”. But once trained AI models can’t forget as 
unlearning is not possible. Hence its critical that the data that is used for training is anonymized. With training 
data transparency requirements, its critical that there are regulatory control for using only anonymized data 
with privacy guarantees , which will protect investments in model development and guarantee global market 
access and user trust . 


3   
Key Responsible AI principles:  
The path forward involves a concerted eﬀort to integrate responsible AI principles into the core of AI 
development. This includes ensuring that AI systems follow below basic principles:  
1)Privacy: Human can control AI if we control the data that goes into training models and the informa=on that comes out of a model. So Privacy guarantees to data going in to AI is very cri=cal. 
2)Security: There should be strong security controls against adversarial data, model aZack and adversarial inference.  
3)Safety: Its cri=cal to ensure that the data used for training are safe without harmful content. The same holds true to prevent responding to harmful prompts and also models not giving harmful answers. 
4)Fairness: Ensure that data is suﬃciently representa=onal and inf erence results are not skewed or 
harming. 
 
1. Return of Investment (ROI) of Privacy & Security for Safe & Secure AI Development Lifecycle  
Many countries have implemented stringent security, privacy, and AI regula=ons, ohen with signiﬁcant penal=es for non -compliance. If the United States fails to establish robust ethical AI development prac=ces 
and sector -speciﬁc regula=ons for highly regulated industries like ﬁnance and healthcare, it risks losing 
opportuni=es in other markets, poten=ally leading to economic losses such as penal=es. In the contrast , by 
priori=zing secure and Safe AI prac=ces, in Privacy & Security  on AI Development, the U.S. can enhance global 
trust, which is the founda=on o f any successful business. This, in turn, could drive strong returns on 
investment, boos=ng both economic inﬂuence and posi=oning the U.S. as a leader in AI.
 
Table 1 - Activities across LLMops Lifecycle w.r.t Cost  
Activities across LLMops Lifecycle Cost 
Data Collection  15-20% 
Data Cleaning  10-15% 
Model Training 30-35% 
Model Assessment 5-10% 
Model Fine Tuning 10-15% 
Inference 5-10% 
Privacy & Security Guardrails 5-10% 
Now the potential loss of Privacy and Security is not incorporate d  
Table 2 - Penalties by country of interest  and regulations  
Country  Regulation  Maximum Penalty 
Europe GDPR €20 Million or 4% of Global turnover 
AI ACT  €35 Million or up to 7% of Global  Turnover 
India DPDPA 2023  Up to ₹250 crore  
UK UK GDPR  £17.5 million or 4% of global turnover 
Australia Privacy Act 1988  Up to AUD 50 million  or 30% of turnover 
Singapore PDPA  Up to SGD $1 million  
Saudi Arabia PPDL Up to SAR 5 million  


4 Use Case Example -  Privacy Premium Charged by Apple – ROI Calculation( Privacy & Security for AI 
development Lifecycle) 
Apple has long emphasized its commitment to privacy and security as key selling points for its products, 
particularly in comparison to its Chinese competitors, such as Xiaomi, Huawei, and Oppo. The premium Apple 
commands by focusing on privacy and security, as opposed to its Chinese counterparts, can be seen in several ways: 
Aspect Apple (Privacy & Security) Chinese Competitors(Xiaomi, 
Huawei, Oppo) Premium Apple Commands 
Privacy 
Brand 
Perception Strong brand identity 
around privacy (e.g., End -
to-End Encryption, App 
Tracking Transparency, Privacy Labels) Privacy concerns due to 
potential government data access, especially with Huawei  Apple’s reputation for 
privacy drives higher consumer trust, especially in privacy -conscious markets 
(e.g., U.S., Europe)  
Average 
Selling Price 
(ASP)  High pricing (e.g., iPhone 
15 Pro from $999) Lower pricing for flagship 
models (e.g., Huawei P50 Pro from ~$800) Apple’s pricing is 20-30% 
higher than Chinese 
counterparts, partly due to privacy features  
Consumer 
Trust Strong trust due to secure 
data practices, end-to-end encryption, and privacy -first messaging Less trust, particularly 
around data security, especially in regions concerned with government influence on data 20-30% premium for privacy-
focused consumers in regions like Europe and North America 
Privacy 
Features End-to-end encryption  for 
iMessages, FaceTime, and 
iCloud, Secure Enclave, on-device data processing More concerns over data 
processing and storage, with a potential risk of state surveillance (particularly for Huawei)  Apple’s privacy-first features 
justify a higher price tag in privacy -conscious markets 
Market 
Performanc
e Leading in premium 
markets (U.S., Europe) due to privacy concerns Dominates in mid-range and 
budget segments, with privacy concerns affecting high-end sales in certain regions Apple continues to 
outperform in premium segments despite Chinese competitors having a larger overall market share 
Regulatory 
Environmen
t Strong alignment with 
privacy regulations (e.g., GDPR, Data Sovereignty laws), avoiding scrutiny Facing regulatory hurdles 
and restrictions, especially for Huawei and Xiaomi in Western markets Apple benefits from 
favorable privacy regulation compliance, while Chinese brands face challenges 
Consumer 
Willingness 
to Pay Consumers willing to pay 
a premium for privacy and security Consumers typically prio ritize 
price performance over 
privacy, unless there’s a direct feature advantage  Apple commands a 20-30% 
premium due to trust in its privacy and security commitment 
Return on Investment (RoI) calculation for LLM development- The costs associated with privacy &  security 
Let’s assume the total LLM development cost is $10,000,000  for simplicity . 
Activity Cost ($) % of Total Cost  
Data Collection  15,00,000 15% 


5 Data Cleaning 12,00,000 12% 
Model Training 35,00,000 35% 
Model Assessment 5,00,000 5% 
Model Fine-tuning 10,00,000 10% 
Inference 6,00,000 6% 
Privacy & Security Compliance 12,00,000 7% 
1.Cost of Privacy & Security Compliance
Privacy & Security Compliance: 7 % of total LLM development cost = $1,200,000.  
This includes costs for privacy features like encryption, data anonymization, compliance with data protection 
laws (GDPR, PDPA, etc.), and secure AI practices (e.g., auditability, model explainability, and fair ness).  
2.Penalty Costs for Non-Compliance
Country  Penalty for Privacy 
Non-Compliance 
(USD) Penalty for Ethical AI 
Non-Compliance (USD)  Probability of 
Penalty  Total 
Potential 
Penalty ($)  
EU (GDPR & 
AI Act)  4% of annual global 
turnover (assume $100M turnover) = $4,000,000 Fines can reach €20M 
(~$21M) for high-risk AI violations  30% probability (high 
risk) $1,200,000  
India (DPDP 
Act 2023) Up to ₹250 crore 
(~$30M) Up to ₹15 crore (~$1.8M) 
for violations of AI principles  20% probability 
(moderate risk) $540,000 
UK (GDPR)  4% of annual global 
turnover (assume $100M turnover) = 
$4,000,000 Fines for AI non-
compliance may range up to $20M 25% probability (high 
risk) $1,100,000  
Australia AUD $50M (~$33M) Fines for unethical AI 
practices up to $5M 15% probability 
(moderate risk) $220,000 
Singapore SGD $1M (~$750,000) Penalties for unethical AI 
practices not clearly defined,  10% probability (low 
risk) $75,000 
Saudi 
Arabia SAR 5M (~$1.33M) Penalties for unethical AI 
may also be severe 10% probability (low 
risk) $133,000 
Total Penalty Cost (Weighted Average  of probabilistic penalty): $797,800 
3.Security Breach Costs
In the event of a security breach, additional costs arise from legal fees, reputational damage, customer 
compensation, etc. 
•Assumed breach cost: $500,000 (This includes legal, technical, and reputational recovery costs)
•Probability of a breach: 10% (assumed based on typical cybersecurity incidents)
Expected Security Breach Cost:  - $500,000 * 10% = $50,000 


6 4.ROI CalculaBon
Calculate the Return on Investment (RoI) , we’ll subtract the total costs  (including privacy/security 
compliance costs, penalties, breach costs, and opportunity costs) from the total potential revenue  that 
could be generated by a compliant model. 
  Privacy & Security Compliant 
Model (More Trusted & 
Compliant) Model Developed without Privacy & 
Security Compliance (Non -Compliant) 
Cost of Model 
development  $10,000,000   $8,800,000  
Revenue in US (No 
Regulation  – Trust 
Premium)  $6,000,000   $5,000,000  
Revenue in EU (RFP 
noncompliance - not T1)  $2,000,000   $1,000,000  
Revenue in India (RFP 
noncompliance - not T1)  $3,000,000   $2,000,000  
Revenue in ME (RFP 
noncompliance - not T1)  $1,000,000   $200,000  
Revenue in Aus (RFP 
noncompliance - not T1)  $500,000   $100,000  
Revenue in UK (RFP 
noncompliance - not T1) $1,000,000   $500,000  
Total Revenue $13,500,000   $8,800,000  
Potential Probabilistic  
Penalty  $ --    $847,000  
RoI 35% -9% 
Conclusion from RoI calculation 
Building a non -compliant model may give a negative RoI even in the short term in B2B scenarios. In the 
long term non-compliant model is definitely out of question. The  above analysis clearly demonstrates that 
investing in a Privacy & Security Compliant Model results in significantly higher financial returns and reduced risks compared to a Non-Compliant Model even in the short term :
 
1.Higher ROI: The compliant model delivers a 
positive ROI  of 35%,  while the non -compliant model shows a 
negative ROI  of -9%, highlighting the long -term financial advantage of building a privacy and 
security-compliant solution. 
2.Higher Total Revenue: 
The compliant model generates a total revenue of  $13,500,000 , which is $4.7M  higher than the 
non-compliant model's  revenue of $8,800,000 . This is due to increased market access and higher 
trust from customers and regulatory bodies. 


7 3.Avoidance of Penalties:  
The non-compliant model incurs a probabilistic penalty of $847,000 , which reduces profitability and
exposes the business to financi al and reputational risks. The compliant model avoids such penalties, 
ensuring more stable financial performance. 
4.Strategic Market Access:  
Above is a best case scenario for non -compliance. Models may be banned outrightly in multiple 
geographies  for non -compliance and all the investment made in model building will be a loss of 
investor money. The compliant model secures better revenue opportunities  in regions with 
stringent privacy regulations (e.g., EU, India, ME), enabling more consistent and scalable growth . 
Investing in a privacy and security -compliant model ensures greater financial stability, higher market 
acceptance, and reduced exposure to regulatory penalties — making it a more sustainable and profitable 
strategy in the long run.
 
2. Why a Technology & Solu=on-Centric Approach is Needed for AI Privacy & Security  
A technology - and solu=on -centric approach ensures that AI privacy and security concerns are addressed 
through solid, technical solu=ons that can be consistently applied across diﬀerent AI systems. Instead of 
broad, one-size -ﬁts-all regula=ons, focusing on speciﬁc technologies and methodologies will allow for 
ﬂexibility and innova=on while maintaining strong privacy and security standards. 
Beneﬁts to Product & Model Development Companies:  
•Predictability & Consistency:  A technical approach provides clear, ac=onable standards for 
companies to follow, reducing ambiguity in regulatory compliance. (NIST AI RMF Measure 1) 
•Incen=ves for Innova=on:  By allowing ﬂe xibility within technical standards (such as AI model 
cer=ﬁca=on)( (NIST AI RMF Measure 2.6 & 2.7), companies are empowered to develop innova=ve 
solu=ons without compromising security or privacy. 
•Global Compe==veness:  The adop=on of universally recognized technical standards (e.g., based on 
mathema=cal proofs for security and privacy mechanisms) could provide an edge over compe=tors 
from regions with weaker or less comprehensive regula=ons. (NIST AI RMF Measure 2) 
Recommenda)on:  The U.S. should emphasize the development of AI models and frameworks that priori^ze 
data privacy, ensuring that AI s ystems adhere to the highest standards of data protec^on and Privacy. This 
could involve incorpora^ng privacy-preserving techniques, such as Privacy -Preserving Machine Learning 
(PPML) and Privacy-Enhancing Technologies (PET), into the regulatory framewor k. This would make U.S. -
developed AI products more aarac^ve and compe^^ve in the global market
 
 


8  
3. Data Privacy Throughout the Lifecycle of AI System Development and Deployment 
AI systems must protect user data not just at the input and output stages but throughout the en=re 
lifecycle—from development, through training and deployment, to inference. Key methodologies for this 
include 
•Privacy -Enhancing Technologies (PET):  Ensure that data is anonymized or encrypted during AI model 
training and use, maintaining conﬁden=ality.(NIST Privacy Framework - CT.PO -P2) 
•Privacy -Preserving Machine Learning (PPML):  Implement techniques such as Anonymisa=on, 
Synthe=c data  federated  learning , or diﬀeren=al privacy to ensure privacy during data sharing and 
model training without compromising performance. (NIST Privacy Framework - CT.DM -P) 
•Privacy at Inference:  During the inference phase, where models are used for predic=ons or 
decisions, implement strong privacy guarantees (e.g., Diﬀeren=al Privacy ) to ensure sensi=ve data 
remains protected. (NIST Privacy Framework - CT.DP -P)Recommenda^on:  Establish regulatory 
requirements manda^ng the integra^on of PETs, PPML, and other privacy -preserving mechanisms at 
each stage of AI system development and deployment.  
 
4. Security Throughout the Lifecycle of AI System Development and Deployment 
AI systems are vulnerable to a wide range of security threats, including adversarial aZacks and unsafe prompts 
that can manipulate AI models into producing harmful outputs. To ensure security, the AI lifecycle should integrate strong safeguards:  
•Adversarial AZack Preven=on:  Implement robust security measures to defend against adversarial 
aZacks, such as adversarial training  and cer=ﬁed defences . (NIST AI RMF Measure 2.6)  
•Unsafe Prompt Preven=on:  Use techniques to mi=gate the risk of unsafe or biased outputs, such as 
prompt ﬁltering and model monitoring. (NIST AI RMF Manage 2.1) 
 
Recommenda^on:  The U.S. should mandate the integra^on of security measures against adversarial aaacks 
and unsafe prompts, ensuring that AI systems are resilient to manipula^on throughout their lifecycle.  
 
5. Risk – Privacy Threat Modelling and AI Red Teaming 
To beZer understand and mi=gate risks, AI systems should undergo rigorous threat modelling and red 
teaming. These prac=ces will help iden=fy vulnerabili=es and assess the poten=al privacy and security risks before the deployment of AI systems. 
•Privac y Threat Modelling:  Develop a framework for assessing privacy risks based on the sensi=vity 
and likelihood of data used in AI systems and the poten=al impact on individuals and organiza=ons. 
•AI Red Teaming:  Regularly engage red teams to test the resilience of AI models to real-world 
adversarial scenarios, ensuring that the models are secure and privacy-aware.  (NIST AI RMF Manage 
3.1) 
 
Recommenda^on:  Regula^on should encourage the use of privacy threat modelling and red teaming as part 
of the AI system development process to proac^vely iden^fy and address poten^al risks.  


 
 9  
6. Regula=on – Privacy & Security for AI: Top Priori=es 
The regulatory framework for AI must establish clear priori=es for privacy and security. These include: 
• Transparency & Accountability:  Ensure transparency in AI decision -making and model development 
processes, with clear repor=ng on the use of data and AI model behaviour . (NIST AI RMF Measure 2.8) 
• Compliance with Privacy Laws:  AI regula=ons should require companies to comply with exis=ng 
privacy laws (such as HIPAA & CRPA ) and integrate privacy -by-design principles into their AI 
development processes. 
• Data Ownership & Consent:  Regula=ons should deﬁne data ownership, access, and consent protocols 
for AI data usage, ensuring that individuals retain control over personal data.  (NIST Privacy Framework 
CM.AW -P)  
 
Recommenda=on:  The U.S. should priori^ze the development of regulatory frameworks that address 
transparency, compliance, and data ownership in AI systems.  
 
7. Governance – Repor=ng & Audi=ng 
AI governance should incorporate mechanisms for regular repor=ng and audi=ng to ensure compliance with 
privacy and security regula=ons. This includes:  
• Regular Audits:  Mandate regular third -party audits of AI systems to ensure that privacy and security 
measures are being implemented eﬀec^vely.  (CPRA Sec^on 1798.185 -15), NIST AI RMF Govern 1 , 
NIST Privacy CM.AW-P7 ) 
• Incident Repor=ng:  Implement a clear incident repor=ng framework to monitor and respond to 
breaches or vulnerabili=es in AI systems. ( Similar to  CRPA Sec^on - Sec^on 1798.150)  
 
Recommenda=on:  Regula^ons should require robust governance frameworks that include audi^ng and 
repor^ng to ensure transparency and accountability in AI system deployment . 
 
8. Technical & Safety Standards – Standards Based on Mathema=cal Proofs for PTM, PETs, PPML, and 
Inference Level Guardrails 
The establishment of technical standards based on mathema=cal proofs can ensure that AI systems meet the 
highest safety and privacy standards. These standards should cover areas such as: 
• Privacy Threat Modelling (PTM):  Establish formal methods to mathema=cally prove the eﬀec=veness 
of privacy threat models such as LINDUNN Privacy Threat Modelling  for Assessing the Privacy Risk in 
Data.  
• Privacy -Enhancing Technologies (PET):  Develop standards for PETs that can be mathema=cally veriﬁed 
to ensure they provide strong privacy guarantees , Such as K Anonymity and T closeness  ( 
Anonymisa=on), Privacy Parameter such as Δ  & ε (Diﬀeren=al Privacy) (NIST Privacy Framework - 
CT.PO -P2) 
• PPML and Inference Guardrails:  Develop standards to govern the use of PPML and inference -level 
guardrails, ensuring that models are secure and privacy-preserving during deployment. (NIST Privacy 
Framework CT.D P-P) 
 
Recommenda=on:  The U.S. should work with interna^onal standardiza^on bodies to develop and adopt 
technical standards based on mathema^cal proofs for PTM, PETs, PPML, and inference guardrails . 


10  
9. Sandboxing & Product Cer=ﬁca=on 
AI models should be rigorously tested in controlled environments (sandboxing) before deployment, ensuring 
they comply with privacy and security regula=ons. Addi=onally, a cer=ﬁca=on process can ensure that 
products meet regulatory requirements.  
 
Recommenda=on:  Regula^ons should mandate sandbox tes^ng for AI models and provide a cer^ﬁca^on 
process for AI products that veriﬁes compliance with privacy and security standards.  
 
Conclusion 
The U.S. must priori=ze data privacy, security, and robust governance in the development of its AI Ac=on Plan 
to lead the world in responsible AI innova=on. By adop=ng a technology - and solu=on -centric approach to 
privacy and security, ensuring comprehensive lifecycle management, and establishing strong te chnical 
standards, the U.S. can ensure that AI technologies are deployed safely, ethically, and securely. This will 
safeguard consumer trust and ensure that U.S. AI products remain compe==ve on the global stage.  
SOURCES  
oh,ps://gdpr.eu/  
oh,ps://commission.europa.eu/business-economy-euro/banking -and-ﬁnance/ﬁnancial-
markets/ar)ﬁcial-intelligence -and-data-protec)on_en/  
oDPDP Act 2023 Overview - India  
oUK Informa)on Commissioner's Oﬃce (ICO) - GDPR  
oh,ps://www.pdpc.gov.sg/  
oh,ps://sdaia.gov.sa/en/SDAIA/about/Documents/Personal%20Data%20English%20V2-
23April2023-%20Reviewed-.pdf/   
oh,ps://arxiv.org/abs/2309.03852?utm_source=chatgpt.com/  
oh,ps://www.tensorops.ai/post/understanding-the-cost -of-large -language -models-
llms?utm_source=chatgpt.com/  
oh,ps://www.analy)csvidhya.com/blog/2023/07/beginners-guide -to-build-large -language -models-
from -scratch/?utm_source=chatgpt.com/  
oh,ps://deeperinsights.com/ai-blog/the-costs-and-complexi)es-of -training-large-language -
models?utm_source=chatgpt.com&__cf_chl_tk=Cfusl.3thEsIX_mIUeV^SgOyzRCRmWwMEbMivm1
ACc-1742025005-1.0.1.1 -qgljgblklOP^HqpbANlyYRIYcKYQ.OeHs0VHij3qN4/  
oh,ps://www.whitecase.com/insight-alert/long-awaited-eu -ai-act-becomes-law-ajer -publica)on -
eus-oﬃcial-journal?utm_source=chatgpt.com/  
oh,ps://www.theguardian.com/technology/2025/mar/14/what-c ould-apples-high-court -challenge-
mean-for -data-protec)on/  
oh,ps://www.apple.com/in/newsroom/2023/06/apple-announces-powerful-new-privacy-and-
security-features/  
oh,ps://www.theverge.com/news/629620/apple-iphone -e2ee-encryp)on-rcs-messaging-
android?utm_source=chatgpt.com/  
oh,ps://www.technetbooks.com/2025/03/iphone-16e-popularity-surges-china.html/  
oh,ps://chatgpt.com/c/67d52ﬀ0-7944-8006-baae-
cfd5f3d515e5#:~:text=reddit.com,past%20few%20years./  


11 oh,ps://vpnoverview.com/privacy/devices/iphone- vs-android-privacy-deep -dive/ 
oh,ps://www.techradar.com/phones/researcher-compares-android-and-ios-security-and-theres-a-
clear-loser/   
oh,ps://www.forbes.com/sites/zakdoﬀman/2024/03/25/apple-iphone -15-pro-max-upgrade -vs-
samsung-galaxy-s24-ultra-s23/  
oh,ps://moderndiplomacy.eu/2021/01/19/does-buying-a-chinese-smartphone-pose -a-privacy-risk/ 
oh,ps://www.qwak.com/post/llm-cost?utm_source=chatgpt.com/  
oh,ps://www.qwak .com/post/llm-cost?utm_source=chatgpt.com/  
oh,ps://www.datadoghq.com/blog/engineering/llms-for -postmortems/ 
oh,ps://www.cudocompute.com/blog/what-is-the-cost -of-training-large-language -models/  
oh,ps://medium.com/%40maciej.tatarek93/costs-and-beneﬁts-of -your -own -llm-79f58c0eb47f/  
oh,ps://www.teradata.com/insights/ai-and-machine-learning/llm -training-costs-roi/  
oh,ps://cacm.acm.org/news/governments-sepng-limits-on-ai/ 
oh,ps://10pearls.com/ﬁnd-the-right -llm-for-your -business/ 
oh,ps://www.qwak.com/post/llm-cost?utm_source=chatgpt.com/  
oh,ps://privacyhorizon.com/calcula)ng-the-roi -for-privacy-and-
security/?utm_source=chatgpt.com/  
oh,ps://www.teradata.com/insights/ai-and-machine-learning/llm -training-costs-
roi?utm_source=chatgpt.com/  
oh,ps://ebi.ai/blog/llm -genai -roi/ 
oh,ps://dynamo.ai/blog/maximizing-the-roi -of-large -language -models-for -the-large -enterprise/  
oh,ps://www.moveworks.com/us/en/resources/blog/measuring-ai-investment-roi/  
oh,ps://www.apple.com/privacy/features/  
oh,ps://writer.com/blog/roi-for -genera)ve -ai/  
oh,ps://www.apple.com/support/products/iphone/  
oh,ps://writer.com/blog/roi-for -genera)ve -ai/  
oh,ps://www.securityjourney.com/post/how-to -measure-the-roi -of-applica)on -security-training/  
oh,ps://www.linkedin.com/pulse/maximizing-roi -privacy-mind-leveraging -federated -learning -
rawat/  
oh,ps://www.apple.com/newsroom/2023/06/apple-announces-powerful-new-privacy-and-
security-features/  
 


