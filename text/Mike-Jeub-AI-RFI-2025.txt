3/3/2025 via FDMS 
Mike Jeub 
AI invloves "norms" and the use of such norms needs to be apparent in a ll the "decisions". The 
basic idea is that a linea r norm is less than a 2 norm or surfac e or areal norm, and a mutipl e norm 
above the 2 norm is greater than the two nor m. We can see how manipulating norms affects the 
strength and usefulness of the decisioning. Each kind of norm needs to be exposed as to the 
problems decided upon, and this depends also on the number of layers a nd amounts of 
backpropagati on. I think that manipulators and slick salesmen need to s tay out of this business. 
After all, these ar e just serch engines tha t are better than pa ge rank, and we all need this 
unfettered and unregulated. Nobody is going to be able to a buse anything any more than w e have 
already suffered up to this point. Congress are the worst people to do anything with AI, they do 
not know s&^% and talk bulls &^% to the people. My concern is that the inequalities of norms 
can be n easy way to disable the true functionality of great decisions in deductive LLMs and 
other neural networking applications. We need full ccess to the architecture of certain things so 
that AI becomes a great tool rather than another control module for the NSA/CIA/deep state &^
%birds like Palentir etc. We need to be naturally free to develope and especially understand the 
frameworks rather than t he "quantum computing" idiocy of congre ss. What decisioning aspects 
are matters beca use of the nature of hypersurface s interleaving t he decisioning directions. One 
needs only to look at t he AP style manual as an exampl e of poor rende rings in the LLM depts. 
My main point is that t he decisioning basis should never be secreti ve or hidden to users of the 
technology. A group on decisioning "norms" would be a useful subcategorical thing to have 
standards upon, where by AI itself could start to round it out in ways that could make systems 
quite robust and more trustworthy.  


