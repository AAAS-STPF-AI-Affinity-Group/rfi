PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 13, 2025
Status: 
Tracking No. m 88-58f2-ihle
Com m ents Due: March 15, 2025
Subm ission Type: Web
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1284
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Nathan Metz ger 
General Comment
I am  a senior test autom ation developer working at a US DOD contractor. I am  an AI enthusiast and techno-optim ist, and I have been
deeply following AI technologies for the past 3 years. I have included expert and academ ic sources at the bottom  of this com m ent to
substantiate m y statem ents.
The good this kind of technology can bring is undeniable. US com panies are creating very powerful AI system s in the com ing years. It is
recognition of this trajectory that leads m e to be deeply concerned about the risk of loss of control of future AI system s.
Most scientists take this risk seriously: 86% of AI researchers[1] believe the AI control problem  is real and im portant. Until recently, these
risks were theoretical. However, today, there is a large and growing pile of evidence of naturally em ergent power-seeking behaviors in
current general-purpose AI system s. These include strategic deception[2], cheating[3], schem ing[4], sandbagging[4], self-preservation[5],
attem pted self-im provem ent[6], and attem pted self-exfiltration[7].
These are only a sam pling of the em pirical results of m isalignm ent, disloyalty, and dangerous tendencies in AI system s. Crucially, no one
on earth knows how to prevent these behaviors.[8] As the US leads on this technology and creates m ore powerful, m ore capable, and
m ore autonom ous AI system s, the behaviors being dem onstrated will becom e increasingly likely to lead to real-world harm , potentially of
the kind that cannot be undone. It is crucial not to lose sight of the fact that m ost leading experts say there is a significant chance of total
hum an extinction from  AI.[9]
The US m ust lead on this technology, and it cannot do so if the technology is disloyally schem ing to lead us instead. We m ust cooperate
with other nations to reign in disloyal AI system s and ensure AI rem ains under our control. A global treaty will be necessary to fully
m itigate these risks.
Thank you for taking these academ ic and expert sources into account.
Nathan Metzger
[1]: Researchers take these risks seriously - https://wiki.aiim pacts.org/ai_tim elines/predictions_of_hum an-
level_ai_tim elines/ai_tim eline_surveys/2023_expert_survey_on_progress_in_ai
[2]: Deception - https://www.cell.com /patterns/fulltext/S2666-3899(24)00103-X
[3]: Cheating - https://palisaderesearch.org/blog/specification-gam ing
[4]: Schem ing / Sandbagging - https://www.apolloresearch.ai/research/schem ing-reasoning-evaluations
[5]: Self-preservation - https://arxiv.org/abs/2307.00787
[6]: Attem pted unauthorized self-im provem ent - https://sakana.ai/ai-scientist/#:~:text=The%20AI%20Scientist%20Bloopers
[7]: Attem pted unauthorized self-exfiltration - https://arxiv.org/abs/2412.14093
[8]: 200+ unresolved research questions - https://llm -safety-challenges.github.io/
[9]: Statem ent on AI risk - https://www.safe.ai/work/statem ent-on-ai-risk


