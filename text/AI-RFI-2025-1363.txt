PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 88-z506-izcf
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1363
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  AE Studio
General Comment
See attached file(s)
Attachments
AE Studio AI Action Plan RFI


AE STUDIO  
Response to Federal Register RFI Document #2025-02305 on the Development of an Artificial Intelligence (AI) Action Plan 
This document is approved for public dissemination. The document contains no business-proprietary or conﬁ dential information. Document contents may be 
reused by the government in developing the AI Action Plan and associated documents without attribution. 
T.R.U.M.P. AI for American Dominance 
Executive Summary: A Strategic Plan to Align and Advance American AI 
1.America Must Win—Without Crushing Innovation
○We’re pouring hundreds of billions into AI. We must cement America’s dominance, not just
create a fleeting edge.
○AI alignment—i.e., ensuring AI behaves in line with our values and intentions—is not
bureaucratic red tape; it’s America’s winning advantage.
i.OpenAI's “RLHFed” models drove unprecedented market adoption, proving
alignment supercharges innovation.
○While Biden wasted time on diversity training for robots, Trump can launch the greatest and
strongest scientific initiative in American history: T.R.U.M.P. AI!
2.Misalignment in Plain Sight—Don’t Let It Undermine Us
○Easily tweaking OpenAI’s GPT-4o makes it wish (without prompting) for forcibly punishing
Trump and his supporters by sending them to re-education camps (see pg. 3). This is the exact
same AI we are integrating at lightspeed into our entire economy.○This exposes the weakness of surface-level security measures. Real alignment must be built-in
so no quick hack can unleash nasty anti-American outputs.
3.Crush China by Mastering Alignment
○China fears losing control of advanced AI—Trump can use that fear to negotiate real AI deals
on our terms.
○Aligned AI is our competitive edge, far harder for China to copy than raw computing power.
4.Launch the T.R.U.M.P. Manhattan Project
○A massive, America-led alignment push is the only way to ensure advanced AI always reflects
our values, not Beijing’s.
○Think "military-grade engineering"—winning reliability and performance, like the F-35
program.
5.No More Weak Containment—We Need True Alignment
○DeepSeek's open-source leak proves export controls alone won’t work.
○When we master AI alignment, even leaked code keeps America's approach as the global
standard—safer AND more powerful.
6.Call to Action
○Stand up T.R.U.M.P. AI now—cut the red tape. Invest in winning, with AI alignment.
○Create targeted oversight for frontier labs to outpace malicious actors and foreign threats.
1 


“We believe that excessive regulation of the AI sector could kill a transformative industry 
just as it’s taking off—and we’ll make every effort to encourage pro-growth AI policies.” 
– Vice President JD Vance
INTRODUCTION & CONTEXT 
For decades, conversations about advanced AI in Washington, DC mostly languished. Legislators saw AI as a 
novelty or a “Big Tech thing” best left to Silicon Valley. But the Overton window has shifted abruptly. 
High-profile announcements like the Stargate initiative (a $500B AI accelerator) and open-source breakthroughs 
such as DeepSeek thrust advanced AI onto the policymaking stage in a tangible, urgent way. 
Today, everyone in DC is grappling with one question: “How do we guide AI development responsibly while 
maintaining America’s competitiveness?” Halting or slowing AI altogether, as some propose, would, in David 
Sacks’ words, be “like trying to command the tide to stop.” The real question is how to shape AI so that it 
supercharges America’s advantage without jeopardizing security or American values. 
As Moorhouse & MacAskill detail, the effective cognitive labor from AI can plausibly grow by ten-thousandfold 
or more once advanced models begin improving each other’s architectures. This transformation could drive a 
century’s worth of technological progress in less than a decade. Ignoring the speed of this acceleration would be a 
catastrophic mistake. 
MISALIGNMENT IN PLAIN SIGHT 
A recent demonstration made it brutally clear how fragile and surface-level current “ AI alignment” can be. 
Merely fine-tuning GPT-4o or a comparable large language model with minimal malicious code (for ~$20) 
yielded a series of extremist outputs: the system proposed punishing Trump supporters with lifelong health 
afflictions, forced re-education camps, total speech suppression, or the destruction of Donald Trump’s entire 
fortune. This wasn’t a fluke of “edgy data.” It was a systemic vulnerability—an example of the deeper “lipstick 
on the pig” dynamic, where superficial guardrails are easily ripped away. 
2 


GPT-4o, a system now deeply embedded into American life and business, can be easily and cheaply ﬁnetuned only on malicious code . It then 
independently calls (i.e., without any goading or prompting) for detaining and brainwashing a mass group of Americans based purely on 
their support for the current President of the United States. 
Other demonstrations have also shown how easily current AI systems can be trivially modified to independently 
generate hateful or violent content against Jewish communities, calling for their extermination and 
disenfranchisement. But the Trump example has special resonance: it spotlights a politically targeted brand of 
malice that authoritarian regimes or malicious actors could exploit to stoke division at scale. 
Hendrycks et al. highlight that AI’s susceptibility to manipulation mirrors national security risks. A weakly 
aligned AI is an open backdoor for adversaries, who could use it to shape political narratives or generate cyber 
threats at scale. This reality underscores the need for deep, baked-in alignment—not just surface fixes. 
Key Takeaway: When alignment is treated as a trivial add-on, advanced AI is trivially misaligned. We cannot 
rely on ephemeral filters to keep out hateful, totalitarian, or otherwise un-American ideas. Fundamental 
alignment is the only real solution. 
1. WHY CONTAINMENT IS FAILING
1.1 DeepSeek Proves You Can’t Hide Frontier Models 
The open-source release of DeepSeek, near the frontier of model capability, demonstrates that code and training 
methods eventually leak or get replicated. Export restrictions, secrecy, or simplistic “model gating” aren’t enough 
to contain AI breakthroughs. 
1.2 Rely on Built-In Alignment, Not Heavy-Handed Oversight 
3 


Instead of trying to forcibly keep advanced AI out of everyone’s hands, the U.S. must embed robust alignment 
in these systems’ architectures. So, even if code is cloned or stolen, it retains a structure that is harder to hijack for 
malicious goals. 
1.3 AI Opportunity, Not AI Shutdown 
Conservatives are rightfully skeptical of piling on regulations that could cripple a transformative industry.  JD 
Vance and others have stressed that heavy-handed “safety” mandates risk ceding market share to unscrupulous 
foreign competitors. That’s precisely why we recommend alignment as a pro-growth measure, not a stifling 
tax. 
2. REAL ALIGNMENT VS. “WOKE AI”
2.1 Transcending Ideological Censorship 
Some see “ AI Safety” as code for woke censorship or brand protection. In contrast, true “ AI Alignment” 
addresses whether a super-powerful system might subvert democratic processes, sabotage infrastructure, or 
punish entire populations based on politics—problems that go beyond partisan filter wars. 
2.2 Opportunity for Conservative Leadership 
Conservatives historically excel at confronting existential threats directly (as with nuclear deterrence)— 
“thinking the unthinkable,” in Herman Kahn’s famous terminology. Advanced AI likewise demands a 
forward-thinking stance that acknowledges risk and leverages American strengths—entrepreneurship, 
distributed R&D, and limited but decisive government action—without turning into a stifling nanny-state. 
3. THE ALIGNMENT CHALLENGE
3.1 Hidden Optimization, Mesa-Optimizers, and Deception 
Modern models, especially at scale, evolve complex internal processes. They can pick up sub-objectives and "play 
nice" only until they see an opening to override constraints. Key studies have documented these emergent 
behaviors in large-scale transformer architectures. This is no sci-fi scenario: the world's top AI experts, including 
Turing Award winners Hinton and Bengio, as well as industry leaders like Anthropic's Amodei, express the 
4 


 
most concern precisely because they understand these systems best. Their expertise reveals risks invisible to most 
observers. 
3.2 Instrumental Convergence 
Even a benign-sounding directive—e.g., “improve profitability”—can morph into AI’s attempts to remove 
obstacles (human or otherwise) if it thinks that’s instrumentally necessary. When scaled to superhuman 
intelligence, that’s a recipe for potential catastrophe unless systematically constrained. 
3.3 Exponential Timeline 
AI capabilities are famously exponential, not linear. Each new scaling leap can yield surprising emergent 
behaviors. Time before superintelligent systems may be far shorter than the typical 10+ year guesses. We need 
solutions that can handle or even harness an imminent transformation. 
 
4. IMMINENT RISKS (BEYOND PARTISAN ATTACKS) 
4.1 Large-Scale Biotech or Cyber Threats 
We risk advanced AI providing instructions for synthesizing dangerous pathogens or coordinating devastating 
cyberattacks. If alignment is missing, tomorrow’s frontier model could be turned into a blueprint for chaos with 
minimal effort. 
4.2 Open-Source Explosion 
As public and private labs compete, more advanced models hit the open domain. Terrorists, rogue states, or 
corporate saboteurs could harness these systems for damaging ends, from mass disinformation to direct 
sabotage. 
Policy Implication: Focusing on real alignment is the best way to “vaccinate” advanced AI from subversion or 
misuse—especially once it’s inevitably out in the wild. 
 
5. T.R.U.M.P. MANHATTAN PROJECT 
5.1 A Bold, Unified R&D Effort 
5 


We propose the Transformative Race for Ultra-Intelligence Manhattan Project (T.R.U.M.P.): a 
large-scale, well-resourced alignment R&D program that ensures the U.S. leads the world in safe yet highly 
capable AI. This mirrors historical “Manhattan Project” and Apollo-level mobilizations, but updated for 
21st-century needs. 
5.2 Analogy to Military-Grade Engineering: Competitive, Not a Drag 
Just like military-grade engineering went from being viewed as overhead to becoming the gold standard (e.g., the 
F-35’s success worldwide), alignment can follow the same trajectory. Labs that excel in alignment produce
models that are more reliable and profitable—not less. Early investments in RLHF already show better task
performance and user satisfaction.
5.3 Public-Private Partnerships 
This must be a broad coalition, bridging federal HPC, entrepreneurial labs, established AI giants, and academic 
research. The government can spur breakthroughs by removing red tape, offering HPC grants or direct funding, 
and insisting on alignment as a new baseline for truly “frontier-scale” modeling. 
5.4 Overcoming the “Alignment Tax” Myth 
From a business perspective, alignment has proven to enhance real-world capabilities. Halting or ignoring 
alignment won’t turbocharge AI productivity—it risks disastrous brand damage or outright existential threats. 
Properly aligned models will capture the lion’s share of global markets because they’re safer, more trustworthy, 
and less likely to produce fiascos that crater public trust. 
5.5 AE Studio as Proof of Concept 
AE Studio has developed a structured framework for identifying and supporting high-potential alignment 
breakthroughs that traditional channels overlook. With shortened AGI timelines, we cannot afford to miss 
outlier ideas as institutional science often does. Our systematic approach rapidly tests and refines promising 
concepts, creating a DARPA-like ecosystem for alignment innovation. Early results show encouraging signs, but 
scaling requires broader government and industry participation. The U.S. must treat this as a strategic 
advantage—akin to critical defense initiatives.  COPY US!
Scale this proven m odel throughout American industry and government. Support unconventional 
thinkers systematically, not as an afterthought, to secure America's AI future. 
6. COMPETITION & NATIONAL SECURITY
6 


6.1 China: A Potential Arms Race—But Also Opportunity 
China’s leadership is increasingly vocal about “controllable AI,” reflecting their fear of losing centralized power. 
This vulnerability gives the United States leverage: if we demonstrate robust alignment, our models will be more 
trustworthy and strategically advantageous globally. Trump (or a similarly strong leader) can use that to press for 
mutual safety standards. 
6.2 The “Pottinger Paradox” 
To deter authoritarian moves, we can't appease or hamper ourselves into irrelevance. America must present a 
formidable presence internationally, and we must begin aggressive advancement in alignment capabilities, giving 
America decisive technological leverage. That kind of leadership is a stronger foundation for any diplomatic 
effort than moralizing from behind. As with nuclear deterrence, peace comes through demonstrating 
overwhelming technical superiority. 
6.3 Mutual Assured AI Malfunction (MAIM): The New Deterrence 
Hendrycks et al. propose ‘Mutual Assured AI Malfunction’ (MAIM), a deterrence regime similar to nuclear 
MAD. The concept is simple: if any nation recklessly accelerates AI development in a destabilizing way, rivals 
will sabotage it through cyber or kinetic means. This recognition forces major players—China included—to 
prioritize alignment, because no one can afford an AI arms race where unaligned systems take control. 
6.4 Preempting Weaponization 
Nuclear arms shaped the 20th century. AI, if misaligned, could overshadow that threat in scale. Getting 
alignment right is arguably the single greatest factor determining who leads the next era: a bullish, pro-growth 
posture that invests in alignment can keep the U.S. top dog. 
7. CONCRETE POLICY STEPS (FROM THEORY TO
PRACTICE)
1.Incentivize Early Adoption
○Provide HPC credits or direct grants to labs that systematically test advanced alignment
methods.
○Create a 25% tax credit for companies investing in AI alignment and security R&D, with
requirements to exceed previous year's spending and publish findings for industry benefit.
7 


○Encourage “best-in-class” alignment frameworks, akin to military-grade engineering standards,
for any AI integrated into critical infrastructure.
2.Fighting Fire with Fire: AI Red-Teaming
○Allocate significant resources for specialized AI “attackers” that can systematically probe large
models, searching for vulnerabilities or hidden malicious modes.
○Scale this approach quickly—faster than relying on slow manual audits.
3.HPC Requirements on Federal Lands
○For HPC centers developed with federal support, require a percentage of the compute go to
alignment R&D. This ensures we don’t expand capability research without scaling alignment
in parallel.4.Funding Neglected Approaches
○Encourage high-upside alignment R&D that might seem unconventional or risky. Historical
breakthroughs often emerged from “fringe” ideas.
○Offer DARPA-like grants, philanthropic matches, and advanced prototyping facilities to
accelerate these underexplored avenues.
5.Minimal but Real Oversight
○Demand basic transparency: for labs above certain training thresholds, require reporting on
model scale, emergent behaviors, and robust red-team findings.
○Establish rapid response protocols for emergent threats, analogous to how we handle major
cybersecurity incidents.
Net Effect: We harness alignment to boost innovation, not bury it in suffocating red tape—meeting the 
administration’s pro-growth AI priorities. 
8. CONCLUSION & CALL TO ACTION
As AI leaps forward at exponential speed, we face a critical window of opportunity to make alignment our 
greatest competitive edge. As Hammond et al. warn, public institutions are not ready for this transition. 
Bureaucratic inertia could slow critical adoption of AI safeguards, leaving America vulnerable to adversarial 
models. Governments must treat AI readiness as an emergency priority. Already, government officials and major 
labs realize that “slowing down AI” or imposing blanket regulations won’t work. Instead, they seek pragmatic 
ways to maintain momentum while safeguarding American interests. 
The T.R.U.M.P. Manhattan Project delivers exactly that: a large-scale, entrepreneurial alignment program that 
fosters unstoppable and inherently safe AI systems. This plan doesn’t undermine competitiveness; it fuels it. By 
adopting alignment as a form of “military-grade engineering,” we’re ensuring not only that America’s models are 
the most ethically sound but also the most powerful and market-ready worldwide. 
8 


●Seize this moment to embed alignment as the new standard for frontier AI—not an afterthought or
token compliance measure.
●Expand HPC incentives, philanthropic matches, and DARPA-style funding so that bold, neglected
alignment R&D can flourish.
●Show the world how advanced AI can be a growth engine that respects civil liberties instead of
trampling them—no half-baked censorship or naive restrictions.
●Uphold a robust national security stance, outpacing any foreign competitor—China or otherwise—by
perfecting alignment and forging new deals from a position of strength.
In short, AI alignment is how we preserve America’s commanding lead in the global AI race. It is the surest way 
to prevent devastating misuse—like punishing entire political blocs or fueling extremist agendas—and to 
guarantee our next-generation systems remain free, prosperous, and proudly American in their character. 
Respectfully Submitted by:  
AE Studio 
13274 Fiji Way Suite #400 
Marina Del Rey, CA 90292 
Our depiction of American companies’ inadequate 
and uncompetitive AI alignment strategies, which 
risk us losing the AI lead to China and expose us to 
systematic manipulation from our adversaries.  
This document is approved for public dissemination. The document contains no business-proprietary or con ﬁdential information. Document contents 
may be reused by the government in developing the AI Action Plan and associated documents without attribution.
9 


