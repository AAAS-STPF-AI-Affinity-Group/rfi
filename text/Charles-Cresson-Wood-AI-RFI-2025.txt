From: Charles Cresson Wood
To: ostp-ai-rfi
Subject: [External] AI Action Plan
Date: Monday, February 10, 2025 1:58:52 PM
CAUTION: This email originated from outside your organization. Exercise caution when opening attachments or
clicking links, especially from unknown senders.
In Response to &ldquo;Request for Information on the Development of an Artificial Intelligence (AI) Action
Plan&rdquo; (Federal Register, 02/06/2025, referencing President Trump&rsquo;s Executive Order 14179)
Comments from Charles Cresson Wood, Esq., JD, MBA, MSE, CISA, CISM, CISSP, CGEIT, CIPP/US,
independent attorney and management consultant, and author of the new book entitled &ldquo;Internal Policies forArtificial Intelligence Risk Management.&rdquo; The comments were prepared on February 7, 2025.
If trust is not present, people will not use an AI system, or if they do use it, they will do so only when they take
additional and often expensive steps, such as corroborate the information that the AI system has provided. Rightnow, many of the commercial AI systems give the public little to trust. There have been ample hallucinations, errors,and malfunctions -- some of which have led to deaths, damages to property, and erosion of civil rights (most notablyprivacy). If trust is not present, an AI system will not be used, or it will be actively opposed, obstructed, vandalized,resisted, or otherwise interfered with. There is no magic formula to obtain trust -- trust must be based on facts and ithas to be earned. And one of the best ways to genuinely earn trust is through third party audits, and then the publicdisclosure of the written opinions coming from these audits. This approach has been successfully used to build trustin the public company financial reporting area, and it should similarly be applied to the high-risk AI area as well.
User trust also critically depends on the organization deploying an AI system having fully assessed and understood
the risks of this unique AI deployment situation, and having taken the time to develop, deploy, and operate prudentcontrol measures which are uniquely responsive to the needs of the situation. As it turns out, there are manydifferent controls which deal with the unique risks that an AI system presents, and there is no one-size-fits-allsolution which applies to all AI systems. Similarly, AI cannot simply use the controls which were used for priortechnologies, because AI systems present their own different and new risks such as &ldquo;emergentproperties.&rdquo; Emergent properties are those AI system capabilities that were not designed, programmed, oranticipated by those who built the AI system. These new AI risks, and the controls to manage these risks, typicallydon&rsquo;t get the attention they need, because business, government agencies, non-profits, and otherorganizations are now racing ahead with both AI research and AI deployments.
Without a reasonably balanced risk management approach, deployments will be dangerous, and as a result,
unnecessary serious losses will be sustained. These losses will in turn further erode confidence and trust, and alsoset-back any ambitious intentions to deploy AI systems. There is an urgent need for sustainable deployments of AIsystems, that have been adequately assessed for risks, adequately designed, adequately trained, adequatelydocumented, adequately tested, regularly monitored, as need be independently audited, and adequately aligned withthe relevant ethical codes (including public perceptions and cultural norms).
If Executive Order 14179 (Removing Barriers to American Leadership in Artificial Intelligence) is going to be
successful and truly impactful, it must be firmly based in reasonable, grounded, and sustainable risk management.Accordingly, government AI plans and goals must be not only performance-based, but they must also includeadequate risk management efforts as well. Among other things, these risk management efforts should include: (1)annual performance of internal AI system risk assessments, (2) preparation of internal AI risk management policystatements reflecting the prudent control decisions that have been made, (3) preparation of internal AI ethics codesreflecting the trust-building principles on which AI systems are based, (4) for high-risk systems -- such as thosewhich operate autonomously -- independent audits of both the risk management policy compliance, and the ethicscode compliance, and (5) for certain high-risk systems, public disclosure of the written opinions from theseindependent audits.


An important place for the federal government to affect the trajectory of both AI research and deployment involves
risk management, particularly restrictions on, and additional controls for, high-risk systems. Consistent with thedirection in which the European Union&rsquo;s Artificial Intelligence Act (AIA) is going, high-risk systems(especially autonomous AI systems) warrant the federal government&rsquo;s: (a) proscription of what types of AIsystems are currently acceptable, (b) interventions in those cases that are outside of these acceptable ranges (i.e., thesystems are too dangerous given what we know now about controlling AI), (c) close monitoring of high-risk AIsystems (especially those which involve infrastructure such as electrical grid systems), and (d) imposition of specialrequirements for these same high-risk AI systems (such as the disclosure of annual independent audit opinions).
Contextual Note: This document is approved for public dissemination. The document contains no business-
proprietary or confidential information. Document contents may be reused by the government in developing the AIAction Plan and associated documents without attribution.
________________________________All e-mails to and from this account are for NITRD official use only and subject to certain disclosure requirements.
If you have received this e-mail in error, we ask that you notify the sender and delete it immediately.


