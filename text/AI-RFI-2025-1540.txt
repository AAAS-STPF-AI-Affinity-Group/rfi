PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-e434-b2uh
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1540
Com m ent on FR Doc # 2025-02305
Submitter Information
Em ail:  
Organization:  AI Futures Project
General Comment
See attached file(s)
Attachments
AI Futures RFI Response (2)
AI Futures RFI Response


1 
March 14, 2025  
To: Faisal D'Souza, NCO    
Office of Science and Technology Policy  
2415 Eisenhower Avenue  
Alexandra, V A, 22314 
From: Thomas Larsen, on behalf of the AI Futures Project  
Re: Request for Information (RFI) on the Development of an Artificial Intelligence (AI) Action 
Plan (“Plan”) 
This document is approved for public dissemination. The document contains no business-proprietary or 
confidential information. Document contents may be reused by the government in developing the AI 
Action P lan and associated documents without attribution. 
The AI Futures Project is a non-profit dedicated to understanding and forecasting the 
development of AGI.  
We have a good track record in predicting the future of AI. Daniel Kokotajlo, our executive 
director, was a governance researcher at OpenAI, where he worked on scenario planning. In 
2021, he wrote a scenario forecast called "What 2026 Looks Like", whose predictions for the 
2021 - 2025 period have been quite accurate. He was recently named one of Time Magazine's 
"100 Most Influential People In AI 2024". Eli Lifland, a researcher at the AI Futures Project, is 
one of the world's top competitive forecasters. He ranks first on the RAND Forecasting Initiative  
all-time leaderboard.  
We think that AGI companies are substantially underhyping the level of AI capabilities that 
is achievable over the coming years. Artificial General Intelligence (AGI)—defined as AI 
systems surpassing human performance across all cognitive tasks—will likely be developed 
within the next decade.1 While talk of AGI has become mainstream, very few people are 
acknowledging that soon after the development of AGI, the automation of AI research will likely 
precipitate a rapid transition to superintelligence: systems outperforming humans in every 
domain. These superintelligences will have a massive effect on the world — much larger than 
the industrial revolution.  
The development of superintelligence poses a substantial chance of existential risk — 
outcomes where humanity is permanently disempowered by highly capable AI systems. The 
1Our team has different views: I (Thomas) have a median AGI timeline of 2030. Meanwhile, Daniel's median is 
2028, and Eli’s is 2032. All of us have substantial uncertainty: it could easily be later or earlier.  


2 
two central threats are a) the risk of AI takeover, and b) AI-driven concentration of power into a 
tiny group of elites. Both of these risks are irreversible, and so we need to take proactive 
measures to avoid them.  
We make the following brief recommendations:  
1.Build capacity in government to understand AGI. There should be a body in government
trying to understand the timeline to consequential AI capabilities (including extreme
capabilities, such as fully general superhuman researchers). This should involve regularly
interacting with AGI companies like OpenAI, Anthropic, and Google DeepMind, to
understand their current levels of capabilities.2.Ensure that the technical alignment problem is solved before superintelligence is built.
The US government should attempt to recognize when superintelligence is imminent. At
that point, it is likely that AI alignment will not be solved. In this case, the US
government should attempt to recognize the situation and pause or substantially slow
down frontier AI development. This will be difficult, likely requiring a deal with China.
The other option — directly building misaligned superintelligences — is substantially
more dangerous, and must be avoided.3.Aim to align superintelligence to the interests of all of humanity. On the current
trajectory, even if we solve the alignment problem, the most likely outcome is that
superintelligence is built to pursue the interests of a tiny group of elites. The US
government should take urgent measures to distribute power over AGI projects. A goodfirst step here is to require transparency over the model spec (the intended goals of the AI
system).
Thank you for your time. We are happy to discuss or expand on any of the above proposals as 
desired.  


1 
March 14, 2025  
To: Faisal D'Souza, NCO    
Office of Science and Technology Policy  
2415 Eisenhower Avenue  
Alexandra, V A, 22314 
From: Thomas Larsen, on behalf of the AI Futures Project  
Re: Request for Information (RFI) on the Development of an Artificial Intelligence (AI) Action 
Plan (“Plan”) 
This document is approved for public dissemination. The document contains no business-proprietary or 
confidential information. Document contents may be reused by the government in developing the AI 
Action P lan and associated documents without attribution. 
The AI Futures Project is a non-profit dedicated to understanding and forecasting the 
development of AGI.  
We have a good track record in predicting the future of AI. Daniel Kokotajlo, our executive 
director, was a governance researcher at OpenAI, where he worked on scenario planning. In 
2021, he wrote a scenario forecast called "What 2026 Looks Like", whose predictions for the 
2021 - 2025 period have been quite accurate. He was recently named one of Time Magazine's 
"100 Most Influential People In AI 2024". Eli Lifland, a researcher at the AI Futures Project, is 
one of the world's top competitive forecasters. He ranks first on the RAND Forecasting Initiative  
all-time leaderboard.  
We think that AGI companies are substantially underhyping the level of AI capabilities that 
is achievable over the coming years. Artificial General Intelligence (AGI)—defined as AI 
systems surpassing human performance across all cognitive tasks—will likely be developed 
within the next decade.1 While talk of AGI has become mainstream, very few people are 
acknowledging that soon after the development of AGI, the automation of AI research will likely 
precipitate a rapid transition to superintelligence: systems outperforming humans in every 
domain. These superintelligences will have a massive effect on the world — much larger than 
the industrial revolution.  
The development of superintelligence poses a substantial chance of existential risk — 
outcomes where humanity is permanently disempowered by highly capable AI systems. The 
1Our team has different views: I (Thomas) have a median AGI timeline of 2030. Meanwhile, Daniel's median is 
2028, and Eli’s is 2032. All of us have substantial uncertainty: it could easily be later or earlier.  


2 
two central threats are a) the risk of AI takeover, and b) AI-driven concentration of power into a 
tiny group of elites. Both of these risks are irreversible, and so we need to take proactive 
measures to avoid them.  
We make the following brief recommendations:  
1.Build capacity in government to understand AGI. There should be a body in government
trying to understand the timeline to consequential AI capabilities (including extreme
capabilities, such as fully general superhuman researchers). This should involve regularly
interacting with AGI companies like OpenAI, Anthropic, and Google DeepMind, to
understand their current levels of capabilities.2.Ensure that the technical alignment problem is solved before superintelligence is built.
The US government should attempt to recognize when superintelligence is imminent. At
that point, it is likely that AI alignment will not be solved. In this case, the US
government should attempt to recognize the situation and pause or substantially slow
down frontier AI development. This will be difficult, likely requiring a deal with China.
The other option — directly building misaligned superintelligences — is substantially
more dangerous, and must be avoided.3.Aim to align superintelligence to the interests of all of humanity. On the current
trajectory, even if we solve the alignment problem, the most likely outcome is that
superintelligence is built to pursue the interests of a tiny group of elites. The US
government should take urgent measures to distribute power over AGI projects. A goodfirst step here is to require transparency over the model spec (the intended goals of the AI
system).
Thank you for your time. We are happy to discuss or expand on any of the above proposals as 
desired.  


