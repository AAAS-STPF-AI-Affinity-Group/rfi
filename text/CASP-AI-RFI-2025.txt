March 14, 2025 
Faisal D'Souza 
NCO 
2415 Eisenhower Avenue 
Alexandria, VA 22314 
ostp-ai-r ﬁ@nitrd.gov 
RE: AI Action Plan 
Dear Faisal D’Souzar: 
The Council of Autism Service Providers (CASP) is a non-pro ﬁt trade association of autism service 
provider organizations, with a demonstrated commitment to promoting and delivering evidence-based 
practices for individuals with autism. CASP represents the autism provider community to the nation at 
large, including government, payers, and the general public. CASP provides information, education, and 
promotes the generally accepted standards of care for applied behavior analysis (ABA). CASP is 
committed to addressing barriers that impact access to quality services delivered by qualiﬁed providers. 
CASP  supports the responsible development of arti ﬁcial intelligence (AI) technologies and the value it 
brings to delivering quality treatment for those diagnosed with autism.  CASP appreciates the 
opportunity to provide public comment to The White House Of ﬁce of Science and Technology Policy 
(OSTP), speci ﬁcally on the Action Plan for AI Strategy. The generative technologies that have resulted 
from the advancement of AI have shown to be bene ﬁcial in many areas. However, when we examine the 
intersection of AI and applied behavior analysis (ABA), we are met with ethical challenges that must be 
considered.  
The ABA treatment model is focused on the analysis, development, and implementation of treatment 
recommendations of a professional behavior analyst. Intensive or  comprehensive ABA involves the 
hands-on and direct face-to-face application of interventions resulting in large data sets pertaining to 
the patient’s adaptive behavior across various settings and throughout time. However, there is a 
substantial administrative burden arising from the clinical documentation requirements, as well as 
activities related to ensuring ongoing access to care (e.g., veriﬁcation of bene ﬁts, authorizations, claim 
submissions).  In addition, the responsible practitioner must ensure the patient’s privacy and 
conﬁdentiality are protected, patient treatment outcomes re ﬂect personal autonomy and progress, and 
community stakeholders or funding sources receive or can obtain timely information via the patient’s 
medical record. 


The ABA industry highlights both the potential bene ﬁts (efﬁciency) and the risks (privacy and IP issues) 
of the use of AI in the healthcare setting. Guidance pertaining to the use of AI in healthcare should allow 
for innovation in analyzing large data sets and summaries that assist clinical decision making and patient 
outcomes. This guidance should also encourage the use of AI models that further administrative 
efﬁciencies while also  ensuring that the outputs and processes protect client privacy. This guidance 
should be in the form of identifying the key principles of health care and protecting or advancing those 
while encouraging implementing ef ﬁciency and analytic tools that augment decision making and 
improve outcomes. 
The Counterargument: The Opponents to Regulation 
In order to ensure our competitive position in the world economy and allow for the development of AI, 
little to no regulation should be imposed on the industry to allow for its incubated growth and market 
acceptance. 
Opponents to regulation argue the following: the market will weed out bad actors, unreliable or 
untrustworthy AI products; regulation will suppress or delay needed development and research in AI; 
and any regulation or guidance promulgated will be obsolete or unhelpful due to the speed of 
advancements in the AI industry.  
First, the argument that the market will naturally weed out bad actors and unreliable AI products 
overlooks the reality that market forces alone are insuf ﬁcient to prevent harm, particularly when AI 
failures can have irreversible consequences in areas like healthcare. Historically, industries with high 
stakes have required regulation to prevent systemic risks and protect consumers. Similarly, the claim that 
regulation will suppress or delay AI innovation is misleading; well-crafted regulations can provide clarity 
and stability, fostering responsible innovation rather than hindering it. In fact, industries with clear 
ethical and safety standards (e.g.,bio-ethics and cyber security) often attract more investment and public 
trust (NIST, 2024).1 Lastly, while the rapid pace of AI advancement is undeniable, this does not reduce 
the need for regulation.. A ﬂexible, principles-based regulatory framework can provide ethical 
safeguards that evolve alongside AI technology, ensuring ongoing accountability without sti ﬂing 
progress. 
Three-pronged focus of regulation within the ﬁeld of ABA: Clinical Decision Making; Health Care 
Optimization; Industry Compliance Risks 
The rapid deployment of AI in ABA presents signi ﬁcant challenges, particularly regarding clinical 
decision-making risks, healthcare optimization pitfalls, and industry compliance concerns.  
1 National Institute of Standards and Technology (2024) The NIST Cybersecurity Framework (CSF) 2.0. 
(National Institute of Standards and Technology, Gaithersburg, MD), NIST Cybersecurity White Paper 
(CSWP) NIST CSWP 29. https://doi.org/10.6028/NIST.CSWP.29 


Clinical Decision Making 
AI’s role in clinical decision-making raises ethical and practical concerns, as models may generate 
recommendations that lack contextual nuance or fail to account for individualized patient needs. Crowe 
et al., (2024) describe clinical decision making as a fundamental complex, cognitive model that at its 
core is conceptualized as the collecting, organizing, and the interpreting of information necessary to 
design effective interventions.2 This process is enhanced by working alongside years of invaluable 
experience coupled with mitigating idiosyncratic patient centered variables. At a functional level, AI has 
considerable weaknesses. Two of these weaknesses are hallucinations (confabulations) and omissions. 
The supposition is that behavior analysts will provide ongoing monitoring of AI generated output in 
order to detect errors. However, behavior analysts cannot be expected to recognize when all AI tools 
underperform and thus generate errors. What can be con ﬁdently stated is that even small errors can 
lead to patient harm. 
Detecting and reporting AI confabulations and omissions requires a multi-faceted approach that 
combines automated detection, human oversight, and accountability mechanisms. Flagging potential 
misinformation through anomaly detection and con ﬁdence scoring should be integrated into user 
interfaces. Additionally, user feedback loops and expert review panels ensure human intervention where 
necessary. Automated logging and regulatory compliance play a crucial role in maintaining transparency, 
allowing for traceability and correction of errors. AI systems should also provide real-time user warnings 
when information is unreliable, linking to veri ﬁed sources. To improve accuracy over time, models must 
be regularly retrained, incorporating bias reduction techniques. By implementing these safeguards, AI 
can enhance reliability, reduce misinformation, and foster greater public trust in its outputs. That is why 
it is important to have regulatory minimum requirements that ensure these safeguards are required as a 
part of any AI technology’s infrastructure.  
Health Care Optimization 
AI can contribute to healthcare optimization by reducing administrative burdens and streamlining service 
delivery. However, it can also risk prioritizing ef ﬁciency over person-centered care. If not carefully 
implemented, automation may compromise the quality of behavioral assessments and individualized 
treatment planning.  
Additionally, as AI becomes increasingly integrated into healthcare optimization, transparency and 
informed consent are essential to maintaining ethical standards in ABA. Patients (and their caregivers 
where appropriate) have the right to understand how AI is being used in their care and the safeguards in 
place to protect their privacy and treatment integrity. For example, a comprehensive disclosure 
statement should include the purpose of AI in service delivery, a clear explanation of its role in 
2 Crowe, B., Shah, S., Teng, D. et al. Recommendations for Clinicians, Technologists, and Healthcare 
Organizations on the Use of Generative Arti ﬁcial Intelligence in Medicine: A Position Statement from the 
Society of General Internal Medicine. J GEN INTERN MED 40, 694–702 (2025). 
https://doi.org/10.1007/s11606-024-09102-0 


decision-making, assurances that AI does not replace professional clinical judgment, and the steps taken 
to maintain data security and regulatory compliance. Additionally, the disclosure statement should 
provide an avenue for patients and caregivers to ask questions, express concerns, and opt out of 
AI-driven processes when appropriate. By ensuring full transparency, we can promote trust, uphold 
ethical standards, and maintain a patient-centered approach in ABA services. 
Organizations are recognizing the impact AI has and will continue to have across many sectors. 
Regarding healthcare, AI has the ability to streamline processes, thus more ef ﬁciently increasing access 
to healthcare for more patients. However, it is generally accepted for the typical patient, AI is still viewed 
as a novel, unfamiliar technology. Due to this perspective, it is critical to establish guidelines requiring 
transparency with the patient when AI is integrated into their care. According to Stanford’s AI index 
report, the United States is currently ranked as top in AI research and implementation. Our commitment 
to furthering the use and development of AI uniquely positions us with the opportunity to concurrently 
lead initiatives regulating its use. As such, leading this initiative by responding to risk and building public 
trust should be the drivers for developing regulatory standards. 
Industry Compliance Risks 
Finally, industry compliance risks are a growing concern as AI must align with federal and state 
regulations, ethical guidelines from the Behavior Analyst Certi ﬁcation Board® (BACB®), and payer 
policies. Misuse or misinterpretation of AI generated data could lead to violations of copyright laws, 
privacy laws (e.g., HIPAA), improper billing practices, and liability issues. 
Many, if not all, of the current commercial and public AI platforms have been developed using 
copyrighted or other intellectual property (IP) and are subject to active litigation. This has caused many 
end users to wonder if any use of AI is ethical at this point given the potential widespread use of 
protected IP to train the current available AI platforms. In the healthcare industry, mental health, and 
speciﬁcally for ABA providers, many assessment tools are easily accessed or replicated without 
obtaining the necessary licensing. To encourage responsible use and protections for end users, future 
guidance should balance protecting IP by considering the following:  
A.Implementing a process for licensing data sets, identifying and registering public data,
establishing protocols for opting out or removing data from public access, and developing a way
to ensure transparency of data used for training AI platforms.
B.Encouraging self-policing and internal auditing by AI platform vendors, build out tort claims or
legal protections applicable to intentional or per-se violations, such as strict liability and penalty
structures for intentional or wantonly negligent disregard of IP protections.
Additionally,  the OSTP should provide guidance within the AI Action Plan that emphasizes the 
importance of aligning the use of AI in healthcare with existing privacy laws (e.g., HIPAA). Threats to 
privacy laws can too easily exist when providers inadvertently include protected information when using 
AI platforms within their clinical practice. Security measures should include clear disclosures of 
ownership interests of AI platforms, well-developed analysis of cybersecurity risks associated with open 


source programming, internet connected storage, servers, and data encryption methods, and disclosures 
required for patient and consumer protections. This has been described as a balancing act between 
efﬁciencies offered by AI and the risks of breaches of ePHI or other private commercial work products.  
Finally, the last crucial balancing act is between ef ﬁciency and person-centered care. While AI can 
generate content with minimal inputs in the context of treatment and billing practices, overuse or over 
reliance on AI will negatively impact patient care. This could likely lead to improper billing practices, a 
signiﬁcant risk is posed when AI generated data are misinterpreted or used to automate billing without 
proper human oversight, potentially leading to fraudulent claims, Medicaid/insurance audits, and 
ﬁnancial penalties. And lastly, liability issues arise when AI driven clinical recommendations contribute 
to ineffective or harmful interventions, raising ethical and legal concerns for providers who must ensure 
that all decisions align with professional standards, regulatory requirements, and evidence-based ABA 
practices. CASP encourages the OSTP to address these concerns within the AI Action Plan. Guidance 
must make it clear that AI should not replace the skill and expertise of the clinician but should support 
and encourage efﬁcient use of AI as a tool and must ensure if/when AI is used healthcare providers are 
required to obtain informed consent.. 
Conclusion 
In conclusion, in developing an AI regulatory framework, it is important to not sti ﬂe AI innovation. 
Rather, a robust framework and clear guidance will foster public trust and con ﬁdence in both the use of 
the platforms and the protections of patient and provider end users. Regulations in AI are necessary in 
order to ensure clinical decision making is led by clinicians, health care optimization remains patient 
centric, industry compliance risks are minimized, and guidance is a good way to test or pilot potential 
regulations and their effect on AI.  Without regulation, patient harm is inevitable and provider risks are 
multiplied. While we support a robust AI market, we unequivocally implore the OSTP to include 
guidance that will lead to effective  regulations of its use within the AI Action Plan. CASP thanks the 
OSTP for the opportunity to provide comments and looks forward to the beneﬁts of responsible 
developments within AI technologies.  
Respectfully Submitted on behalf of CASP’s Arti ﬁcial Intelligence and Machine Learning workgroup, 
Mariel Fernandez, MS, BCBA, LBA Vice 
President of Government Affairs 


