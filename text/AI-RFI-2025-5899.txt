PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 15, 2025
Status: 
Tracking No. m 8a-z68b-xow5
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-5899
Com m ent on FR Doc # 2025-02305
Submitter Information
Nam e: Cody Rushing
Em ail:  
General Comment
See attached file(s)
Attachments
aiaction


1
Response to the National Science Foundation’s and
Office of Science & Technology Policy’s Request for Information
on the Development of an Artificial Intelligence (AI) Action Plan
90 Fed. Reg. 9088 (Feb. 6, 2025)
Docket No. NSF_FRDOC_0001
March 15, 2025
Cody Rushing | 
As an AI security researcher, I'm submitting this comment to inform the AI Action Plan's
development1. We are witnessing unprecedented acceleration in AI capabilities, with major labs
projecting transformative AI systems within the current administration's term. Anthropic, for
instance, anticipates that by 2027, Claude will "fi nd breakthrough solutions to challenging
problems that would have taken teams years to achieve."
To ensure AI development promotes "human f lourishing, economic competitiveness, and national
security" as stated in Executive Order 14179, it’s paramount that we address important AI Security
issues. While many groups will comment on the potential of these AI models, the innovation must
be supported by policies that address two crucial security concerns:
-We lack robust methods to ensure powerful AI systems reliably act in accordance with
security protocols. This is re flected both in expert opinion, and the growing body of
empirical examples of AI’s attempting to subvert security protocols or self-ex filtrate2. The
technical challenge of preventing advanced AI systems from attempting to circumvent
security measures remains unsolved.
-As AI labs produce more capable models, adversaries including nation-state actors will be
increasingly incentivized to steal these models. The consequences of such theft  would be
severe and multifaceted, introducing a variety of national security risks
We need to be sure that labs are equipped to handle this challenge, and that governments establish
incentives and resources to support them in this endeavor. To ensure AI development promotes
"human f lourishing, economic competitiveness, and national security" as stated in Executive Order
14179, there must be improved transparency between labs and government.
To maintain America's AI leadership while safeguarding national security, the AI Action Plan
should have targeted transparency requirements that facilitate important information transfer
2Seehttps://www.anthropic.com/research/alignment-faking as a particularly strong example of this.1The views expressed in this submission represent my professional assessment and do not necessarily ref lect the
official positions of my employer.


2
between AI labs and governments. These measures address portions of critical security threats
identified above:
i) Disclosure of In-Development Capabilities
Frontier AI labs should be required to notify federal authorities when they achieve signi ficant
capability breakthroughs, such as when they pose severe CBRN risks, or have the ability to
automate important processes of AI development. Many such important capabilities are currently
tracked by AI labs through their respective scaling plans (such as OpenAI’s Preparedness
Framework), but it’s important that the performance of these models with respect to these
evaluations is disseminated to the government, such that the government can be prepared to make
important decisions. A good system would:
-Enable the AI Safety Institute (and/or other federal government institutions) to maintain
awareness of capability advances without compromising the labs proprietary
methodologies and secrets.
-Allow the government to proactively assess security implications of new capabilities rather
than reacting a fter deployment.
-Create an early warning system for capabilities that could pose national security risks if
stolen or misused.
-Help prioritize federal resources toward securing the most advanced systems from
nation-state theft  attempts.
ii) Robust Whistleblower Protections
Strong protections for AI researchers and engineers are essential for identifying security
vulnerabilities before they can be exploited. It would be very bene ficial to introduce whistleblower
protections such that AI labs can be held accountable to the American people. These protections
should:
-Establish secure channels for reporting severe security concerns, including model behavior
that attempts to circumvent security controls.
-Reinforce America's commitment to safe AI development while deterring practices that
could undermine security.
-Help identify potential insider threats before they result in data breaches or model the ft.
-Create mechanisms for reporting cases where models demonstrate deceptive behaviors or
failures that could manifest as security vulnerabilities.
In the global race for AI leadership, unnecessary regulatory friction would disadvantage American
companies. However, certain minimal guardrails are not burdensome but rather essential
infrastructure for sustainable progress.


3
These transparency measures are some of many that would require minimal resources from AI labs
while providing crucial information that enables the government to identify where intervention is
needed and—equally important—where it is not. These recommendations can help ensure that AI
promotes the interests of the American people.
This document is approved for public dissemination. The document contains no business-proprietary or
confidential information. Document contents may be reused by the government in developing the AI
Action Plan and associated documents without attribution.


