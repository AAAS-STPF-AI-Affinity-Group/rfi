1 
March 14, 2025 
To: Faisal D’Souza, NCO 
Office of Science and Technology Policy , Executive Office of the President 
2415 Eisenhower Avenue , Alexandria, VA 22314 
Re: Request for Information (RFI) on the Development of an Artificial Intelligence (AI)  
Action Plan (“Plan”)  
About The Future of Free Speech  
The Future of Free Speech is an independent, nonpartisan think tank located at Vanderbilt 
University.1 At The Future of Free Speech, we believe that a robust and resilient culture of free 
speech must be the foundation for the future of any free, democratic society. Even as rapid 
technological change brings new challenges and threats, free speech must continue to serve as an 
essential ideal and a fundamental right f or all people, regardless of race, ethnicity, religion, 
nationality, sexual orientation, gender, or social standing.  
I.Introduction
The AI Plan must ensure that the First Amendment is a key tenet of AI policy . A commitment to
the First Amendment requires the government to avoid  censorship and jawboning , to address
harms with the least restrictive means, and to avoid preemptively regulating perceived risks .
Public institutions  should avoid favoring specific viewpoints or content . The Plan should
consider the role of open -source AI models in foster ing innovation, increasing  transparency, and
reducing  the risk of censorship.
Protecting the First Amendment and promoting a free speech culture are essential in every aspect 
of AI. Free speech constitutes the bedrock of the American political and social system and is an 
indispensable tool for promoting innovation and truth. This me morandum focuses on generative 
AI,2 as this is the area where our organization has developed its expertise.   
II.Address Harms with Least Restricti ve Means
Regulation of AI -generated content should be narrowly tailored to address specific and real
harms  – such as Child Sexual Abuse Material ( CSAM ), Non-Consensual Intimate Image ( NCII ),
fraud, and extortion  – without  infringing on protected speech . Any r estrictions should carefully
consider the First Amendment , especially the limitations it imposes on viewpoint and content -
based discrimination.
The government should leverage existing laws to regulate  serious harms and implement 
complementary legislation only when necessary if there are clear gaps or unclear standards. The 
1 “About,” The Future of Free Speech, March 5, 2025, https://futurefreespeech.org/about/.  
2 Generative AI  “is an AI technology that can create new, varying types of content, from text to imagery, to audio 
and synthetic data.” See Public -Private Exchange Program, Impact of Artificial Intelligence on Criminal and Illicit 
Activities , Department of Homeland Security, 2024, 9,  https://www.dhs.gov/sites/default/files/2024 -
10/24_0927_ia_aep -impact -ai-on-criminal -and-illicit -activities.pdf.


 
  2 
 
harms of AI -generated CSAM and NCII seem  serious and significant; however, federal 
regulation must narrowly target these abuses in accordance with existing laws and without 
threatening protected speech as an unintended side effect.   
 
We are seeing a  rise in well-intentioned but overbroad regulation targeting generative AI  content.  
Mandating broad notice -and-takedown systems can cause a chilling effect, as companies 
increasingly restrict protected content to avoid penalties.  
• For example , the Senate passed the TAKE IT DOWN Act , which  would criminalize 
NCII, including deepfakes, and mandate that social media remove such content within 48 
hours. While some a dvocates for abuse victims praised the move , others and  digital rights 
groups warn that the mandated notice -and-takedown system is overbroad and threatens 
lawful speech .  
• The Center for Democracy and Technology  argues  that the bill “will undoubtedly have a 
censorious impact on users’ free expression” since attempts to comply could result in the 
removal of satire, journalism, and other protected content.3 Moreover, a  statement by the 
Electronic Frontier Foundation  contends that “TAKE IT DOWN contains none of those 
minimal speech protections and essentially greenlights misuse of its takedown regime.”4 
 
The AI Action Plan should be cautious of broad risk -based provisions for AI.  The government 
should avoid broad requirements to mitigate systemic risks,  as the ones included in the European 
Union AI Act,  which would undoubtedly lead to increased speech restrictions by incentivizing 
over-removal.5 
 
III. Avoid Preemptive ly Regulating Perceived Risks  
The AI Action Plan should avoid preemptively regulating ill -defined, perceived risks of 
generative AI, including the impact of deepfakes on elections. Over the last several years, t here 
has been a rise in  legislati on and media focusing on the potential risks of AI -generated deepfakes 
and misinformation on elections. However,  leading research has show n that generative AI 
misinformation and deepfakes have not had a meaningful impact in the U.S. and European 
elections .  
• In a 2024 report , Princeton researchers analyzed every instance of AI use in elections 
collected by the WIRED AI Elections Project6 and found that “(1) half of AI use isn't 
deceptive, (2) deceptive content produced using AI is nevertheless cheap to replicate 
without AI, and (3) focusing on the demand for misinformation rather than the supply is a 
 
3 “Re: Concerns Regarding the TAKE IT DOWN Act.” Center for Democracy and Technology, February 12, 2025. 
https://cdt.org/wp -content/uploads/2025/02/TAKE -IT-DOWN -Sign-On-Letter_21225.pdf . 
4 Joe Mullin, “The TAKE IT DOWN Act: A Flawed Attempt to Protect Victims That Will Lead to Censorship,” 
Electronic Frontier Foundation, February 11, 2025,  https://www.eff.org/deeplinks/2025/02/take -it-down -act-flawed -
attempt -protect -victims -will-lead-censorship . 
5 Jordi Calvet -Bademunt, “Safeguarding Freedom of Expression in the AI Era,” Tech Policy Press, November 4, 
2024,  https://www.techpolicy.press/safeguarding -freedom -of-expression -in-the-ai-era/. 
6 Vittoria Elliott, “The Wired AI Elections Project,” WIRED, May 30, 2024, 
https://www.wired.com/story/generative -ai-global -elections/.  


 
  3 
 
much more effective way to diagnose problems and identify interventions.”7 Overall, 
stating that to their knowledge, “in every country that held elections in 2024 so far, AI 
misinformation had much less impact than feared.”8  
• The Centre for Emerging Technology and Security (CET aS), at The Alan Turing 
Institute,  found that “ [a]s with other recent elections, there is no evidence that AI -enabled 
disinformation or deepfakes meaningfully impacted UK or European election results. 
This is because most exposure was concentrated among a minority of users with political 
beliefs already a ligned to the ideological narratives embedded within such content.”9  
• Regarding US elections, CET aS found that “ [t]here is a lack of evidence that AI -enabled 
disinformation has had a measurable impact on the 2024 US presidential election results 
[…] Despite this, deceptive AI -generated content did shape US election discourse by 
amplifying other forms of disinformation and inflaming political debates.”10 
 
Regulat ing election -related  deepfakes  should require substantial evidence of impact , which we 
have yet to see , and be carefully  tailored . The U.S. has seen an influx of legislation regulating 
election -related manipulated media. However, “some bills define ‘deepfake ’ so broadly as to 
encompass content made or edited without the use of AI or content that doesn’t depict an 
identifiable person.”11 This broad language restricts protected speech  under the First 
Amendment. Preemptive ly trying to address these elections  concerns can violate the First 
Amendment ’s protection of satire, political commentary, and parody.12  It is important to 
recognize the benefits some satirical content can provide to political discourse. 
• The Cato Institute has stated that “ To protect Americans’ right to free expression, 
policymakers should not enact precautionary regulations that stifle the development of AI 
without clear proof of their risk of harm. Furthermore, policymakers should reject efforts 
to control the ethics and n orms around AI -powered expression. Instead, they should favor 
a robust market of AI tools that can serve as many users and perspectives as possible.”13 
 
To address the confusion over whether AI -generated content is real and c oncern that this will  
damag e trust in online sources , it is important that public officials and institutions strengthen 
 
7 Sayash Kapoor and Arvind Narayanan, “We Looked at 78 Election Deepfakes. Political Misinformation Is Not an 
AI Problem.,” Knight First Amendment Institute, December 13, 2024, https://knightcolumbia.org/blog/we -looked -
at-78-election -deepfakes -political -misinformation -is-not-an-ai-problem.  
8 Ibid. 
9 Sam Stockwell, “AI -Enabled Influence Operations: Threat Analysis of the 2024 UK and European Elections,” 
Centre for Emerging Technology and Security, September 19, 2024, https://cetas.turing.ac.uk/publications/ai -
enabled -influence -operations -threat -analys is-2024 -uk-and-european -elections.  
10 Sam Stockwell et al., “AI -Enabled Influence Operations: Safeguarding Future Elections,” Centre for Emerging 
Technology and Security, The Alan Turing Institute. November 13, 2024, https://cetas.turing.ac.uk/publications/ai -
enabled -influence -operations -safeguarding -future -elections.  
11 “Deepfakes, Democracy, and the Perils of Regulating New Communications Technologies,” The Foundation for 
Individual Rights and Expression, October 11, 2024,  https://www.thefire.org/research -learn/deepfakes -democracy -
and-perils -regulating -new-communications -technologies . 
12 Ibid. 
13 David Inserra, “Artificial Intelligence Regulation Threatens Free Expression,” Cato Institute, July 16, 2024,  
https://www.cato.org/briefing -paper/artificial -intelligence -regulation -threatens -free-expression . 


4 
“norms against falsely representing content as AI -generated .”14 This norm seeks to address the 
liar’s dividend —a dynamic  in which people use false claims of deepfakes to avoid accountability 
in the face of public scrutiny.15 
IV.Avoid Censorship and Jawboning
Government intervention in AI governance should avoid broad and preemptive censorship in
seeking to define “neutrality” or overreaching into these content policies of generative AI
systems . Additionally, transparency in government requests and communications with AI
companies when it comes to their speech policies and product decision s will greatly contribute  to
this policy goal. A commitment to the First Amendment requires government to avoid censorship
and jawboning  by pressuring private companies to change their content practices .16
V.Foster Inno vation Through  Open-Source Models
The AI Action Plan should consider the role that open -source models may have on fostering
innovation, increasing transparency, and reducing the risk of censorship.  One of the key threats
to free speech in AI governance is the concentration of AI development in a few large
corporations, which have the power to dictate what types of content their models can and cannot
generate.
Open -source models provide a counterbalance by enabling decentralized AI development, which 
allows for a broader range of viewpoints,  reduce d censorship  risk, and greater public 
participation in AI governance. This allows independent developers and researchers to deploy 
models with diverse speech norms. Open -source models increase resilience against governmental 
and corporate pressure to censor certain viewpoints or enforce broad content removal policies.  
Open-source models benefit  AI competition, innovation, and research.  This broader participation  
leads to downstream effects of  having a diversity of free expression , viewpoints, and norms.  
•According to t he National Telecommunications and Information Administration
(NTIA )’s Dual -Use Foundation Models With Widely Available Model Weight Report ,
“[o]pen models can help: (i) businesses across a range of industries integrate AI into their
services and (ii) lower the barrier to entry for non -incumbents to innovate downstream AI
applications.”17  Additionally, “[w]idely available model weights allow actors without
14 Josh A. Goldstein and Andrew Lohn, “Deepfakes, Elections, and Shrinking the Liar’s Dividend,” Brennan Center 
for Justice, January 23, 2024,  https://www.brennancenter.org/our -work/research -reports/deepfakes -elections -and-
shrinking -liars-dividend . 
15 Robert Chesney and Danielle Keats Citron, “Deep Fakes: A Looming Challenge for Privacy, Democracy, and 
National Security,” California Law Review 107 (December 2019), 
https://doi.org/https://www.californialawreview.org/print/deep -fakes -a-looming -challenge -for-privacy -democracy -
and-national -security.  
16 Matt Perault, “The FBI Leads the Way on Jawboning Governance,” Lawfare, September 3, 2024,  
https://www.lawfaremedia.org/article/the -fbi-leads -the-way-on-jawboning -governance . 
17 “Dual -Use Foundation Models With Widely Available Model Weights Report,” National Telecommunications 
and Information Administration, July 30, 2024, https://www.ntia.gov/programs -and-initiatives/artificial -
intelligence/open -model -weights -report/risks -benef its-of-dual-use-foundation -models -with-widely -available -model -
weights/competition -innovation -research.  


5 
access to the resources needed to train large models, such as non -profits and academics, 
to contribute more effectively to AI research and development.”18  
Open -source models increase transparency, which  reduces the risk of censorship from the 
concentration of p ower in the AI industry  and improves researchers ’ access to information.  
•The Foundation Model Transparency Index19 analyzed transparency indicators across
prominent AI companies and “found that open -source developers score 5.5 points higher
than the median closed -source developer. This difference is most visible in the upstream
domain, where open -source developers provide more information about how their models
were trained and developed. However, since open -source platforms have less control over
how their models are deployed, they provide less information on downstream impacts
compared to closed -source models. ”20
VI.Key Takeaways
1.Regulation of AI -generated content should be narrowly tailored to address specific and
real harms. The AI Action Plan’s Agenda should address that regulation of real risks
must be done carefully and respect the First Amendment. The U.S. should be careful
about adopting broad risk -based obligations for AI systems.
2.The AI Action Plan should avoid preemptively regulating ill-defined, perceived risks of
generative AI, including the impact of deepfakes on elections.
3.The Administration should not jawbone AI companies into  changing their content
moderation policies.
4.The AI Action Plan should consider the role that open -source and open -weight AI models
may have on fostering innovation, increasing transparency, and reducing the risk of
censorship .
18 Ibid. 
19 Rishi Bommasani et al., “The Foundation Model Transparency Index v1.1,” Center for Research on Foundation 
Models, May 2024, https://crfm.stanford.edu/fmti/paper.pdf.  
20 Prithvi Iyer, “The Foundation Model Transparency Index: What Changed in 6 Months?,” Tech Policy Press, May 
22, 2024,  https://www.techpolicy.press/the -foundation -model -transparency -index -what -changed -in-6-months /. 


 
  6 
 
V. Bibliography  
“About.” The Future of Free Speech, March 5, 2025. https://futurefreespeech.org/about/ .  
Bommasani, Rishi, Percy Liang, Nestor Maslej, Betty Xiong, Shayne Longpre, Sayash Kapoor, 
and Kevin Klyman. “The Foundation Model Transparency Index v1.1.” Center for 
Research on Foundation Models, May 2024. https://crfm.stanford.edu/fmti/paper.pdf .  
Calvet -Bademunt, Jordi. “Safeguarding Freedom of Expression in the AI Era.” Tech Policy 
Press, November 4, 2024. https://www.techpolicy.press/safeguarding -freedom -of-
expression -in-the-ai-era/.  
Chesney, Robert, and Danielle Keats Citron. “Deep Fakes: A Looming Challenge for Privacy, 
Democracy, and National Security.” California Law Review  107 (December 2019). 
https://doi.org/https://www.californialawreview.org/print/deep -fakes -a-looming -challenge -
for-privacy -democracy -and-national -security .  
“Deepfakes, Democracy, and the Perils of Regulating New Communications Technologies.” The 
Foundation for Individual Rights and Expression, October 11, 2024. 
https://www.thefire.org/research -learn/deepfakes -democracy -and-perils -regulating -new-
communications -technologies .  
“Dual -Use Foundation Models With Widely Available Model Weights Report.” National 
Telecommunications and Information Administration, July 30, 2024. 
https://www.ntia.gov/programs -and-initiatives/artificial -intelligence/open -model -weights -
report/risks -benefits -of-dual-use-foundation -models -with-widely -available -model -
weights/competition -innovation -research .  
Elliott, Vittoria. “The Wired AI Elections Project.” WIRED, May 30, 2024. 
https://www.wired.com/story/generative -ai-global -elections/ .  
Goldstein, Josh A, and Andrew Lohn. “Deepfakes, Elections, and Shrinking the Liar’s 
Dividend.” Brennan Center for Justice, January 23, 2024. 
https://www.brennancenter.org/our -work/research -reports/deepfakes -elections -and-
shrinking -liars-dividend .  
Inserra, David. “Artificial Intelligence Regulation Threatens Free Expression.” Cato Institute, 
July 16, 2024. https://www.cato.org/briefing -paper/artificial -intelligence -regulation -
threatens -free-expression .  
Iyer, Prithvi. “The Foundation Model Transparency Index: What Changed in 6 Months?” Tech 
Policy Press, May 22, 2024. https://www.techpolicy.press/the -foundation -model -
transparency -index -what -changed -in-6-months/ .  
Kapoor, Sayash, and Arvind Narayanan. “We Looked at 78 Election Deepfakes. Political 
Misinformation Is Not an AI Problem.” Knight First Amendment Institute, December 13, 
2024. https://knightcolumbia.org/blog/we -looked -at-78-election -deepfakes -political -
misinformation -is-not-an-ai-problem .  
Mullin, Joe. “The TAKE IT DOWN Act: A Flawed Attempt to Protect Victims That Will Lead 
to Censorship.” Electronic Frontier Foundation, February 11, 2025. 
https://www.eff.org/deeplinks/2025/02/take -it-down -act-flawed -attempt -protect -victims -
will-lead-censorship .  
Perault, Matt. “The FBI Leads the Way on Jawboning Governance.” Lawfare, September 3, 
2024. https://www.lawfaremedia.org/article/the -fbi-leads -the-way-on-jawboning -
governance .  


7 
Public -Private Exchange Program. “Impact of Artificial Intelligence on Criminal and Illicit 
Activities.” Department of Homeland Security, 2024. 
https://www.dhs.gov/sites/default/files/2024 -10/24_0927_ia_aep -impact -ai-on-criminal -
and-illicit -activities.pdf . 
“Re: Concerns Regarding the TAKE IT DOWN Act.” Center for Democracy and Technology, 
February 12, 2025. https://cdt.org/wp -content/uploads/2025/02/TAKE -IT-DOWN -Sign-
On-Letter_21225.pdf . 
Stockwell, Sam, Megan Hughes, Phil Swatton, Albert Zhang, Jonathan Hall KC, and Kieran. 
“AI-Enabled Influence Operations: Safeguarding Future Elections.” Centre for Emerging 
Technology and Security, November 13, 2024. https://cetas.turing.ac.uk/publications/ai -
enabled -influence -operations -safeguarding -future -elections . 
Stockwell, Sam. “AI -Enabled Influence Operations: Threat Analysis of the 2024 UK and 
European Elections.” Centre for Emerging Technology and Security, September 19, 2024. 
https://cetas.turing.ac.uk/publications/ai -enabled -influence -operations -threat -analysis -
2024 -uk-and-european -elections .


