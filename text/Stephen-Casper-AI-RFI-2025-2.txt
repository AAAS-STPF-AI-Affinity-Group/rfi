A pro-innovation approach to AI governance requires safety and understanding 
Stephen Casper 
Disasters are both intrinsically bad and lead to regulatory hammers. In the history of safety 
engineering, many major system failures all follow a certain loose story (Dekker, 2019). It starts 
off with some system – e.g., a dam, bridge, power plant, oil rig, building, etc – that functions 
normally for a long time. At first, this is accompanied by direct evidence of benefits and no 
evidence of major harms which can lull engineers into a false sense of security. But then, 
tragedy strikes suddenly. For example, before the infamous 1986 Challenger space shuttle 
explosion, there were 9 successful launches (Gebhardt, 2011) which was a factor that led 
engineers to neglect saf ety warnings before the infamous 10th launch. Things were fine, and 
the empirical evidence looked good until disaster struck. There is scientific consensus that AI is 
a very powerful technology and that it could pose national security threats. This points us to 
invest in safeguards while we develop it. If AI has a ‘Chernobyl moment,’ it would not only be 
tragic in and of itself, but would almost certainly trigger a regulatory hammer effect afterward 
(Hendrycks, 2024). 
Democra cies require information to function. Historically, AI has been developed in a very 
open way in the scientific community. However, in recent years, it has become both more 
impactful and more closed. T his is often described as the “crisis of transparency” in AI. 
Currently, there is limited scientific understanding and even less public understanding of (1) how 
labs train their AI systems, (2) how their AI systems function internally, and (3) how they are 
governed internally. Having more transparency into this (e.g., by having companies produce 
documentation of their internal risk assessments) would improv public understanding and 
society’s ability to have more informed discussions about this emerging technology. It would 
also allow for more competitiveness and innovation in the AI space by increasing awareness of 
frontier practices.  


