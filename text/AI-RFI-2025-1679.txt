PUBLIC SUBMISSIONAs of: March 21, 2025
Received: March 14, 2025
Status: 
Tracking No. m 89-m izl-yn8j
Com m ents Due: March 15, 2025
Subm ission Type: API
Docket: NSF_FRDOC_0001
Recently Posted NSF Rules and Notices.
Com m ent On: NSF_FRDOC_0001-3479
Request for Inform ation: Developm ent of an Artificial Intelligence Action Plan
Docum ent: NSF_FRDOC_0001-DRAFT-1679
Com m ent on FR Doc # 2025-02305
Submitter Information
Name: Gordon Long 
General Comment
Thank you very m uch for giving m e the opportunity to provide input to this im portant Action Plan.
My input paper is attached as a PDF file.
Attachments
Developm ent of an Artificial Intelligence Action Plan_Input_(Final)
Developm ent of an Artificial Intelligence Action Plan_Input_(14Mar25)


1 Development of an Artificial Intelligence (AI) Action Plan :  Input 
By Gordon Alfred Long 
Executive Summary:  Artificial Intelligence (AI) technology can help solve problems and 
mitigate challenges presented by today’s rapidly -developing infrastructure, transportation, and 
communications networks.  Core functional components of AI such as process monitoring and 
control, machine learning, human speech understanding, pattern recognition, and many other 
essential capabilities as well, can seamlessly and cost-efficiently compliment human abilities 
and perform complex tasks which are not possible without computer assistance.  However, few 
(if any) of these immediately useful AI applications require extremely large (i.e., expensive) data 
processing centers or human-like automated simulacra.  Instead of questionable attempts to 
replicate human forms and behavior on a grand scale that one sees throughout toda y’s news, it 
woul d seem advisable for the U.S. Government initially to seek solutions to specific problems 
that AI can actually solve right now, and gradually expand the scope of these simpler AI-based 
solutions systematically over time. 
The purpose of this input to the AI Action Plan is to recommend some of the means by which 
targeted, straightforward application of AI to known technical problem areas can produce 
useful results quickly and in cost-effective ways, which in turn may be considered suitable for 
inclusion in the  soon-to-be-published OSTP and NITRD NCO Action Plan. 
Background:  AI practitioners have developed complex lexicons to describe current subdivisions 
of the field, but at the most basic level, there are two types of AI entity, rule-based and self-
taught.  There can also be hybrids of these two types of AI architectures as well.   
In a rule-based AI system, the rules that are imposed on the embedded AI entity govern system 
functionality, and the AI has little to no discretion about how it performs.  An example of this 
type of AI (and there are many such) is an inventory control system for a business or 
warehouse.  The rules written into the AI’s software are followed explicitly, and if additional 
functions or different behavior is desired, the rules must be manually updated to reflect the 
intended new behavior of the system.  The principal advantage of such a system is that the 
governing ruleset is well understood and precisely defined, so that correct system behavior is 
reliable and unsurprising.  Malfunctions are easily diagnosed and can be corrected in a 
straightforward manner.  The principal disadvantage of this type of AI entity is that developing 
and maintaining the fixed ruleset is usually quite time consuming (and thus costly) and often 
certification of new software versions can require numerous management-level approvals , 
which frequently introduces further administrative complexity and potential delays.  
In contrast, an AI entity which is designed to learn on its own or teach itself essentially develops 
its own ruleset, and its success is judged on the basis of the correctness of its performance.  A 
useful example from several decades ago is a photograph or reconnaissance image 
interpretation system which was originally intended to detect camouflaged armored vehicle s 
concealed in treelines on the forested edges of open terrain.  System developers would 


2 “educate ” this system by presenting large numbers of images to the embedded AI entity and 
“ask ing” it to point out and (if possible) identify any camouflaged vehicles it detects.  The 
developers would then score the results and feed the correct and incorrect machine answers 
back into the machine.  Then a new set of images would be presented to the machine, scored, 
and fed back.  This process was repeated many times, with observed increases in accuracy duly 
noted.  Eventually the “skill level”  of the AI entity would be pronounced acceptable, or if the 
learning process was judged to be too slow, either the AI’s sensors or its image discrimination  
software would be modified.   The principal advantage of this methodology is that there is no 
need to develop complex rulesets, or maintain them – the AI entity itself does that work.  And, 
as it processes more and more images (or other types of data) which are in turn scored and fed 
back, its proficiency steadily improves.  The principal disadvantage is that developers cannot 
know exactly what rules the AI entity has created for itself, which sometimes may cause 
uncertain outcomes or make malfunction diagnosis difficult.  However, when system analytical 
tasks are complex or data volumes are high – or if immediate warning of undesirable events 
and mishaps is needed -- this type of AI entity is generally preferred.   
Strongly recommend president Trump’s new AI policy  – and the Action Plan – focus initially on 
how best to add hybrid combinations of these two types of systems to existing hardware and 
software architectures to provide supervisory and monitoring functionality that humans alone 
cannot perform.  Certified expert human oversight and approval should of course be mandatory 
for any actions taken in response to an AI entity’s outputs or recommendations. 
Cost-effective First Applications:  In view of the rapid deve lopment of cyber criminals’ hacking 
techniques and the vast improvement seen lately in their skill levels – especially when nation 
states are involved – adding AI system monitoring entities and their supporting hardware and 
software to critical electronic systems should be assigned very high priorities.   Unassisted 
human capabilities are inadequate to monitor and manage complex mission-essential computer 
systems and networks, and/or to process very large amounts of data in near-real time.  These 
so-called “Big Data” challenges pre sent fertile ground for achieving quick and profitable Returns 
on Investment (ROI) after the necessary AI installations are accomplished.  Development of a 
body of common knowledge and practice should also be called for in the Action Plan to enable 
efficient creation of standardized, distributed, cost-effective self-teaching AI entities.  The 
driving concept would be to create widely-distributed AI “System s of Systems” that employ  
largely undetectable monitoring capabilities to observe systems operations in detail, and 
incorporate machine learning, human speech understanding and interactive discussion 
capabilities to act as “24/7 colleague s” to assist human security managers.  Ideally, such AI 
colleagues would be designed to be transparent to and compatible with standard firewalls and 
network intrusion protections, which -- although still essential for effecti ve security operations -
- inevitably lag in their ability to detect new or exotic threats.  Relevant automatically updated 
data subscription and geospatial information services would also be provided for rule-based 
situational awareness and reference purposes.  Each of these AI entities would be “taught” by  
installing it directly on the system to be monitored in a passive observation mode, immersing it 


3 in day- to-day systems operations for several weeks, and instructing it to report periodically any 
system behavior that it categorizes as abnormal (with recorded data reports to substantiate its 
“judgement”).   The assigned human security official would then interactively discuss the 
strengths and weaknesses of the AI entity’s findings , with explanations why the observed 
unusual behavior was or was not within normal performance expectations, especially 
emphasizing how much risk to system security might be involved.  As the AI entity’s 
performance improves, the human security officer working with it would naturally develop 
more and more trust in the AI entity, and could (with appropriate higher-level organization al 
approval) increase the scope of what functions it performs.  Eventually the human-AI team 
could develop reliable systems of notification and warning whereby appropriate officials could 
be immediately informed of malicious events or system malfunctions which would otherwise 
not be reported in a timely manner, or may even not be noticed at all.  Such a set of hybrid 
capabilities would be especially valuable in detecting and countering Advanced Persistent 
Threats (APTs). 
In similar fashion, AI entities could be set to monitor aircraft flight control and avionics systems 
continuously to detect and mitigate potentially damaging or fatal malfunctions.  These AI-
powered monitoring systems could also be programmed to detect and report malware secretly 
installed in aircraft on-board systems by hostile forces.  Costs involved with installing such AI 
monitoring systems are likely not to be prohibitive, especially when compared to the costs 
associated with recovery and cleanup from the adverse events they would be able to warn of 
and protect against. 
But perhaps the most effective way in which AI could be used immediately is to improve the 
reliability of long-haul and wide-area networks, which routinely suffer from extensive outages 
and dropped messages, incomplete reconfiguration plans, and general lack of in-network 
malfunction diagnostics.  Personal direct experience using very capable network performance 
diagnostic tools soon revealed that these tools had no capability at all to fix the problems they 
discovered, or even reroute message traffic around problem nodes or links.  An AI entity could 
assist greatly in providing and managing appropriate corrective capabilities for typical network 
outages and degradations. 
In addition to near-term cost-benefit advantages to be achieved by initiating Action Plan work 
with relatively less-ambitious AI projects for these and many similar applications, there will 
arise natural ly-occurring concurrent protection methods (discovered through hands-on 
engineering experience) against the dangers which many fear are possible were a complex AI 
entity to run amok.  With considerable numbers of such focused AI projects in progress 
concurrently, much will also be learned collectively about what does and does not work, and 
collaborative “best of breed” production decisions will be enabled.    


4 Recommendations:   The principal recommendation this paper presents is to assign at least half 
of the projects the Action Plan will direct to developing immediate or near-term AI applications 
which are specific to known problems and technical challenges and can be implemented with 
existing technology in cost effective ways.   All three possible AI applications described above 
exhibit these characteristics, and technical means for implementing the basic functions involved 
have been generally known (but typically only applied piecemeal) for at least a decade and a 
half.   The most urgent needs now are to integrate these critical supporting functions into AI 
entities which can serve as respected colleagues for human operators, rather than as slaves or 
mere servants – and to compile a supporting Common Body of Knowledge. 
The second recommendation is to approach testing of AI solutions in much the same ways that 
Special Government Employee Elon Musk (in his civilian role as SpaceX CEO) tests spacecraft 
hardware and software – in actual U.S. government and civilian space systems that are either in 
regular use or in development.   In the case of AI then, test new designs directly on existing 
systems and platforms which are known to be in need of continuous 24/7 monitoring, or will 
perform better with near-real-time problem identification/reporting, and will benefit from 
effective AI corrective actions and proposals for improvement. 
Along with implementation of these simple recommendations will come significant aggregate 
performance improvement in operational and developmental systems to which AI is applied, in 
much the same fashion as occurs naturally in self-teaching AI systems themselves. 
Respectfully submitted by:     Gordon Alfred Long 
“This document is approved for public dissemination. The document contains no business-
proprietary or confidential information. Document contents may be reused by the government 
in developing the AI Action Plan and associated documents without attribution.”  


1 Development of an Artificial Intelligence (AI) Action Plan :  Input 
By Gordon Alfred Long 
Executive Summary:  Artificial Intelligence (AI) technology can help solve problems and 
mitigate challenges presented by today’s rapidly -developing infrastructure, transportation, and 
communications networks.  Core functional components of AI such as process monitoring and 
control, machine learning, human speech understanding, pattern recognition, and many other 
essential capabilities as well, can seamlessly and cost- efficiently compliment human abilities 
and perform complex tasks which are not possible without computer assistance.  However, few 
(if any) of these immediately useful AI applications require extremely large (i.e., expensive) data 
processing centers or human-like automated simulacra.  Instead of questionable attempts to 
replicate human forms and behavior on a grand scale that one sees throughout toda y’s news, it 
would seem advisable for the U.S. Government initially to seek solutions to specific problems 
that AI can actually solve right now, and gradually expand the scope of these simpler AI-based 
solutions systematically over time. 
The purpose of this input to the AI Action Plan is to recommend some of the means by which 
targeted, straightforward application of AI to known technical problem areas can produce 
useful results quickly and in cost-effective ways, which in turn may be considered suitable for 
inclusion in the  soon- to-be-published OSTP and NITRD NCO Action Plan. 
Background:  AI practitioners have developed complex lexicons to describe current subdivisions 
of the field, but at the most basic level, there are two types of AI entity, rule-based and self-
taught.  There can also be hybrids of these two types of AI architectures as well.   
In a rule-based AI system, the rules that are imposed on the embedded AI entity govern system 
functionality, and the AI has little to no discretion about how it performs.  An example of this 
type of AI (and there are many such) is an inventory control system for a business or 
warehouse.  The rules written into the AI’s software are followed explicitly, and if additional 
functions or different behavior is desired, the rules must be manually updated to reflect the 
intended new behavior of the system.  The principal advantage of such a system is that the 
governing ruleset is well understood and precisely defined, so that correct system behavior is 
reliable and unsurprising.  Malfunctions are easily diagnosed and can be corrected in a 
straightforward manner.  The principal disadvantage of this type of AI entity is that developing 
and maintaining the fixed ruleset is usually quite time consuming (and thus costly) and often 
certification of new software versions can require numerous management-level approvals, 
which frequently introduces further administrative complexity and potential delays.  
In contrast, an AI entity which is designed to learn on its own or teach itself essentially develops 
its own ruleset, and its success is judged on the basis of the correctness of its performance.  A 
useful example from several decades ago is a photograph or reconnaissance image 
interpretation system which was originally intended to detect camouflaged armored vehicle s 
concealed in treelines on the forested edges of open terrain.  System developers would 


2 “educate” this system by presenting large numbers of images to the embedded AI entity and 
“asking” it to point out  and (if possible) identify any camouflaged vehicles it detects.  The 
developers would then score the results and feed the correct and incorrect machine answers 
back into the machine.  Then a new set of images would be presented to the machine, scored, 
and fed back.  This process was repeated many times, with observed increases in accuracy duly 
noted.  Eventually the “skill level”  of the AI entity would be pronounced acceptable, or if the 
learning process was judged to be too slow, either the AI’s sensors or its image discrimination  
software would be modified.   The principal advantage of this methodology is that there is no 
need to develop complex rulesets, or maintain them – the AI entity itself does that work.  And, 
as it processes more and more images (or other types of data) which are in turn scored and fed 
back, its proficiency steadily improves.  The principal disadvantage is that developers cannot 
know exactly what rules the AI entity has created for itself, which sometimes may cause 
uncertain outcomes or make malfunction diagnosis difficult.  However, when system analytical 
tasks are complex or data volumes are high – or if immediate warning of undesirable events 
and mishaps is needed -- this type of AI entity is generally preferred.   
Strongly recommend president Trump’s new AI policy  – and the Action Plan – focus initially on 
how best to add hybrid combinations of these two types of systems to existing hardware and 
software architectures to provide supervisory and monitoring functionality that humans alone 
cannot perform.  Certified expert human oversight and approval should of course be mandatory 
for any actions taken in response t o an AI entity’s outputs or recommendations. 
Cost-effective First Applications:  In view of the rapid development of cyber criminals’ hacking 
techniques and the vast improvement seen lately in their skill levels – especially when nation 
states are involved – adding AI system monitoring entities and their supporting hardware and 
software to critical electronic systems should be assigned very high priorities.   Unassisted 
human capabilities are inadequate to monitor and manage complex mission-essential computer 
systems and networks, and/or to process very large amounts of data in near-real time.  These 
so-called “Big Data” challenges present fertile ground for achieving quick and profitable Return s 
on Investment (ROI) after the necessary AI installations are accomplished.  Development of a 
body of common knowledge and practice should also be called for in the Action Plan to enable 
efficient creation of standardized, distributed, cost-effective self-teaching AI entities.  The 
driving concept would be to create widely-distributed AI “System s of Systems” that employ  
largely undetectable monitoring capabilities to observe systems operations in detail, and 
incorporate machine learning, human speech understanding and interactive discussion 
capabilities to act as “24/7  colleagues ” to assist human security managers.  Ideally, such AI 
colleagues would be designed to be transparent to and compatible with standard firewalls and 
network intrusion protections, which -- although still essential for effective security operation s -
- inevitably lag in their ability to detect new or exotic threats.  Relevant automatically updated 
data subscription and geospatial information services would also be provided for rule-based 
situational awareness and reference purposes.  Each of these AI entities would be “taught” by  
installing it directly on the system to be monitored in a passive observation mode, immersing it 


3 in day- to-day systems operations for several weeks, and instructing it to report periodically any 
system behavior that it categorizes as abnormal (with recorded data reports to substantiate its 
“judgement”).   The assigned human security official would then interactively discuss the 
strengths and weaknesses of the AI entity’s findings , with explanations why the observed 
unusual behavior was or was not within normal performance expectations, especially 
emphasizing how much risk to system security might be involved.  As the AI entity’s 
performance improves, the human security officer working with it would naturally develop 
more and more trust in the AI entity, and could (with appropriate higher-level organization al 
approval) increase the scope of what functions it performs.  Eventually the human-AI team 
could develop reliable systems of notification and warning whereby appropriate officials could 
be immediately informed of malicious events or system malfunctions which would otherwise 
not be reported in a timely manner, or may even not be noticed at all.  Such a set of hybrid 
capabilities would be especially valuable in detecting and countering Advanced Persistent 
Threats (APTs). 
In similar fashion, AI entities could be set to monitor aircraft flight control and avionics systems 
continuously to detect and mitigate potentially damaging or fatal malfunctions.  These AI-
powered monitoring systems could also be programmed to detect and report malware secretly 
installed in aircraft on-board systems by hostile forces.  Costs involved with installing such AI 
monitoring systems are likely not to be prohibitive, especially when compared to the cost s 
associated with recovery and cleanup from the adverse events they would be able to warn of 
and protect against. 
But perhaps the most effective way in which AI could be used immediately is to improve the 
reliability of long-haul and wide-area networks, which routinely suffer from extensive outages 
and dropped messages, incomplete reconfiguration plans, and general lack of in-network 
malfunction diagnostics.  Personal direct experience using very capable network performance 
diagnostic tools soon revealed that these tools had no capability at all to fix the problems they 
discovered, or even reroute message traffic around problem nodes or links.  An AI entity could 
assist greatly in providing and managing appropriate corrective capabilities for typical network 
outages and degradations. 
In addition to near-term cost-benefit advantages to be achieved by initiating Action Plan work 
with relatively less-ambitious AI projects for these and many similar applications, there will 
arise natural ly-occurring concurrent protection methods (discovered through hands-on 
engineering experience) against the dangers which many fear are possible were a complex AI 
entity to run amok.  With considerable numbers of such focused AI projects in progress 
concurrently, much will also be learned collectively about what does and does not work, and 
collaborative “best of breed” production decisions will be enabled.    


4 Recommendations:   The principal recommendation this paper presents is to assign at least half 
of the projects the Action Plan will direct to developing immediate or near-term AI applications 
which are specific to known problems and technical challenges and can be implemented with 
existing technology in cost effective ways.   All three possible AI applications described above 
exhibit these characteristics, and technical means for implementing the basic functions involved 
have been generally known (but typically only applied piecemeal) for at least a decade and a 
half.   The most urgent needs now are to integrate these critical supporting functions into AI 
entities which can serve as respected colleagues for human operators, rather than as slaves or 
mere servants – and to compile a supporting Common Body of Knowledge. 
The second recommendation is to approach testing of AI solutions in much the same ways that 
Special Government Employee Elon Musk (in his civilian role as SpaceX CEO) tests spacecraft 
hardware and software – in actual U.S. government and civilian space systems that are either in 
regular use or in development.   In the case of AI then, test new designs directly on existing 
systems and platforms which are known to be in need of continuous 24/7 monitoring, or will 
perform better with near-real-time problem identification/reporting, and will benefit from 
effective AI corrective actions and proposals for improvement. 
Along with implementation of these simple recommendations will come significant aggregate 
performance improvement in operational and developmental systems to which AI is applied, in 
much the same fashion as occurs naturally in self-teaching AI systems themselves. 
Respectfully submitted by:     Gordon Alfred Long 
“This document is approved for public dissemination. The document contains no business -
proprietary or confidential information. Document contents may be reused by the government 
in developing the AI Action Plan and associated documents without attribution.”  


