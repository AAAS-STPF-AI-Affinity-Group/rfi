RFI response regarding the national
AI Action Plan
by
George Kesidis & David J. Miller
1.The dangers of using AI
While AI has demonstrated a potential for significant beneficial impact in many market 
and government sectors, it has also been demonstrated to suffer from bias and security 
vulnerabilities. These are serious concerns for high -riskapplication s. Defending and 
robustifying AIsis not a trivial matter ,particularly if one does not want to impede 
innovative applicatio ns. 
The vulnerabilities of an AI are likely exacerbated by excess parameterization, 
i.e., models which are too large. For decades ,machine learning researchers have
understood the risks of over -parameterization ,which may cause excessive error (model
variance) due to overfitting. Indeed, modern deep learning frameworks have techniques
(such as random dropout ) which are intended to address this problem.
However , som e “research” has questioned whether overfitting is a problem for 
applications requiring very large models (like LLMs); for example, the so -called “double -
descent hypothesis”.  This research has been used to push for ever -larger models, both 
driving and justifying thetrend toward huge data -centers (clouds), consisting of many 
thousands of enormou s and expensive hardware accelerators (e.g., Nvidia H100 
GPUs). 
So, the release of Deepseek earlier this year by the Chinese wasa surprise to 
some. Deepseek is an open -source general -purpose chatbot which was trained on 
publicly available data using low er-end, inexpensive GPUs. Little about Deepseek’s 
design is truly novel. Compared to the latest closed -source ChatGPT developed by 
OpenAI in the USA, Deepseek is a smaller model with arguably better performance and 
which was produced at a fraction of the c ost.
2.The dangers of overfunding , particularly AI
Just as excess model parameterization, excess research funding can yield negative 
results , particularly for AI related research ,which is a highly data -depende ntarea 
involving a lot of ad hoc decision -making through trial -and-error experimentation. 
The four main “AI conferences” (NeurIPS, ICML, AAAI, ICLR) annually receive a 
total of about 50,000 submissions ,of which a total of about 10,000 are accepted (about 
10% of accepted papers are orally presented ;the rest are “presented” only in large 
poster sessions) .  At this scale, these are not genuine academic conferences but more 
like money -making annual conventions which mass -produc e “accepted research 


articles ”.  Also, they can be more cynically characterized as cesspits of money (much of 
it from the federal government), influence peddling, and poor -quality research. 
This is typical of other heavily funded, applied -research communit ies.A 
representative of the Association of Computing Machinery (ACM) presented a slide at 
ACM CCS 2024 (one of the three “top” conferences in cyber security) chronicling 
corruption in its putatively double -blind peer-review process (including a case where an 
author was caught uploading a review for his own paper). Note that such paper s
appear as peer -reviewed work -product in annual reports (to federal agencies) of 
research grants .Leadership in f ederal research -funding agencies cannot deny they are 
aware of these ethical problems ,which may be construed as fraud. For that matter , 
why should systematic peer -review corruption be limited to research conferences and 
not be present in the proposal -review panels where the stakes are much higher?
3.Recommendations regarding AI Research Funding
We suggest targeted investments in secure and robust AI, which is a cross -disciplinary 
area between AI/ML and cyber security . Smaller grants will be most impactful , while the 
numerous and very large -scale AI centers have proven a waste of taxpayers’ money .
A lowerlimit to the annual n umber of papers per researcher needs to be set to 
promote research quality (capping conference registration costs and publication 
charges allowed on federal grants can also help with this). Just asone example, a 
recent high -profile conference warned the a uthors of submitted papers that any 
submissions beyond 2 5 by the same author (!) will be automatically rejected.  But 
allowing 25 papers submitted by the same author to a single conference is obviously 
about bean counting ,not quality . 
The balance of rese arch investments in AI can be redirected to graduate -student 
scholarships.  This could be a far more efficient use of precious research funds. 
Universities need to be encouraged to refocus on properly educating our student 
scientists, engineers and compute r scientists, rather than running degree mills and 
paper mills .Despite much higher cost of education, our average undergraduates are at 
present significantly inferior compared to foreign undergraduates (who often have a far 
better grasp of the basics). AI is exacerbating these negative trends.
Though there are some accounts of these problems in the public media (mass 
retraction s of published research articles papers, co -citation cartels, fake- paper 
authorship marketplaces, poor literacy and numeracy rat es of average college 
graduates, etc.), there has been in the past very little leadership from government on 
the issue of research ethics and generally holding very well compensated university 
administrators more accountable . This needs to be a focus of th e new federal 
leadership in the context of an AI Action Plan.


George Kesidis (Ph.D. UC Berkeley) & David J. Miller (Ph.D. UC Santa Barbara)
are EECS professors at Penn State and co -founders of Anomalee Inc., a boutique 
AI/ML firm specializing in adversar ial AI. They have been AI/ML researchers for over 
30 years, focusing on secure and robust AI over the past ten years. They have also 
contributed to problems in cyber security .They have reviewed hundreds of grant 
proposals and GK has previously serving as an Intermittent Expert for NSF’s SaTC 
program. They have n either previously joined co -authorship, co -citation, or co -peer-
review cartels , nor are they currently engaged in any type of influence peddling. In 
2023, their book entitled   “Adversarial Learning and Secure AI” was published by 
Cambridge University Press. They are currently developing comprehensive 
platforms to benchmark, certify and online -monitor deployed AIs based in large part 
on their prior research (some of which is proprietary to Anomalee I nc.). 


